{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b5059c1-d782-40d3-a67f-3ed6a1434016",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4509a8e0982bed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv pandas transformers torch sentence-transformers scikit-learn numpy openai spotipy flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e358f7c7-2554-43a1-9aca-84dc06b1e252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fastapi\n",
      "  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting watchfiles\n",
      "  Downloading watchfiles-1.1.0-cp313-cp313-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: click>=7.0 in c:\\python313\\lib\\site-packages (from uvicorn) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\vicente chiriguaya\\appdata\\roaming\\python\\python313\\site-packages (from uvicorn) (0.16.0)\n",
      "Collecting starlette<0.48.0,>=0.40.0 (from fastapi)\n",
      "  Downloading starlette-0.47.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\python313\\lib\\site-packages (from fastapi) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\vicente chiriguaya\\appdata\\roaming\\python\\python313\\site-packages (from fastapi) (4.14.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\vicente chiriguaya\\appdata\\roaming\\python\\python313\\site-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\vicente chiriguaya\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\vicente chiriguaya\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\vicente chiriguaya\\appdata\\roaming\\python\\python313\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Downloading starlette-0.47.3-py3-none-any.whl (72 kB)\n",
      "Downloading watchfiles-1.1.0-cp313-cp313-win_amd64.whl (292 kB)\n",
      "Installing collected packages: watchfiles, uvicorn, starlette, fastapi\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [uvicorn]\n",
      "   ---------- ----------------------------- 1/4 [uvicorn]\n",
      "   ---------- ----------------------------- 1/4 [uvicorn]\n",
      "   -------------------- ------------------- 2/4 [starlette]\n",
      "   -------------------- ------------------- 2/4 [starlette]\n",
      "   -------------------- ------------------- 2/4 [starlette]\n",
      "   ------------------------------ --------- 3/4 [fastapi]\n",
      "   ------------------------------ --------- 3/4 [fastapi]\n",
      "   ------------------------------ --------- 3/4 [fastapi]\n",
      "   ---------------------------------------- 4/4 [fastapi]\n",
      "\n",
      "Successfully installed fastapi-0.116.1 starlette-0.47.3 uvicorn-0.35.0 watchfiles-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install uvicorn fastapi watchfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152013f0-8ff0-4eb1-9a72-a22859d7d1a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Spotify Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b72d13c-d084-42c1-a6a3-e93716580e06",
   "metadata": {},
   "source": [
    "The first step is to use the prepared Spotify module to extract the data and save it in an SQLite file. (Alternatively, you can configure a MySQL file using the mysql.py wrapper in the libraries folder, or create your own)."
   ]
  },
  {
   "cell_type": "code",
   "id": "93e664ac-83d0-4f7e-ae9d-030b7b6d3c4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T01:01:11.901987Z",
     "start_time": "2025-09-07T01:00:59.947770Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "import webbrowser\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "# Spotify logic (PKCE-only helpers and iterators)\n",
    "from libraries.spotify import get_spotify_client, iter_saved_tracks\n",
    "\n",
    "# SQLite backend\n",
    "from libraries.sqlite import (\n",
    "    create_connection,\n",
    "    ensure_database_and_tables,\n",
    "    upsert_artist,\n",
    "    upsert_album,\n",
    "    upsert_track,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Configuration (edit here)\n",
    "# =========================\n",
    "CLIENT_ID: str = \"ec6c43ef03034d7393a4906cd1df5f06\"         # do not commit real values to public repos\n",
    "REDIRECT_URI: str = \"https://spotify-auth.viant.dev/callback\"  # must match Spotify dashboard\n",
    "SCOPE: str = \"user-library-read\"\n",
    "CACHE_PATH: str = \".cache\"                        # per-user cache file\n",
    "LOCAL_CALLBACK_PORT: int = 8080                  # must match your Vercel relay target (127.0.0.1:<PORT>)\n",
    "\n",
    "# =========================\n",
    "# Server state (global)\n",
    "# =========================\n",
    "SERVER_STATE: Dict[str, Optional[Any]] = {\"server\": None, \"thread\": None}\n",
    "\n",
    "\n",
    "def build_spotify_pkce(\n",
    "    client_id: str,\n",
    "    redirect_uri: str,\n",
    "    scope: str,\n",
    "    cache_path: str,\n",
    "    open_browser: bool = False,\n",
    "):\n",
    "    \"\"\"Return a PKCE auth manager (Spotipy SpotifyPKCE).\"\"\"\n",
    "    from spotipy.oauth2 import SpotifyPKCE  # PKCE class (no client_secret)\n",
    "    return SpotifyPKCE(\n",
    "        client_id=client_id,\n",
    "        redirect_uri=redirect_uri,\n",
    "        scope=scope,\n",
    "        open_browser=open_browser,  # we manually open the browser\n",
    "        cache_path=cache_path,\n",
    "    )\n",
    "\n",
    "\n",
    "def wait_for_token(auth_mgr, timeout: int = 300, interval: float = 1.0) -> bool:\n",
    "    \"\"\"Poll the cache until a valid token exists or timeout expires.\"\"\"\n",
    "    start = time.time()\n",
    "    while time.time() - start < timeout:\n",
    "        try:\n",
    "            token = auth_mgr.get_cached_token()\n",
    "            if token and token.get(\"access_token\"):\n",
    "                return True\n",
    "        except Exception:\n",
    "            # Some spotipy versions may raise if cache is empty; ignore and keep polling\n",
    "            pass\n",
    "        time.sleep(interval)\n",
    "    return False\n",
    "\n",
    "\n",
    "def run_server(app: Flask) -> None:\n",
    "    \"\"\"Run Flask using a Werkzeug WSGI server we can stop programmatically.\"\"\"\n",
    "    from werkzeug.serving import make_server\n",
    "    httpd = make_server(\"127.0.0.1\", LOCAL_CALLBACK_PORT, app)\n",
    "    SERVER_STATE[\"server\"] = httpd\n",
    "    # Blocks current thread; intended to be called from a background thread.\n",
    "    httpd.serve_forever()\n",
    "\n",
    "\n",
    "def stop_server() -> None:\n",
    "    \"\"\"Stop the local auth callback server if it is running.\"\"\"\n",
    "    server = SERVER_STATE.get(\"server\")\n",
    "    thread = SERVER_STATE.get(\"thread\")\n",
    "\n",
    "    if server is None:\n",
    "        print(\"â„¹ï¸ No server is currently running.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        server.shutdown()  # Properly shut down the Werkzeug server\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to shutdown server cleanly: {e}\")\n",
    "\n",
    "    if thread and thread.is_alive():\n",
    "        thread.join(timeout=3)\n",
    "\n",
    "    SERVER_STATE.update({\"server\": None, \"thread\": None})\n",
    "    print(\"ðŸ›‘ Server stopped.\")\n",
    "\n",
    "\n",
    "def ensure_pkce_authorized(client_id: str, redirect_uri: str, scope: str, cache_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Ensure PKCE flow is completed.\n",
    "    If a valid token is already cached, nothing happens.\n",
    "    Otherwise:\n",
    "      - starts a local Flask server on 127.0.0.1:<PORT>/callback\n",
    "      - opens the Spotify consent page\n",
    "      - exchanges the 'code' and stores tokens in cache\n",
    "      - blocks until the token is available or timeout\n",
    "    \"\"\"\n",
    "    auth = build_spotify_pkce(\n",
    "        client_id=client_id,\n",
    "        redirect_uri=redirect_uri,\n",
    "        scope=scope,\n",
    "        cache_path=cache_path,\n",
    "        open_browser=False,\n",
    "    )\n",
    "\n",
    "    # If token already cached, skip auth\n",
    "    try:\n",
    "        token = auth.get_cached_token()\n",
    "        if token and token.get(\"access_token\"):\n",
    "            return\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    @app.route(\"/callback\")\n",
    "    def callback():\n",
    "        code = request.args.get(\"code\")\n",
    "        if not code:\n",
    "            return \"Missing authorization code.\", 400\n",
    "        # Complete token exchange and persist to cache\n",
    "        auth.get_access_token(code, check_cache=False)\n",
    "        return \"Auth OK. You can close this tab.\"\n",
    "\n",
    "    # Start local receiver with a stoppable server and open consent page\n",
    "    t = threading.Thread(target=run_server, args=(app,), daemon=True)\n",
    "    t.start()\n",
    "    SERVER_STATE[\"thread\"] = t\n",
    "\n",
    "    auth_url = auth.get_authorize_url()\n",
    "    webbrowser.open_new_tab(auth_url)\n",
    "    print(f\"Open this URL if the browser did not pop up:\\n{auth_url}\")\n",
    "    print(f\"Waiting for callback on http://127.0.0.1:{LOCAL_CALLBACK_PORT}/callback ...\")\n",
    "\n",
    "    # Block until token is present (or timeout)\n",
    "    ok = wait_for_token(auth, timeout=300, interval=1.0)\n",
    "\n",
    "    # Always stop the local server to free the port\n",
    "    stop_server()\n",
    "\n",
    "    if not ok:\n",
    "        raise TimeoutError(\"PKCE authorization timed out. Try again or check redirect/callback setup.\")\n",
    "\n",
    "\n",
    "def run_import(client_id: str, redirect_uri: str, scope: str, cache_path: str) -> None:\n",
    "    \"\"\"Fetch saved tracks from Spotify and persist them using selected backend.\"\"\"\n",
    "    # Build a Spotify client using the same config used for PKCE cache\n",
    "    sp = get_spotify_client(\n",
    "        client_id=client_id,\n",
    "        redirect_uri=redirect_uri,\n",
    "        scope=scope,\n",
    "        cache_path=cache_path,\n",
    "        open_browser=False,   # no need; token already cached\n",
    "    )\n",
    "\n",
    "    conn = create_connection()\n",
    "    ensure_database_and_tables(conn)\n",
    "\n",
    "    count = 0\n",
    "    for row in iter_saved_tracks(sp, page_limit=50):\n",
    "        a = row[\"artist\"]\n",
    "        al = row[\"album\"]\n",
    "        t = row[\"track\"]\n",
    "\n",
    "        upsert_artist(conn, a[\"spotify_id\"], a[\"name\"])\n",
    "        upsert_album(\n",
    "            conn,\n",
    "            al[\"spotify_id\"],\n",
    "            al[\"name\"],\n",
    "            al[\"artist_spotify_id\"],\n",
    "            al[\"image_url\"],\n",
    "            al[\"image_height\"],\n",
    "            al[\"image_width\"],\n",
    "        )\n",
    "        upsert_track(\n",
    "            conn,\n",
    "            t[\"spotify_id\"],\n",
    "            t[\"name\"],\n",
    "            t[\"popularity\"],\n",
    "            t[\"href\"],\n",
    "            t[\"artist_spotify_id\"],\n",
    "            t[\"album_spotify_id\"],\n",
    "        )\n",
    "        count += 1\n",
    "\n",
    "    conn.close()\n",
    "    print(f\"Imported or updated {count} tracks.\")\n",
    "\n",
    "# =========================\n",
    "# Entry point\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Authorizing...\")\n",
    "\n",
    "    # 1) Complete PKCE (opens browser if cache is empty, waits until token is stored)\n",
    "    ensure_pkce_authorized(\n",
    "        client_id=CLIENT_ID,\n",
    "        redirect_uri=REDIRECT_URI,\n",
    "        scope=SCOPE,\n",
    "        cache_path=CACHE_PATH,\n",
    "    )\n",
    "\n",
    "    print(\"Start to import tracks\")\n",
    "\n",
    "    # 2) Run the import using the cached token\n",
    "    run_import(\n",
    "        client_id=CLIENT_ID,\n",
    "        redirect_uri=REDIRECT_URI,\n",
    "        scope=SCOPE,\n",
    "        cache_path=CACHE_PATH,\n",
    "    )\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authorizing...\n",
      "Open this URL if the browser did not pop up:\n",
      "https://accounts.spotify.com/authorize?client_id=ec6c43ef03034d7393a4906cd1df5f06&response_type=code&redirect_uri=https%3A%2F%2Fspotify-auth.viant.dev%2Fcallback&code_challenge_method=S256&code_challenge=riEaLZ0dtNY6ejcew5Dsa2hqccX_rvRgsakgm5T7Uzs&scope=user-library-read\n",
      "Waiting for callback on http://127.0.0.1:8080/callback ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [06/Sep/2025 20:01:02] \"GET /callback?code=AQCkvtuK3dkJrJl6e5a3K5ztPfY-1wWfpNFyIrbV-62taOLSEugvOG8ZPMoD_gnd78QD0299F26gsquddZ4qygaYrkhBlIcN0RBF76JJBjJkaAVHDzF_bAcv5svn6gQoQNDRQLzZFARM-zBvLI4b2N_Z2is9fJVKxFfzg0A_swyDeeSR0AuudJJrwqvXzz-lsdVlEncI4uNaSaEtXL5XzHOOJZAhvTJMVk3x0lZc7kkrS3AfNHUDdRww6TKuk5PdMcm7jIQER9t3L52XCNMgBFg HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ›‘ Server stopped.\n",
      "Start to import tracks\n",
      "Imported or updated 911 tracks.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "c9d5ca9b-40fd-4f69-af51-d26f7efb2ead",
   "metadata": {},
   "source": [
    "# Keywords Extraction\n",
    "\n",
    "With our Spotify data loaded, we proceed to extract our playlist (currently limited to only 1,000 songs, but it will still give us a broad view of what we listen to). Weâ€™ll do this through an LLM. Remember to add your API key to your .env file under OPENAI_API_KEY. This process may take several minutes to complete.\n",
    "\n",
    "What we did in this step was use AI to generate keywords from the song title and name. Ideally, this should be done with the lyrics, but since there is no public source for them, AI can help us instead.\n",
    "\n",
    "If you want to use the lyrics for a more accurate analysis, use the `extract_from_lyrics` function inside `extract_keywords`. To do so, you must have populated the database with the lyrics in the `lyrics` field of the `Tracks` table.\n",
    "\n",
    "### Cost Estimate\n",
    "\n",
    "The average cost of processing 1,000 records in OpenAI is approximately **$0.40 USD or less**, depending on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6be49-af66-4ff2-8403-d573115997db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract_keywords.extract_from_title_artist as keywords\n",
    "\n",
    "keywords.extract_keywords_from_title_artist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02ca1acf1acbd8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sentimental Analysis for Spotify Playlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13e00b7b6669067",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ceb4914f0472db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import libraries.sqlite as db\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ba37bb36eb2f6",
   "metadata": {},
   "source": [
    "We retrieve the data from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55477bcc298240ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911\n"
     ]
    }
   ],
   "source": [
    "conn = db.create_connection()\n",
    "tracks = db.fetch_tracks_dataset(conn)\n",
    "conn.close()\n",
    "print(len(tracks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bdb1aad8f43d7c",
   "metadata": {},
   "source": [
    "We build the dataframe to better analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fb6f055b2c3b32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_spotify_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00AoRZ8103mVeOVfpTfGuR</td>\n",
       "      <td>Luny Tunes</td>\n",
       "      <td>Mayor Que Yo 3</td>\n",
       "      <td>[romantic tension, reggaeton vibe, urban night...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00BuKLSAFkaEkaVAgIMbeA</td>\n",
       "      <td>Lady Gaga</td>\n",
       "      <td>Telephone</td>\n",
       "      <td>[communication, urgency, escape, party, connec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00i0O74dXdaKKdCrqHnfXm</td>\n",
       "      <td>Ricky Martin</td>\n",
       "      <td>La Mordidita (feat. Yotuel)</td>\n",
       "      <td>[passion, dance, seduction, energy, rhythm, La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00iQcGMeC6agUvBjHdkAAM</td>\n",
       "      <td>La Oreja de Van Gogh</td>\n",
       "      <td>Un Cuento Sobre el Agua</td>\n",
       "      <td>[nostalgia, reflection, melancholy, love, memo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>017PF4Q3l4DBUiWoXk4OWT</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>Break My Heart</td>\n",
       "      <td>[heartbreak, love, vulnerability, emotional pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>01YedSX5OYtSCWdO1DRhvY</td>\n",
       "      <td>La Oreja de Van Gogh</td>\n",
       "      <td>20 de Enero</td>\n",
       "      <td>[love, memory, nostalgia, heartbreak, reflecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>01uqI4H13Gsd8Lyl1EYd8H</td>\n",
       "      <td>Macklemore &amp; Ryan Lewis</td>\n",
       "      <td>Same Love (feat. Mary Lambert)</td>\n",
       "      <td>[equality, love, acceptance, social justice, L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>02JIdsrod3BYucThfUFDUX</td>\n",
       "      <td>Camille</td>\n",
       "      <td>Le Festin</td>\n",
       "      <td>[hope, dreams, perseverance, joy, inspiration,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>02XnQdf7sipaKBBHixz3Zp</td>\n",
       "      <td>Lady Gaga</td>\n",
       "      <td>Paparazzi</td>\n",
       "      <td>[obsession, fame, media, love, danger, pop, da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>02bKaAG61tMw9c63fzKXal</td>\n",
       "      <td>Alex Ubago</td>\n",
       "      <td>Sin miedo a nada (feat. Amaia Montero)</td>\n",
       "      <td>[fearless, love, courage, hope, commitment, ro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         track_spotify_id              artist_name  \\\n",
       "0  00AoRZ8103mVeOVfpTfGuR               Luny Tunes   \n",
       "1  00BuKLSAFkaEkaVAgIMbeA                Lady Gaga   \n",
       "2  00i0O74dXdaKKdCrqHnfXm             Ricky Martin   \n",
       "3  00iQcGMeC6agUvBjHdkAAM     La Oreja de Van Gogh   \n",
       "4  017PF4Q3l4DBUiWoXk4OWT                 Dua Lipa   \n",
       "5  01YedSX5OYtSCWdO1DRhvY     La Oreja de Van Gogh   \n",
       "6  01uqI4H13Gsd8Lyl1EYd8H  Macklemore & Ryan Lewis   \n",
       "7  02JIdsrod3BYucThfUFDUX                  Camille   \n",
       "8  02XnQdf7sipaKBBHixz3Zp                Lady Gaga   \n",
       "9  02bKaAG61tMw9c63fzKXal               Alex Ubago   \n",
       "\n",
       "                                    title  \\\n",
       "0                          Mayor Que Yo 3   \n",
       "1                               Telephone   \n",
       "2             La Mordidita (feat. Yotuel)   \n",
       "3                 Un Cuento Sobre el Agua   \n",
       "4                          Break My Heart   \n",
       "5                             20 de Enero   \n",
       "6          Same Love (feat. Mary Lambert)   \n",
       "7                               Le Festin   \n",
       "8                               Paparazzi   \n",
       "9  Sin miedo a nada (feat. Amaia Montero)   \n",
       "\n",
       "                                            keywords  \n",
       "0  [romantic tension, reggaeton vibe, urban night...  \n",
       "1  [communication, urgency, escape, party, connec...  \n",
       "2  [passion, dance, seduction, energy, rhythm, La...  \n",
       "3  [nostalgia, reflection, melancholy, love, memo...  \n",
       "4  [heartbreak, love, vulnerability, emotional pa...  \n",
       "5  [love, memory, nostalgia, heartbreak, reflecti...  \n",
       "6  [equality, love, acceptance, social justice, L...  \n",
       "7  [hope, dreams, perseverance, joy, inspiration,...  \n",
       "8  [obsession, fame, media, love, danger, pop, da...  \n",
       "9  [fearless, love, courage, hope, commitment, ro...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tracks, columns=[\"track_spotify_id\", \"artist_name\", \"title\", \"keywords\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac2f74d64dca02d",
   "metadata": {},
   "source": [
    "Now we are going to create a dictionary based on the main keywords to normalize them. Using embeddings, we can adjust the threshold to make the group larger or smallerâ€”the lower the number, the larger the group. In this case, I set it to 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6969abc933c80007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique keywords: 1637\n",
      "Example of normalized (First 20):\n",
      "1960s -> 70s\n",
      "1970s -> 70s\n",
      "1970s style -> 70s\n",
      "1970s vibe -> 70s\n",
      "70s -> 70s\n",
      "1980s -> 80s\n",
      "1980s vibe -> 80s\n",
      "80s -> 80s\n",
      "80s pop -> 80s\n",
      "80s pop rock -> 80s\n",
      "80s rock -> 80s\n",
      "80s style -> 80s\n",
      "80s vibe -> 80s\n",
      "90s -> 80s\n",
      "90s alternative -> 80s\n",
      "90s vibe -> 80s\n",
      "90s-2000s vibe -> 80s\n",
      "1980s influence -> 80s influence\n",
      "80s influence -> 80s influence\n",
      "2000s -> 2000s\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# ========= 1) Create a unique keywords list =========\n",
    "all_kws = list(itertools.chain.from_iterable(df['keywords']))\n",
    "unique_kws = sorted({\n",
    "    \" \".join(str(k).lower().split())\n",
    "    for k in all_kws\n",
    "    if isinstance(k, str) and k.strip()\n",
    "})\n",
    "\n",
    "print(f\"Total unique keywords: {len(unique_kws)}\")\n",
    "\n",
    "# ========= 2) Embeddings =========\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "emb = model.encode(unique_kws, normalize_embeddings=True)\n",
    "\n",
    "# ========= 3) Hierarchical cluster =========\n",
    "clu = AgglomerativeClustering(\n",
    "    n_clusters=None,\n",
    "    metric='cosine',\n",
    "    linkage='average',\n",
    "    distance_threshold=0.4  # ajusta el threshold: mÃ¡s pequeÃ±o = clusters mÃ¡s finos\n",
    ")\n",
    "labels = clu.fit_predict(emb)\n",
    "\n",
    "# ========= 4) Build the clusters =========\n",
    "clusters = {}\n",
    "for kw, lab in zip(unique_kws, labels):\n",
    "    clusters.setdefault(lab, []).append(kw)\n",
    "\n",
    "\n",
    "# ========= 5) Choose canonical representative per cluster =========\n",
    "def pick_rep(words):\n",
    "    # heurÃ­stica: palabra con menos tokens, si empata, la mÃ¡s corta\n",
    "    return sorted(words, key=lambda w: (len(w.split()), len(w)))[0]\n",
    "\n",
    "\n",
    "suggested_norm = {}\n",
    "for words in clusters.values():\n",
    "    rep = pick_rep(words)\n",
    "    for w in words:\n",
    "        suggested_norm[w] = rep\n",
    "\n",
    "# ========= 6) Save suggested dictionary =========\n",
    "with open(\"keyword_norm_suggested.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(suggested_norm, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Example of normalized (First 20):\")\n",
    "for k, v in list(suggested_norm.items())[:20]:\n",
    "    print(f\"{k} -> {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda478309ec2fd91",
   "metadata": {},
   "source": [
    "Count unique normalized keywords (the canonical representatives). If the number of suggested words is considered optimal compared to the number of unique words, we can proceed; otherwise, we adjust the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a50d812caeddd830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1637 unique words - suggested words 845\n"
     ]
    }
   ],
   "source": [
    "unique_norm = set(suggested_norm.values())\n",
    "print(f\"{len(unique_kws)} unique words - suggested words {len(unique_norm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2f133bfdef488",
   "metadata": {},
   "source": [
    "Keeps the original 'keywords' column and adds a new column\n",
    "    'normalized_keywords' with normalized keywords as a list (unique, no duplicates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96f3f424d0e697c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>keywords</th>\n",
       "      <th>normalized_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mayor Que Yo 3</td>\n",
       "      <td>Luny Tunes</td>\n",
       "      <td>[romantic tension, reggaeton vibe, urban night...</td>\n",
       "      <td>[modern, party, romance, flirty, interaction, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Telephone</td>\n",
       "      <td>Lady Gaga</td>\n",
       "      <td>[communication, urgency, escape, party, connec...</td>\n",
       "      <td>[escape, electro, modern, vibrant, party, rela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La Mordidita (feat. Yotuel)</td>\n",
       "      <td>Ricky Martin</td>\n",
       "      <td>[passion, dance, seduction, energy, rhythm, La...</td>\n",
       "      <td>[fun, vibrant, love, party, flirty, nightlife,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Un Cuento Sobre el Agua</td>\n",
       "      <td>La Oreja de Van Gogh</td>\n",
       "      <td>[nostalgia, reflection, melancholy, love, memo...</td>\n",
       "      <td>[melody, love, rain imagery, romance, moment, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Break My Heart</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>[heartbreak, love, vulnerability, emotional pa...</td>\n",
       "      <td>[modern, love, regret, relationship, romance, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title           artist_name  \\\n",
       "0               Mayor Que Yo 3            Luny Tunes   \n",
       "1                    Telephone             Lady Gaga   \n",
       "2  La Mordidita (feat. Yotuel)          Ricky Martin   \n",
       "3      Un Cuento Sobre el Agua  La Oreja de Van Gogh   \n",
       "4               Break My Heart              Dua Lipa   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [romantic tension, reggaeton vibe, urban night...   \n",
       "1  [communication, urgency, escape, party, connec...   \n",
       "2  [passion, dance, seduction, energy, rhythm, La...   \n",
       "3  [nostalgia, reflection, melancholy, love, memo...   \n",
       "4  [heartbreak, love, vulnerability, emotional pa...   \n",
       "\n",
       "                                 normalized_keywords  \n",
       "0  [modern, party, romance, flirty, interaction, ...  \n",
       "1  [escape, electro, modern, vibrant, party, rela...  \n",
       "2  [fun, vibrant, love, party, flirty, nightlife,...  \n",
       "3  [melody, love, rain imagery, romance, moment, ...  \n",
       "4  [modern, love, regret, relationship, romance, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_normalization_array(df: pd.DataFrame, norm_dict: dict) -> pd.DataFrame:\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"normalized_keywords\"] = df_copy[\"keywords\"].apply(\n",
    "        lambda kws: list({\n",
    "            norm_dict.get(str(k).lower().strip(), str(k).lower().strip())\n",
    "            for k in kws if isinstance(k, str) and k.strip()\n",
    "        })\n",
    "    )\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "df_norm = apply_normalization_array(df, suggested_norm)\n",
    "df_norm[[\"title\", \"artist_name\", \"keywords\", \"normalized_keywords\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae06a4663fe5cdd",
   "metadata": {},
   "source": [
    "We can see the top keywords, and if we want, we can go back to the dictionary creation step to re-normalize with another threshold (this is the key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87646202ca7e1f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emotion</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>passion</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mood</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>energy</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>romance</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pop</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rhythm</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>desire</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>melody</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dance</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>expressive</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>reflection</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nostalgia</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>relationship</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>breakup</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>introspection</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>vocals</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hope</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tempo</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          keyword  count\n",
       "0         emotion    642\n",
       "1         passion    459\n",
       "2            mood    450\n",
       "3          energy    441\n",
       "4            love    432\n",
       "5         romance    427\n",
       "6             pop    409\n",
       "7          rhythm    389\n",
       "8          desire    380\n",
       "9          melody    348\n",
       "10          dance    336\n",
       "11     expressive    319\n",
       "12     reflection    316\n",
       "13      nostalgia    314\n",
       "14   relationship    311\n",
       "15        breakup    308\n",
       "16  introspection    303\n",
       "17         vocals    295\n",
       "18           hope    293\n",
       "19          tempo    291"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def top_normalized_keywords(df: pd.DataFrame, top_n: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns the top N most frequent normalized keywords across all tracks.\n",
    "    \"\"\"\n",
    "    all_kws = list(itertools.chain.from_iterable(df[\"normalized_keywords\"]))\n",
    "    counter = Counter(all_kws)\n",
    "    top = counter.most_common(top_n)\n",
    "    return pd.DataFrame(top, columns=[\"keyword\", \"count\"])\n",
    "\n",
    "\n",
    "top_keywords_df = top_normalized_keywords(df_norm, top_n=20)\n",
    "top_keywords_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c36eb8fe4850fa0",
   "metadata": {},
   "source": [
    "To simplify the process, we will save the DataFrame with normalized keywords into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1c48764be4b8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.to_csv(\"tracks_with_normalized_keywords.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a4ceeb2d93deec",
   "metadata": {},
   "source": [
    "We also save our normalized keywords in the database to ensure persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ede7ad30806bc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'normalized_keywords' already exists.\n",
      "911 rows updated.\n"
     ]
    }
   ],
   "source": [
    "conn = db.create_connection()\n",
    "db.ensure_normalized_keywords_column(conn)\n",
    "db.update_normalized_keywords(conn, df_norm)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f087764a035b14ba",
   "metadata": {},
   "source": [
    "## Sentimental Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420cecf9206795e6",
   "metadata": {},
   "source": [
    "Utilities functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b4bd61f322fb18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:56:04.999545Z",
     "start_time": "2025-08-29T14:56:04.981321Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "import json\n",
    "import ast\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import libraries.sqlite as db\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def load_df_if_needed(df: pd.DataFrame | None = None,\n",
    "                      csv_path: str = \"tracks_with_normalized_keywords.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame ready for processing.\n",
    "    If df is None OR df does not contain 'normalized_keywords', tries to load from CSV.\n",
    "    \"\"\"\n",
    "    if df is None or \"normalized_keywords\" not in df.columns:\n",
    "        if not os.path.exists(csv_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"DataFrame is missing and '{csv_path}' was not found. \"\n",
    "                \"Provide a DataFrame with columns: track_spotify_id, artist_name, title, keywords (list).\"\n",
    "            )\n",
    "        df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def ensure_keywords_list(df: pd.DataFrame, col: str = \"keywords\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robustly converts the column `col` into a Python list[str] per row.\n",
    "    Handles:\n",
    "      - JSON lists: [\"love\",\"party\"]\n",
    "      - Python repr lists: ['love', 'party']\n",
    "      - Double-escaped JSON (e.g., '\"[\\\"love\\\",\\\"party\\\"]\"')\n",
    "      - Comma-separated fallbacks: love, party\n",
    "      - NaN/None â†’ []\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        raise KeyError(f\"Column '{col}' not found in DataFrame.\")\n",
    "\n",
    "    def _strip_outer_quotes(s: str) -> str:\n",
    "        # Remove a single pair of wrapping quotes if present: '\"[...]' or \"'[...]'\"\n",
    "        if len(s) >= 2 and ((s[0] == s[-1] == '\"') or (s[0] == s[-1] == \"'\")):\n",
    "            return s[1:-1]\n",
    "        return s\n",
    "\n",
    "    def _parse(x):\n",
    "        # Already a list\n",
    "        if isinstance(x, list):\n",
    "            return [str(t).strip() for t in x if isinstance(t, (str, int, float)) and str(t).strip()]\n",
    "\n",
    "        # Missing\n",
    "        if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "            return []\n",
    "\n",
    "        # String-like\n",
    "        if isinstance(x, str):\n",
    "            s = x.strip()\n",
    "            if not s:\n",
    "                return []\n",
    "\n",
    "            # Try to un-wrap if double-escaped: e.g., '\"[\\\"love\\\",\\\"party\\\"]\"'\n",
    "            s_unwrapped = _strip_outer_quotes(s)\n",
    "\n",
    "            # 1) Try JSON\n",
    "            for candidate in (s, s_unwrapped):\n",
    "                if candidate.startswith(\"[\") and candidate.endswith(\"]\"):\n",
    "                    try:\n",
    "                        v = json.loads(candidate)\n",
    "                        if isinstance(v, list):\n",
    "                            return [str(t).strip() for t in v if str(t).strip()]\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            # 2) Try Python literal (handles single quotes lists)\n",
    "            for candidate in (s, s_unwrapped):\n",
    "                if candidate.startswith(\"[\") and candidate.endswith(\"]\"):\n",
    "                    try:\n",
    "                        v = ast.literal_eval(candidate)\n",
    "                        if isinstance(v, list):\n",
    "                            return [str(t).strip() for t in v if str(t).strip()]\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            # 3) Fallback: comma-separated\n",
    "            parts = [p.strip().strip(\"'\").strip('\"') for p in s.split(\",\")]\n",
    "            return [p for p in parts if p]\n",
    "\n",
    "        # Anything else â†’ try stringify\n",
    "        try:\n",
    "            s = str(x).strip()\n",
    "            if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "                v = ast.literal_eval(s)\n",
    "                if isinstance(v, list):\n",
    "                    return [str(t).strip() for t in v if str(t).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "        return []\n",
    "\n",
    "    out = df.copy()\n",
    "    out[col] = out[col].apply(_parse)\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Emotion analysis (model-based, no lexicon)\n",
    "# -------------------------\n",
    "# Load a pre-trained emotion classifier (GoEmotions fine-tuned DistilRoBERTa)\n",
    "# 1) Multi-label emotion pipeline (GoEmotions)\n",
    "_emotion_pipe = None\n",
    "\n",
    "\n",
    "def get_emotion_pipeline_multilabel():\n",
    "    \"\"\"\n",
    "    Multi-label emotion classifier based on GoEmotions.\n",
    "    Uses sigmoid (not softmax) under the hood.\n",
    "    \"\"\"\n",
    "    global _emotion_pipe\n",
    "    if _emotion_pipe is None:\n",
    "        _emotion_pipe = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"joeddav/distilbert-base-uncased-go-emotions-student\",\n",
    "            return_all_scores=True,\n",
    "            top_k=None,  # return all labels with scores\n",
    "            truncation=True\n",
    "        )\n",
    "    return _emotion_pipe\n",
    "\n",
    "\n",
    "# 2) Utility: classify a list of keywords IN BATCH, then aggregate per label\n",
    "def analyze_emotion_from_keywords_multilabel(\n",
    "        keywords: list[str],\n",
    "        min_score_threshold: float = 0.15,  # keep labels with mean score >= threshold\n",
    "        top_k: int = 5,  # return top-k emotions after threshold\n",
    "        batch_size: int = 32,\n",
    "        max_keywords: int | None = 100  # cap to avoid very long batches\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Multi-label emotion analysis:\n",
    "      - Classifies each keyword separately (batched).\n",
    "      - Averages scores per emotion across keywords.\n",
    "      - Filters by threshold and returns top_k labels.\n",
    "\n",
    "    Returns a list of dicts: [{\"label\": \"...\", \"score\": 0.xx}, ...]\n",
    "    \"\"\"\n",
    "    if not keywords:\n",
    "        return []\n",
    "\n",
    "    # Clean and cap keywords\n",
    "    kws = [str(k).strip() for k in keywords if isinstance(k, str) and str(k).strip()]\n",
    "    if not kws:\n",
    "        return []\n",
    "    if max_keywords is not None:\n",
    "        kws = kws[:max_keywords]\n",
    "\n",
    "    pipe = get_emotion_pipeline_multilabel()\n",
    "\n",
    "    # Batch inference\n",
    "    all_scores = pipe(kws, batch_size=batch_size)  # list of list[{\"label\",\"score\"}]\n",
    "\n",
    "    # Aggregate scores per label (mean over keywords)\n",
    "    # Initialize label space from the first result\n",
    "    if not all_scores or not all_scores[0]:\n",
    "        return []\n",
    "\n",
    "    label_scores = defaultdict(list)\n",
    "    labels = [d[\"label\"] for d in all_scores[0]]\n",
    "    for per_kw in all_scores:\n",
    "        for d in per_kw:\n",
    "            label_scores[d[\"label\"]].append(float(d[\"score\"]))\n",
    "\n",
    "    mean_scores = {lab: (sum(vals) / len(vals)) for lab, vals in label_scores.items()}\n",
    "\n",
    "    # Threshold and sort\n",
    "    filtered = [{\"label\": lab, \"score\": sc} for lab, sc in mean_scores.items() if sc >= min_score_threshold]\n",
    "    filtered.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    # Take top_k (if threshold filters too much and nothing remains, fall back to top_k without threshold)\n",
    "    if not filtered:\n",
    "        fallback = [{\"label\": lab, \"score\": sc} for lab, sc in mean_scores.items()]\n",
    "        fallback.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return fallback[:top_k]\n",
    "    return filtered[:top_k]\n",
    "\n",
    "\n",
    "# 3) Attach emotions to your DataFrame using the ORIGINAL 'keywords' column\n",
    "def attach_emotions_multilabel(df: pd.DataFrame, top_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds 'emotions' column with the aggregated multi-label results\n",
    "    computed from the 'keywords' column for each row.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    df_out[\"emotions\"] = df_out[\"keywords\"].apply(\n",
    "        lambda kws: analyze_emotion_from_keywords_multilabel(\n",
    "            kws,\n",
    "            min_score_threshold=0.15,\n",
    "            top_k=top_k\n",
    "        )\n",
    "    )\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec384ffd9db2104c",
   "metadata": {},
   "source": [
    "We are going to perform sentiment analysis. To do this, weâ€™ll use a pre-trained model: joeddav/distilbert-base-uncased-go-emotions-student. This model will help us extract the emotions from our keywords. We can define how many emotions we want to retrieve by setting the top_k parameter in our attach_emotions_multilabel function. Depending on this value, we can decide how many emotions to visualize and how deep we want the analysis to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d128a88da989dc14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:56:18.548829Z",
     "start_time": "2025-08-29T14:56:07.393557Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_spotify_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>title</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00AoRZ8103mVeOVfpTfGuR</td>\n",
       "      <td>Luny Tunes</td>\n",
       "      <td>Mayor Que Yo 3</td>\n",
       "      <td>[{'label': 'excitement', 'score': 0.1183473470...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00BuKLSAFkaEkaVAgIMbeA</td>\n",
       "      <td>Lady Gaga</td>\n",
       "      <td>Telephone</td>\n",
       "      <td>[{'label': 'realization', 'score': 0.078663845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00i0O74dXdaKKdCrqHnfXm</td>\n",
       "      <td>Ricky Martin</td>\n",
       "      <td>La Mordidita (feat. Yotuel)</td>\n",
       "      <td>[{'label': 'excitement', 'score': 0.1284560971...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00iQcGMeC6agUvBjHdkAAM</td>\n",
       "      <td>La Oreja de Van Gogh</td>\n",
       "      <td>Un Cuento Sobre el Agua</td>\n",
       "      <td>[{'label': 'caring', 'score': 0.09976261936128...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>017PF4Q3l4DBUiWoXk4OWT</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>Break My Heart</td>\n",
       "      <td>[{'label': 'desire', 'score': 0.09165769649669...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>01YedSX5OYtSCWdO1DRhvY</td>\n",
       "      <td>La Oreja de Van Gogh</td>\n",
       "      <td>20 de Enero</td>\n",
       "      <td>[{'label': 'caring', 'score': 0.09090986732393...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>01uqI4H13Gsd8Lyl1EYd8H</td>\n",
       "      <td>Macklemore &amp; Ryan Lewis</td>\n",
       "      <td>Same Love (feat. Mary Lambert)</td>\n",
       "      <td>[{'label': 'realization', 'score': 0.136109698...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>02JIdsrod3BYucThfUFDUX</td>\n",
       "      <td>Camille</td>\n",
       "      <td>Le Festin</td>\n",
       "      <td>[{'label': 'optimism', 'score': 0.102420230284...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>02XnQdf7sipaKBBHixz3Zp</td>\n",
       "      <td>Lady Gaga</td>\n",
       "      <td>Paparazzi</td>\n",
       "      <td>[{'label': 'excitement', 'score': 0.0882992495...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>02bKaAG61tMw9c63fzKXal</td>\n",
       "      <td>Alex Ubago</td>\n",
       "      <td>Sin miedo a nada (feat. Amaia Montero)</td>\n",
       "      <td>[{'label': 'caring', 'score': 0.14290625721216...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         track_spotify_id              artist_name  \\\n",
       "0  00AoRZ8103mVeOVfpTfGuR               Luny Tunes   \n",
       "1  00BuKLSAFkaEkaVAgIMbeA                Lady Gaga   \n",
       "2  00i0O74dXdaKKdCrqHnfXm             Ricky Martin   \n",
       "3  00iQcGMeC6agUvBjHdkAAM     La Oreja de Van Gogh   \n",
       "4  017PF4Q3l4DBUiWoXk4OWT                 Dua Lipa   \n",
       "5  01YedSX5OYtSCWdO1DRhvY     La Oreja de Van Gogh   \n",
       "6  01uqI4H13Gsd8Lyl1EYd8H  Macklemore & Ryan Lewis   \n",
       "7  02JIdsrod3BYucThfUFDUX                  Camille   \n",
       "8  02XnQdf7sipaKBBHixz3Zp                Lady Gaga   \n",
       "9  02bKaAG61tMw9c63fzKXal               Alex Ubago   \n",
       "\n",
       "                                    title  \\\n",
       "0                          Mayor Que Yo 3   \n",
       "1                               Telephone   \n",
       "2             La Mordidita (feat. Yotuel)   \n",
       "3                 Un Cuento Sobre el Agua   \n",
       "4                          Break My Heart   \n",
       "5                             20 de Enero   \n",
       "6          Same Love (feat. Mary Lambert)   \n",
       "7                               Le Festin   \n",
       "8                               Paparazzi   \n",
       "9  Sin miedo a nada (feat. Amaia Montero)   \n",
       "\n",
       "                                            emotions  \n",
       "0  [{'label': 'excitement', 'score': 0.1183473470...  \n",
       "1  [{'label': 'realization', 'score': 0.078663845...  \n",
       "2  [{'label': 'excitement', 'score': 0.1284560971...  \n",
       "3  [{'label': 'caring', 'score': 0.09976261936128...  \n",
       "4  [{'label': 'desire', 'score': 0.09165769649669...  \n",
       "5  [{'label': 'caring', 'score': 0.09090986732393...  \n",
       "6  [{'label': 'realization', 'score': 0.136109698...  \n",
       "7  [{'label': 'optimism', 'score': 0.102420230284...  \n",
       "8  [{'label': 'excitement', 'score': 0.0882992495...  \n",
       "9  [{'label': 'caring', 'score': 0.14290625721216...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you already have a DataFrame named `df`, pass it directly to attach_emotions(df, top_k=3).\n",
    "# Otherwise, load from CSV (expects at least: track_spotify_id, artist_name, title, keywords):\n",
    "try:\n",
    "    df = load_df_if_needed(df=None, csv_path=\"tracks_with_normalized_keywords.csv\")\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    raise\n",
    "\n",
    "# Ensure keywords are lists, then attach emotions\n",
    "df = ensure_keywords_list(df, col=\"keywords\")\n",
    "df = attach_emotions_multilabel(df, top_k=28)  # Top K depends on how many emotions the dataset can describe.\n",
    "\n",
    "# Inspect result (emotions column contains top-3 emotions with scores per track)\n",
    "df[[\"track_spotify_id\", \"artist_name\", \"title\", \"emotions\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791fbe4274e1f5ad",
   "metadata": {},
   "source": [
    "We verify what we obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47c5678daa6a3846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:56:24.048280Z",
     "start_time": "2025-08-29T14:56:24.042284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"label\": \"excitement\",\n",
      "    \"score\": 0.11834734700620174\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"desire\",\n",
      "    \"score\": 0.11584901049733162\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"realization\",\n",
      "    \"score\": 0.06584796600043774\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"curiosity\",\n",
      "    \"score\": 0.056092655062675474\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"pride\",\n",
      "    \"score\": 0.04987131379544735\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"caring\",\n",
      "    \"score\": 0.04905091166496277\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"joy\",\n",
      "    \"score\": 0.04893150057643652\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"approval\",\n",
      "    \"score\": 0.045935837030410764\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"amusement\",\n",
      "    \"score\": 0.04559797925874591\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"love\",\n",
      "    \"score\": 0.04034377865493297\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"relief\",\n",
      "    \"score\": 0.03395199995487928\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"optimism\",\n",
      "    \"score\": 0.032062261253595355\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"admiration\",\n",
      "    \"score\": 0.03173048753291369\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"surprise\",\n",
      "    \"score\": 0.028245389647781848\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"nervousness\",\n",
      "    \"score\": 0.02772440867498517\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"anger\",\n",
      "    \"score\": 0.023773623518645762\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"confusion\",\n",
      "    \"score\": 0.023211246244609355\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"annoyance\",\n",
      "    \"score\": 0.022599574867635964\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"neutral\",\n",
      "    \"score\": 0.021813798658549786\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"remorse\",\n",
      "    \"score\": 0.017274447213858367\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"embarrassment\",\n",
      "    \"score\": 0.016199860293418168\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"gratitude\",\n",
      "    \"score\": 0.014982777517288923\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"disapproval\",\n",
      "    \"score\": 0.014538814779371023\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"fear\",\n",
      "    \"score\": 0.01356489971280098\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"grief\",\n",
      "    \"score\": 0.012934787813574076\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"disgust\",\n",
      "    \"score\": 0.011203878419473767\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"sadness\",\n",
      "    \"score\": 0.010017019286751748\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"disappointment\",\n",
      "    \"score\": 0.008302435353398323\n",
      "  }\n",
      "]\n",
      "[\n",
      "  {\n",
      "    \"label\": \"realization\",\n",
      "    \"score\": 0.07866384570176403\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"excitement\",\n",
      "    \"score\": 0.07137863931711763\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"caring\",\n",
      "    \"score\": 0.063702977320645\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"nervousness\",\n",
      "    \"score\": 0.04975703082163818\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"relief\",\n",
      "    \"score\": 0.04889440852760648\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"desire\",\n",
      "    \"score\": 0.04744359202838192\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"joy\",\n",
      "    \"score\": 0.04709285170732377\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"curiosity\",\n",
      "    \"score\": 0.042218314638982214\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"amusement\",\n",
      "    \"score\": 0.042052595594820254\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"annoyance\",\n",
      "    \"score\": 0.0411691565241199\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"pride\",\n",
      "    \"score\": 0.04061927545505265\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"optimism\",\n",
      "    \"score\": 0.03894861958300074\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"anger\",\n",
      "    \"score\": 0.03834470818401314\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"approval\",\n",
      "    \"score\": 0.03658842285706972\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"confusion\",\n",
      "    \"score\": 0.03106284779884542\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"love\",\n",
      "    \"score\": 0.029925755312433466\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"surprise\",\n",
      "    \"score\": 0.027716655322971444\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"disapproval\",\n",
      "    \"score\": 0.02641218500987937\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"embarrassment\",\n",
      "    \"score\": 0.024831849354086444\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"remorse\",\n",
      "    \"score\": 0.02343522778634603\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"neutral\",\n",
      "    \"score\": 0.022962018886270624\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"admiration\",\n",
      "    \"score\": 0.021868234558496624\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"gratitude\",\n",
      "    \"score\": 0.02143762056948617\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"fear\",\n",
      "    \"score\": 0.0185180239204783\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"disgust\",\n",
      "    \"score\": 0.01805095258168876\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"grief\",\n",
      "    \"score\": 0.017203716687314834\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"disappointment\",\n",
      "    \"score\": 0.015057882255253693\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"sadness\",\n",
      "    \"score\": 0.014642597292549908\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Get the first non-empty emotions entry\n",
    "first_non_empty = df.loc[df[\"emotions\"].map(lambda x: isinstance(x, list) and len(x) > 0), \"emotions\"].iloc[0]\n",
    "second = df.loc[df[\"emotions\"].map(lambda x: isinstance(x, list) and len(x) > 0), \"emotions\"].iloc[1]\n",
    "\n",
    "# Pretty print as JSON for readability\n",
    "print(json.dumps(first_non_empty, indent=2))\n",
    "print(json.dumps(second, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fec9a24d7eadf9",
   "metadata": {},
   "source": [
    "We will denormalize, create an appropriate table, and insert the extracted emotions for later analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14974d3ff603ca30",
   "metadata": {},
   "source": [
    "1. Discover unique emotions from df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe1c98a2e5882808",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:56:29.019529Z",
     "start_time": "2025-08-29T14:56:29.009788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 emotions discovered\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def all_emotions_from_df(df):\n",
    "    \"\"\"\n",
    "    Returns a sorted list of unique emotion labels present in df['emotions'].\n",
    "    \"\"\"\n",
    "    labels = set()\n",
    "    for lst in df[\"emotions\"]:\n",
    "        if isinstance(lst, list):\n",
    "            for d in lst:\n",
    "                lab = str(d.get(\"label\", \"\")).strip().lower()\n",
    "                if lab:\n",
    "                    labels.add(lab)\n",
    "    return sorted(labels)\n",
    "\n",
    "\n",
    "def to_snake(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Safe snake_case for MySQL column names.\n",
    "    \"\"\"\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    return re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "\n",
    "\n",
    "emotions = all_emotions_from_df(df)\n",
    "emotion_cols = [to_snake(e) for e in emotions]\n",
    "print(len(emotion_cols), \"emotions discovered\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de815c27553bac6f",
   "metadata": {},
   "source": [
    " 2. Build DDL for a wide, denormalized table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b34c46d10e65c83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T15:09:02.874068Z",
     "start_time": "2025-08-29T15:09:02.871353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example: run once in MySQL\n",
    "table_name = \"track_emotions_wide\"\n",
    "ddl = db.build_create_table_sql(table_name, emotion_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3586c74a471f13",
   "metadata": {},
   "source": [
    "3. Pivot df to wide (percentages per emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1adcf0aa383f27e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T15:10:13.559458Z",
     "start_time": "2025-08-29T15:10:13.487038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 100.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_spotify_id</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>neutral</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00AoRZ8103mVeOVfpTfGuR</td>\n",
       "      <td>3.1730</td>\n",
       "      <td>4.5598</td>\n",
       "      <td>2.3774</td>\n",
       "      <td>2.2600</td>\n",
       "      <td>4.5936</td>\n",
       "      <td>4.9051</td>\n",
       "      <td>2.3211</td>\n",
       "      <td>5.6093</td>\n",
       "      <td>11.5849</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0344</td>\n",
       "      <td>2.7724</td>\n",
       "      <td>2.1814</td>\n",
       "      <td>3.2062</td>\n",
       "      <td>4.9871</td>\n",
       "      <td>6.5848</td>\n",
       "      <td>3.3952</td>\n",
       "      <td>1.7274</td>\n",
       "      <td>1.0017</td>\n",
       "      <td>2.8245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00BuKLSAFkaEkaVAgIMbeA</td>\n",
       "      <td>2.1868</td>\n",
       "      <td>4.2053</td>\n",
       "      <td>3.8345</td>\n",
       "      <td>4.1169</td>\n",
       "      <td>3.6588</td>\n",
       "      <td>6.3703</td>\n",
       "      <td>3.1063</td>\n",
       "      <td>4.2218</td>\n",
       "      <td>4.7444</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9926</td>\n",
       "      <td>4.9757</td>\n",
       "      <td>2.2962</td>\n",
       "      <td>3.8949</td>\n",
       "      <td>4.0619</td>\n",
       "      <td>7.8664</td>\n",
       "      <td>4.8894</td>\n",
       "      <td>2.3435</td>\n",
       "      <td>1.4643</td>\n",
       "      <td>2.7717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00i0O74dXdaKKdCrqHnfXm</td>\n",
       "      <td>3.0423</td>\n",
       "      <td>6.3857</td>\n",
       "      <td>1.1765</td>\n",
       "      <td>1.5285</td>\n",
       "      <td>4.3693</td>\n",
       "      <td>5.3163</td>\n",
       "      <td>1.6337</td>\n",
       "      <td>4.5355</td>\n",
       "      <td>8.1992</td>\n",
       "      <td>...</td>\n",
       "      <td>4.1455</td>\n",
       "      <td>1.5365</td>\n",
       "      <td>1.8562</td>\n",
       "      <td>7.4870</td>\n",
       "      <td>5.0033</td>\n",
       "      <td>5.1344</td>\n",
       "      <td>3.4660</td>\n",
       "      <td>1.1589</td>\n",
       "      <td>0.6413</td>\n",
       "      <td>2.7813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00iQcGMeC6agUvBjHdkAAM</td>\n",
       "      <td>3.2848</td>\n",
       "      <td>3.6995</td>\n",
       "      <td>1.1481</td>\n",
       "      <td>1.5090</td>\n",
       "      <td>4.0445</td>\n",
       "      <td>9.9763</td>\n",
       "      <td>1.6346</td>\n",
       "      <td>5.2694</td>\n",
       "      <td>7.3965</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9853</td>\n",
       "      <td>1.9932</td>\n",
       "      <td>2.5723</td>\n",
       "      <td>4.2105</td>\n",
       "      <td>3.6533</td>\n",
       "      <td>6.7260</td>\n",
       "      <td>6.6287</td>\n",
       "      <td>3.2235</td>\n",
       "      <td>4.3334</td>\n",
       "      <td>2.3926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>017PF4Q3l4DBUiWoXk4OWT</td>\n",
       "      <td>2.0447</td>\n",
       "      <td>3.3372</td>\n",
       "      <td>1.7681</td>\n",
       "      <td>1.9308</td>\n",
       "      <td>3.1180</td>\n",
       "      <td>5.5102</td>\n",
       "      <td>4.2814</td>\n",
       "      <td>4.4483</td>\n",
       "      <td>9.1658</td>\n",
       "      <td>...</td>\n",
       "      <td>4.8037</td>\n",
       "      <td>2.3928</td>\n",
       "      <td>1.5849</td>\n",
       "      <td>5.3296</td>\n",
       "      <td>3.2392</td>\n",
       "      <td>5.0693</td>\n",
       "      <td>2.8545</td>\n",
       "      <td>4.4973</td>\n",
       "      <td>5.9266</td>\n",
       "      <td>2.2279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         track_spotify_id  admiration  amusement   anger  annoyance  approval  \\\n",
       "0  00AoRZ8103mVeOVfpTfGuR      3.1730     4.5598  2.3774     2.2600    4.5936   \n",
       "1  00BuKLSAFkaEkaVAgIMbeA      2.1868     4.2053  3.8345     4.1169    3.6588   \n",
       "2  00i0O74dXdaKKdCrqHnfXm      3.0423     6.3857  1.1765     1.5285    4.3693   \n",
       "3  00iQcGMeC6agUvBjHdkAAM      3.2848     3.6995  1.1481     1.5090    4.0445   \n",
       "4  017PF4Q3l4DBUiWoXk4OWT      2.0447     3.3372  1.7681     1.9308    3.1180   \n",
       "\n",
       "   caring  confusion  curiosity   desire  ...    love  nervousness  neutral  \\\n",
       "0  4.9051     2.3211     5.6093  11.5849  ...  4.0344       2.7724   2.1814   \n",
       "1  6.3703     3.1063     4.2218   4.7444  ...  2.9926       4.9757   2.2962   \n",
       "2  5.3163     1.6337     4.5355   8.1992  ...  4.1455       1.5365   1.8562   \n",
       "3  9.9763     1.6346     5.2694   7.3965  ...  5.9853       1.9932   2.5723   \n",
       "4  5.5102     4.2814     4.4483   9.1658  ...  4.8037       2.3928   1.5849   \n",
       "\n",
       "   optimism   pride  realization  relief  remorse  sadness  surprise  \n",
       "0    3.2062  4.9871       6.5848  3.3952   1.7274   1.0017    2.8245  \n",
       "1    3.8949  4.0619       7.8664  4.8894   2.3435   1.4643    2.7717  \n",
       "2    7.4870  5.0033       5.1344  3.4660   1.1589   0.6413    2.7813  \n",
       "3    4.2105  3.6533       6.7260  6.6287   3.2235   4.3334    2.3926  \n",
       "4    5.3296  3.2392       5.0693  2.8545   4.4973   5.9266    2.2279  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def df_to_emotion_wide(df: pd.DataFrame, emotions: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a wide DataFrame with one column per emotion (percentage 0â€“100),\n",
    "    filling 0 where the emotion is absent.\n",
    "    \"\"\"\n",
    "    cols = [\"track_spotify_id\"] + [to_snake(e) for e in emotions]\n",
    "    out_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        base = {c: 0.0 for c in cols}\n",
    "        base[\"track_spotify_id\"] = str(row[\"track_spotify_id\"]).strip()\n",
    "        lst = row[\"emotions\"] if isinstance(row[\"emotions\"], list) else []\n",
    "        for d in lst:\n",
    "            lab = str(d.get(\"label\", \"\")).strip().lower()\n",
    "            score = round(float(d.get(\"score\", 0.0) * 100), 4)\n",
    "            col = to_snake(lab)\n",
    "            if col in base:\n",
    "                val = score  # store as percentage\n",
    "                # if multiple entries for the same label, keep max\n",
    "                base[col] = max(base[col], val)\n",
    "        out_rows.append(base)\n",
    "    wide = pd.DataFrame(out_rows, columns=cols)\n",
    "    return wide\n",
    "\n",
    "\n",
    "wide_df = df_to_emotion_wide(df, emotions)\n",
    "print(f\"Value: {wide_df.iloc[0, 1:].sum()}\")  # The value needs to be ~= 100\n",
    "wide_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a7977cf34c58c",
   "metadata": {},
   "source": [
    "4. Bulk upsert into MySQL (denormalized table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "460edef314387e9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T15:10:16.804589Z",
     "start_time": "2025-08-29T15:10:16.711432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDL ejecutado con Ã©xito.\n"
     ]
    }
   ],
   "source": [
    "conn = db.create_connection()\n",
    "try:\n",
    "    db.truncate_track_emotions_wide(conn)\n",
    "except:\n",
    "    print(\"Tabla no truncada\")\n",
    "\n",
    "db.execute_DDL(conn,ddl)\n",
    "db.upsert_emotion_wide(conn, table_name, wide_df)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211dc6ce6b3f6b23",
   "metadata": {},
   "source": [
    "We store the extracted emotions in the track table of the database for added reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ccc51b3368adc10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T15:10:19.811696Z",
     "start_time": "2025-08-29T15:10:19.645549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated emotions for 911 tracks.\n"
     ]
    }
   ],
   "source": [
    "conn = db.create_connection()\n",
    "db.update_track_emotions(conn, df)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22024bc6681c482",
   "metadata": {},
   "source": [
    "## Emotion Dictionary Creation\n",
    "\n",
    "In this step, we will **extract the emotion columns from `wide_df`**, use the language model (LLM) to classify them as **Positive, Neutral, or Negative**, and also assign a representative **emoji**.\n",
    "\n",
    "With this information, we will build an **emotion dictionary** and store it in the database in the `emotions_dictionary` table (creating or truncating it beforehand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2d73d44a5238040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:59:19.750293Z",
     "start_time": "2025-08-29T14:59:19.744701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OPENAI_API_KEY found: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carga las variables del archivo .env en el entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Ahora puedes verificar que la clave estÃ¡ disponible\n",
    "print(\"âœ… OPENAI_API_KEY found:\", bool(os.getenv(\"OPENAI_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c49e6918bc4d9d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:59:22.795701Z",
     "start_time": "2025-08-29T14:59:22.753884Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Emotions Dictionary: wide_df -> LLM -> DB\n",
    "# =========================\n",
    "# Requisitos:\n",
    "#   - pip install openai python-dotenv pymysql\n",
    "#   - .env con OPENAI_API_KEY=<tu_key>\n",
    "#   - objeto 'db' con db.create_connection() -> PyMySQL connection\n",
    "#   - DataFrame 'wide_df' ya en memoria\n",
    "#\n",
    "# QuÃ© hace:\n",
    "#   1) Extrae emociones desde wide_df.columns (excluye 'track_spotify_id')\n",
    "#   2) Pide al LLM: emotion -> {emotion, normalize (Positive|Neutral|Negative), emoji}\n",
    "#   3) Crea/TRUNCATE emotions_dictionary\n",
    "#   4) Inserta mapping en MySQL\n",
    "#   5) Devuelve el mapping\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import libraries.sqlite as db\n",
    "\n",
    "# 1) Cargar .env (si existe)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 2) Cliente OpenAI (lee OPENAI_API_KEY del entorno)\n",
    "from openai import OpenAI\n",
    "\n",
    "_client = OpenAI()  # si no existe la var, lanzarÃ¡ error al invocar\n",
    "\n",
    "\n",
    "# ---------- Utilidades ----------\n",
    "\n",
    "def _parse_json_lenient(text: str) -> Any:\n",
    "    \"\"\"\n",
    "    Intenta parsear JSON aunque el modelo devuelva texto extra o ```json fences.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        raise ValueError(\"Empty LLM response.\")\n",
    "    cleaned = text.strip()\n",
    "\n",
    "    # quitar fences ```json ... ```\n",
    "    cleaned = re.sub(r\"^```(?:json|JSON)?\\s*\", \"\", cleaned)\n",
    "    cleaned = re.sub(r\"\\s*```$\", \"\", cleaned)\n",
    "\n",
    "    # intento directo\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # buscar objeto {...}\n",
    "    s, e = cleaned.find(\"{\"), cleaned.rfind(\"}\")\n",
    "    if s != -1 and e != -1 and s < e:\n",
    "        try:\n",
    "            return json.loads(cleaned[s:e + 1])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # buscar array [...]\n",
    "    s, e = cleaned.find(\"[\"), cleaned.rfind(\"]\")\n",
    "    if s != -1 and e != -1 and s < e:\n",
    "        try:\n",
    "            return json.loads(cleaned[s:e + 1])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    raise ValueError(f\"Could not parse JSON from LLM output. Got:\\n{cleaned[:500]}\")\n",
    "\n",
    "\n",
    "def _canon_norm(label: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza etiquetas a Positive|Neutral|Negative (acepta inglÃ©s/espaÃ±ol/minÃºsculas).\n",
    "    \"\"\"\n",
    "    if not label:\n",
    "        return \"Neutral\"\n",
    "    l = label.strip().lower()\n",
    "    if l.startswith((\"pos\", \"poz\", \"positivo\")):\n",
    "        return \"Positive\"\n",
    "    if l.startswith((\"neu\", \"neutro\")):\n",
    "        return \"Neutral\"\n",
    "    if l.startswith((\"neg\", \"negativo\")):\n",
    "        return \"Negative\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "\n",
    "# ---------- LLM ----------\n",
    "\n",
    "def llm_classify_emotions_with_openai(\n",
    "        emotions: List[str],\n",
    "        model: str = \"gpt-4o-mini\",\n",
    "        temperature: float = 0.0,\n",
    "        max_retries: int = 3,\n",
    "        dry_run: bool = False,\n",
    "        debug_print: bool = False,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Pide al chat.completions un JSON con:\n",
    "    [ {emotion, normalize (FP:Full Positive|MP: Positive|N:Neutral|MN:Mid Negative|FN: Full Negative), emoji}, ... ]\n",
    "    Usa parser tolerante para extraer el JSON.\n",
    "    \"\"\"\n",
    "    if not emotions:\n",
    "        return []\n",
    "    if dry_run:\n",
    "        return [{\"emotion\": e, \"normalize\": \"N\", \"emoji\": \"â“\"} for e in emotions]\n",
    "\n",
    "    system_msg = '''\n",
    "    You are a precise JSON generator.\n",
    "    For each input emotion, return a JSON array of objects with the following keys:\n",
    "    - \"emotion\": the emotion name,\n",
    "    - \"normalize\": one of [\"FP\" (Full Positive), \"MP\" (Mid Positive), \"N\" (Neutral), \"MN\" (Mid Negative), \"FN\" (Full Negative)],\n",
    "    - \"emoji\": a single representative emoji.\n",
    "\n",
    "    Return ONLY a valid JSON array, with no explanations and no markdown.\n",
    "    '''\n",
    "    user_msg = \"Emotions:\\n\" + json.dumps(emotions, ensure_ascii=False)\n",
    "\n",
    "    last_text: Optional[str] = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = _client.chat.completions.create(\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_msg},\n",
    "                    {\"role\": \"user\", \"content\": user_msg},\n",
    "                ],\n",
    "                max_tokens=1200,\n",
    "            )\n",
    "            text = resp.choices[0].message.content\n",
    "            last_text = text\n",
    "            parsed = _parse_json_lenient(text)\n",
    "\n",
    "            # Acepta lista directa o objeto con \"result\"\n",
    "            if isinstance(parsed, dict) and \"result\" in parsed and isinstance(parsed[\"result\"], list):\n",
    "                items = parsed[\"result\"]\n",
    "            elif isinstance(parsed, list):\n",
    "                items = parsed\n",
    "            else:\n",
    "                raise ValueError(\"Parsed JSON is not a list nor an object with 'result'.\")\n",
    "\n",
    "            out: List[Dict[str, Any]] = []\n",
    "            for row in items:\n",
    "                if not isinstance(row, dict):\n",
    "                    continue\n",
    "                emo = str(row.get(\"emotion\", \"\")).strip()\n",
    "                norm = str(row.get(\"normalize\", \"\")).strip()\n",
    "                emoji = str(row.get(\"emoji\", \"\")).strip()\n",
    "                if emo and emoji:\n",
    "                    out.append({\"emotion\": emo, \"normalize\": norm, \"emoji\": emoji})\n",
    "\n",
    "            if not out:\n",
    "                raise ValueError(\"Empty mapping after validation.\")\n",
    "            return out\n",
    "\n",
    "        except Exception as e:\n",
    "            if debug_print and last_text:\n",
    "                print(\"LLM RAW OUTPUT (truncated):\\n\", last_text[:800])\n",
    "            if attempt == max_retries:\n",
    "                raise\n",
    "            time.sleep(0.7 * attempt)\n",
    "\n",
    "\n",
    "# ---------- OrquestaciÃ³n ----------\n",
    "\n",
    "def build_and_store_emotions_dictionary_with_llm(\n",
    "        wide_df,\n",
    "        llm_model: str = \"gpt-4o-mini\",\n",
    "        temperature: float = 0.0,\n",
    "        dry_run_llm: bool = False,\n",
    "        verbose: bool = False,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    1) Toma columnas de wide_df (excluye 'track_spotify_id') => emociones.\n",
    "    2) Llama LLM para clasificar y asignar emoji.\n",
    "    3) Crea/TRUNCATE emotions_dictionary y guarda.\n",
    "    4) Devuelve la lista final insertada.\n",
    "    \"\"\"\n",
    "    # 1) Extraer nombres de emociones desde los headers del DF\n",
    "    all_cols = list(wide_df.columns)\n",
    "    emotions_in_df = [c for c in all_cols if c.lower() != \"track_spotify_id\"]\n",
    "    if verbose:\n",
    "        print(\"Emotions from wide_df:\", emotions_in_df)\n",
    "\n",
    "    if not emotions_in_df:\n",
    "        raise ValueError(\"No emotion columns found in wide_df (other than 'track_spotify_id').\")\n",
    "\n",
    "    # 2) LLM -> mapping\n",
    "    mapping = llm_classify_emotions_with_openai(\n",
    "        emotions=emotions_in_df,\n",
    "        model=llm_model,\n",
    "        temperature=temperature,\n",
    "        dry_run=dry_run_llm,\n",
    "        debug_print=verbose,\n",
    "    )\n",
    "\n",
    "    # 3) Ajustar a headers exactos (por si el LLM cambia mayÃºsculas/minÃºsculas)\n",
    "    by_lower_original = {c.lower(): c for c in emotions_in_df}\n",
    "    cleaned: List[Dict[str, Any]] = []\n",
    "    for item in mapping:\n",
    "        key = item[\"emotion\"].strip().lower()\n",
    "        if key in by_lower_original:\n",
    "            cleaned.append({\n",
    "                \"emotion\": by_lower_original[key],\n",
    "                \"normalize\": item[\"normalize\"],\n",
    "                \"emoji\": item[\"emoji\"]\n",
    "            })\n",
    "\n",
    "    if not cleaned:\n",
    "        raise ValueError(\"LLM produced no valid emotions present in wide_df.\")\n",
    "\n",
    "    # 4) Persistir en DB\n",
    "    db.create_and_truncate_emotions_dictionary()\n",
    "    db.insert_emotions_dictionary(cleaned)\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d56048994145fe97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:59:34.998927Z",
     "start_time": "2025-08-29T14:59:25.311290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions from wide_df: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "result = build_and_store_emotions_dictionary_with_llm(\n",
    "    wide_df,\n",
    "    llm_model=\"gpt-4o-mini\",\n",
    "    temperature=0.2,\n",
    "    dry_run_llm=False,  # True para probar sin consumir API\n",
    "    verbose=True  # imprime las columnas/RAW si hay errores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "caa10932-eb8f-4b89-9d8d-1d80aaf6f792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 28 emotions into emotions_dictionary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'emotion': 'admiration', 'normalize': 'FP', 'emoji': 'ðŸ˜'},\n",
       " {'emotion': 'amusement', 'normalize': 'FP', 'emoji': 'ðŸ˜‚'},\n",
       " {'emotion': 'anger', 'normalize': 'FN', 'emoji': 'ðŸ˜ '},\n",
       " {'emotion': 'annoyance', 'normalize': 'MN', 'emoji': 'ðŸ˜’'},\n",
       " {'emotion': 'approval', 'normalize': 'FP', 'emoji': 'ðŸ‘'},\n",
       " {'emotion': 'caring', 'normalize': 'FP', 'emoji': 'â¤ï¸'},\n",
       " {'emotion': 'confusion', 'normalize': 'N', 'emoji': 'ðŸ˜•'},\n",
       " {'emotion': 'curiosity', 'normalize': 'MP', 'emoji': 'ðŸ¤”'},\n",
       " {'emotion': 'desire', 'normalize': 'FP', 'emoji': 'ðŸ˜'},\n",
       " {'emotion': 'disappointment', 'normalize': 'MN', 'emoji': 'ðŸ˜ž'},\n",
       " {'emotion': 'disapproval', 'normalize': 'MN', 'emoji': 'ðŸ‘Ž'},\n",
       " {'emotion': 'disgust', 'normalize': 'FN', 'emoji': 'ðŸ¤¢'},\n",
       " {'emotion': 'embarrassment', 'normalize': 'MN', 'emoji': 'ðŸ˜³'},\n",
       " {'emotion': 'excitement', 'normalize': 'FP', 'emoji': 'ðŸŽ‰'},\n",
       " {'emotion': 'fear', 'normalize': 'FN', 'emoji': 'ðŸ˜¨'},\n",
       " {'emotion': 'gratitude', 'normalize': 'FP', 'emoji': 'ðŸ™'},\n",
       " {'emotion': 'grief', 'normalize': 'FN', 'emoji': 'ðŸ˜¢'},\n",
       " {'emotion': 'joy', 'normalize': 'FP', 'emoji': 'ðŸ˜Š'},\n",
       " {'emotion': 'love', 'normalize': 'FP', 'emoji': 'â¤ï¸'},\n",
       " {'emotion': 'nervousness', 'normalize': 'MN', 'emoji': 'ðŸ˜¬'},\n",
       " {'emotion': 'neutral', 'normalize': 'N', 'emoji': 'ðŸ˜'},\n",
       " {'emotion': 'optimism', 'normalize': 'FP', 'emoji': 'ðŸŒŸ'},\n",
       " {'emotion': 'pride', 'normalize': 'FP', 'emoji': 'ðŸ˜Œ'},\n",
       " {'emotion': 'realization', 'normalize': 'MP', 'emoji': 'ðŸ’¡'},\n",
       " {'emotion': 'relief', 'normalize': 'FP', 'emoji': 'ðŸ˜Œ'},\n",
       " {'emotion': 'remorse', 'normalize': 'MN', 'emoji': 'ðŸ˜”'},\n",
       " {'emotion': 'sadness', 'normalize': 'FN', 'emoji': 'ðŸ˜¢'},\n",
       " {'emotion': 'surprise', 'normalize': 'MP', 'emoji': 'ðŸ˜²'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Inserted {len(result)} emotions into emotions_dictionary\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e94e97a729b0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CLustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17efe6b1209b4ed8",
   "metadata": {},
   "source": [
    "We will now apply data clustering to generate groups that will serve as playlists, built around key aspects of related songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "684e9ba0afaa40b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, ast\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def ensure_list_column(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensures column `col` is a Python list[str].\n",
    "    Handles:\n",
    "      - Python repr lists with single quotes: \"['a', 'b']\"\n",
    "      - JSON lists: [\"a\",\"b\"]\n",
    "      - Comma-separated fallback: a, b\n",
    "    \"\"\"\n",
    "\n",
    "    def _parse(x):\n",
    "        if isinstance(x, list):\n",
    "            return [str(t).strip() for t in x if str(t).strip()]\n",
    "        if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "            return []\n",
    "        if isinstance(x, str):\n",
    "            s = x.strip()\n",
    "            if not s:\n",
    "                return []\n",
    "            # 1) Try Python literal (handles single quotes)\n",
    "            if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "                try:\n",
    "                    v = ast.literal_eval(s)\n",
    "                    if isinstance(v, list):\n",
    "                        return [str(t).strip() for t in v if str(t).strip()]\n",
    "                except Exception:\n",
    "                    pass\n",
    "                # 2) Try JSON (double quotes)\n",
    "                try:\n",
    "                    v = json.loads(s)\n",
    "                    if isinstance(v, list):\n",
    "                        return [str(t).strip() for t in v if str(t).strip()]\n",
    "                except Exception:\n",
    "                    pass\n",
    "                # 3) Fallback: split inner by comma\n",
    "                inner = s[1:-1]\n",
    "                parts = [p.strip().strip(\"'\").strip('\"') for p in inner.split(\",\")]\n",
    "                return [p for p in parts if p]\n",
    "            # 4) Plain comma-separated\n",
    "            return [t.strip() for t in s.split(\",\") if t.strip()]\n",
    "        # last resort\n",
    "        return [str(x).strip()] if str(x).strip() else []\n",
    "\n",
    "    out = df.copy()\n",
    "    out[col] = out[col].apply(_parse)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bff17ca6adc25f8",
   "metadata": {},
   "source": [
    "We check that the normalized_keywords column is of type list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8eeb9d1a9236ae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> ['modern', 'party', 'romance', 'flirty', 'interaction']\n"
     ]
    }
   ],
   "source": [
    "# Convert the column from string to list[str]\n",
    "df = ensure_list_column(df, col=\"normalized_keywords\")\n",
    "\n",
    "# Sanity check\n",
    "print(type(df.loc[df.index[0], \"normalized_keywords\"]), df.loc[df.index[0], \"normalized_keywords\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836fe2fb4fec140",
   "metadata": {},
   "source": [
    "This script implements an end-to-end pipeline to group songs into **thematic playlists** using **semantic keyword embeddings** and clustering with **KMeans**.\n",
    "\n",
    "1. Build embeddings per track\n",
    "- Each song has a set of *normalized keywords*.\n",
    "- Using a *Sentence Transformers* model (`all-MiniLM-L6-v2`), vector embeddings are generated for those keywords.\n",
    "- A *mean pooling* operation (averaging) is applied to obtain a single dense vector representing the entire track.\n",
    "\n",
    "2. Clustering with KMeans\n",
    "- The embeddings of all tracks are clustered into *k* groups using KMeans.\n",
    "- The **Silhouette Score** is computed to quickly evaluate clustering quality.\n",
    "\n",
    "3. Attach clusters back to the DataFrame\n",
    "- A new column `cluster_emb` is added to the original DataFrame, indicating the cluster assignment for each track.\n",
    "\n",
    "4. Cluster interpretability\n",
    "- The most frequent keywords per cluster are extracted, producing a top-N list that summarizes the common themes of each group.\n",
    "- Representative â€œArtist â€” Titleâ€ examples are sampled for each cluster, making it easier to inspect them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1244512e690bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette: 0.08390728384256363\n",
      "\n",
      "Cluster 0 top keywords:\n",
      "passion(7), defiance(7), identity(6), protest(6), emotion(6), culture(6), city(6), rebellion(6), energy(6), empowerment(6), rap(5), freedom(5), equality(5), bold(5), resistance(5), rhythm(5), struggle(5), conflict(5), resilience(5), critique(5)\n",
      "\n",
      "Cluster 1 top keywords:\n",
      "breakup(43), emotion(40), relationship(38), mood(35), conflict(35), vulnerable(34), drama(32), passion(31), pop(28), reflection(23), betrayal(23), desire(22), introspection(22), intense(20), regret(20), vocals(19), hurt(18), love(18), melody(18), expressive(17)\n",
      "\n",
      "Cluster 2 top keywords:\n",
      "romance(39), joy(37), energy(37), dance(37), pop(36), summer(33), fun(30), rhythm(30), love(29), melody(27), vibrant(27), celebration(26), upbeat(25), mood(24), cold(22), youthful(22), lighthearted(20), passion(19), catchy(19), playful(19)\n",
      "\n",
      "Cluster 3 top keywords:\n",
      "energy(42), confident(42), empowerment(37), bold(35), pop(34), dance(32), rhythm(28), freedom(26), attitude(26), catchy(25), modern(24), party(24), rebellion(24), vibrant(23), independent(23), city(22), assertive(21), strong(20), upbeat(19), fun(18)\n",
      "\n",
      "Cluster 4 top keywords:\n",
      "mood(56), introspection(52), emotion(50), dark(43), rock(41), reflection(40), struggle(39), conflict(38), vocals(32), hope(27), energy(26), guitar(25), city(25), atmosphere(24), moody(23), intense(22), vulnerable(21), night(20), rhythm(19), feeling(17)\n",
      "\n",
      "Cluster 5 top keywords:\n",
      "energy(27), rhythm(25), dynamic(21), passion(20), emotion(19), vibrant(18), dance(15), intensity(15), movement(12), beat(12), expressive(11), sensual(10), electro(9), nightlife(9), desire(9), tempo(9), bold(9), pop(9), love(8), exciting(8)\n",
      "\n",
      "Cluster 6 top keywords:\n",
      "emotion(80), reflection(76), nostalgia(64), mood(63), introspection(63), hope(63), melody(50), sentimental(49), love(47), dream(47), desire(46), vocals(42), romance(41), acoustic(40), pop(38), cold(38), heartfelt(34), narrative(34), gentle(31), memory(29)\n",
      "\n",
      "Cluster 7 top keywords:\n",
      "dance(44), energy(44), party(41), fun(40), confident(38), rhythm(38), bold(37), vibrant(34), nightlife(31), celebration(31), pop(30), club(29), beat(24), empowerment(21), dynamic(21), city(21), expressive(20), freedom(20), flirty(20), upbeat(19)\n",
      "\n",
      "Cluster 8 top keywords:\n",
      "emotion(118), mood(112), desire(111), breakup(105), introspection(104), nostalgia(104), reflection(103), sentimental(100), tempo(94), romance(90), regret(88), love(87), sadness(86), vulnerable(82), relationship(80), vocals(76), ballad(70), acoustic(69), pop(68), passion(67)\n",
      "\n",
      "Cluster 9 top keywords:\n",
      "romance(99), emotion(97), love(95), sensual(89), passion(86), sentimental(82), desire(73), relationship(72), cold(71), affection(68), ballad(67), tempo(66), expressive(64), mood(61), hope(59), heartfelt(58), tender(58), devotion(54), nostalgia(52), melody(51)\n",
      "\n",
      "Cluster 10 top keywords:\n",
      "hope(55), uplift(53), resilience(50), energy(50), determination(44), emotion(41), motivation(37), inspiration(35), strong(33), empowerment(30), optimism(28), passion(27), pop(26), dynamic(26), anthem(24), bravery(23), bold(22), freedom(22), growth(21), confident(20)\n",
      "\n",
      "Cluster 11 top keywords:\n",
      "passion(68), rhythm(67), romance(65), emotion(62), dance(62), desire(57), sensual(57), love(54), energy(48), expressive(43), nightlife(42), melody(40), cold(39), vibrant(37), relationship(35), reggae(33), beat(33), connection(31), city(29), flirty(28)\n",
      "\n",
      "Cluster 12 top keywords:\n",
      "party(64), rhythm(64), dance(64), energy(64), celebration(63), fun(56), vibrant(54), movement(53), festive(47), beat(47), club(44), upbeat(43), dynamic(40), nightlife(38), joy(37), melody(35), exciting(32), groove(32), summer(31), latin(28)\n",
      "\n",
      "Cluster 13 top keywords:\n",
      "energy(49), rebellion(44), conflict(43), dark(38), emotion(37), rock(33), anger(28), frustration(25), intensity(25), expressive(24), violent(24), teen(23), intense(23), struggle(22), vocals(21), chaos(19), tempo(19), distorted(19), mood(18), dynamic(18)\n",
      "\n",
      "Cluster 14 top keywords:\n",
      "emotion(57), pop(54), breakup(50), passion(50), relationship(47), dance(47), romance(45), love(41), catchy(41), energy(38), rhythm(38), melody(37), conflict(36), desire(32), mood(30), vulnerable(30), expressive(28), upbeat(26), modern(25), vocals(25)\n",
      "\n",
      "Cluster 0 examples:\n",
      " - Macklemore & Ryan Lewis â€” Same Love (feat. Mary Lambert)\n",
      " - Calle 13 â€” Los Idiotas\n",
      " - Idina Menzel â€” Take Me or Leave Me\n",
      " - Gloria Trevi â€” Hijoeputa\n",
      " - Common â€” Glory (From the Motion Picture Selma)\n",
      " - Willie ColÃ³n â€” El Gran Varon\n",
      " - Sole Gimenez â€” La mala reputaciÃ³n\n",
      " - Calle 13 â€” Que Lloren\n",
      " - Residente â€” La CÃ¡tedra\n",
      " - Molotov â€” Frijolero\n",
      "\n",
      "Cluster 1 examples:\n",
      " - My Chemical Romance â€” Cancer\n",
      " - Elefante â€” Mentirosa\n",
      " - KAROL G â€” 200 COPAS\n",
      " - Nickelback â€” How You Remind Me\n",
      " - No Doubt â€” Don't Speak\n",
      " - MÅ“nia â€” Ni TÃº Ni Nadie\n",
      " - Adele â€” Rolling in the Deep\n",
      " - Ashlee Simpson â€” Pieces Of Me\n",
      " - Maroon 5 â€” Payphone\n",
      " - PXNDX â€” MuÃ±eca\n",
      "\n",
      "Cluster 2 examples:\n",
      " - Dvicio â€” ParaÃ­so\n",
      " - Miranda! â€” Perfecta\n",
      " - Train â€” Hey, Soul Sister\n",
      " - Miranda! â€” El Profe\n",
      " - Morat â€” Yo Contigo, TÃº Conmigo - The Gong Gong Song / El Tema De La PelÃ­cula \"Gru 3 Mi Villano Favorito\"\n",
      " - blink-182 â€” All The Small Things\n",
      " - Bad Bunny â€” Un Verano Sin Ti\n",
      " - Bad Bunny â€” DespuÃ©s de la Playa\n",
      " - Carlos Vives â€” Fruta Fresca\n",
      " - Farruko â€” Chillax (feat. Ky-Mani Marley)\n",
      "\n",
      "Cluster 3 examples:\n",
      " - Lady Gaga â€” Telephone\n",
      " - Meghan Trainor â€” Walkashame\n",
      " - Jessie J â€” Do It Like A Dude\n",
      " - Meghan Trainor â€” NO\n",
      " - Celia Cruz â€” La Vida Es Un Carnaval\n",
      " - P!nk â€” Raise Your Glass\n",
      " - Lizzo â€” About Damn Time\n",
      " - Lil Nas X â€” MONTERO (Call Me By Your Name)\n",
      " - Paulina Rubio â€” Ni Rosas Ni Juguetes\n",
      " - Miley Cyrus â€” Midnight Sky\n",
      "\n",
      "Cluster 4 examples:\n",
      " - Evanescence â€” Going Under\n",
      " - Evanescence â€” Bring Me To Life\n",
      " - Omar CabÃ¡n -YuriFoX- â€” The World (Death Note)\n",
      " - Fito y Fitipaldis â€” Sobra la luz\n",
      " - Caramelos De Cianuro â€” Sanitarios\n",
      " - Keane â€” Everybody's Changing\n",
      " - Linkin Park â€” In The End\n",
      " - AURORA â€” The Seed\n",
      " - System Of A Down â€” Lonely Day\n",
      " - Sing Street â€” Brown Shoes\n",
      "\n",
      "Cluster 5 examples:\n",
      " - The Weeknd â€” Blinding Lights\n",
      " - David Guetta â€” Say My Name\n",
      " - Kings of Leon â€” Sex on Fire\n",
      " - DJ DOC â€” Run to You\n",
      " - Pabllo Vittar â€” AMEIANOITE\n",
      " - Caramelos De Cianuro â€” Las Estrellas - En Vivo\n",
      " - Aterciopelados â€” Baracunatana\n",
      " - Dua Lipa â€” Hallucinate\n",
      " - Lady Gaga â€” Americano\n",
      " - Estopa â€” Fuente de Energia\n",
      "\n",
      "Cluster 6 examples:\n",
      " - La Oreja de Van Gogh â€” Un Cuento Sobre el Agua\n",
      " - Camille â€” Le Festin\n",
      " - Bahiano â€” Tarde Gris\n",
      " - Meghan Trainor â€” 3am\n",
      " - Morat â€” En Un SÃ³lo Dia\n",
      " - The Covers Duo â€” Change (Bleach)\n",
      " - La Oreja de Van Gogh â€” Diciembre\n",
      " - The Covers Duo â€” Silhouette\n",
      " - Coldplay â€” Shiver\n",
      " - La Oreja de Van Gogh â€” Noche\n",
      "\n",
      "Cluster 7 examples:\n",
      " - Rihanna â€” S&M\n",
      " - Ed Sheeran â€” Shape of You\n",
      " - Miranda! â€” Don\n",
      " - Robin Thicke â€” Blurred Lines\n",
      " - TiÃ«sto â€” Don't Be Shy\n",
      " - Lady Gaga â€” LoveGame\n",
      " - Meghan Trainor â€” All About That Bass\n",
      " - Meghan Trainor â€” Title\n",
      " - Paulina Rubio â€” Boys Will Be Boys\n",
      " - Nicky Jam â€” Travesuras\n",
      "\n",
      "Cluster 8 examples:\n",
      " - La Oreja de Van Gogh â€” 20 de Enero\n",
      " - Charlie Puth â€” We Don't Talk Anymore (feat. Selena Gomez)\n",
      " - Fito y Fitipaldis â€” Deltoya\n",
      " - Shakira â€” Moscas en la Casa\n",
      " - Christina Perri â€” jar of hearts\n",
      " - PXNDX â€” Romance En Re Sostenido\n",
      " - La Oreja de Van Gogh â€” El Ultimo Vals\n",
      " - Morat â€” Idiota\n",
      " - Los Enanitos Verdes â€” Eterna Soledad\n",
      " - Dread Mar I â€” Tu Sin Mi\n",
      "\n",
      "Cluster 9 examples:\n",
      " - Alex Ubago â€” Sin miedo a nada (feat. Amaia Montero)\n",
      " - Tiziano Ferro â€” Ti Voglio Bene\n",
      " - Juanes â€” Nada Valgo Sin Tu Amor\n",
      " - Natalia JimÃ©nez â€” QuÃ©date Con Ella\n",
      " - Shakira â€” Dia de Enero\n",
      " - Felipe PelÃ¡ez â€” Vivo Pensando En Ti (feat. Maluma)\n",
      " - Reik â€” QuÃ© Vida la MÃ­a\n",
      " - La Oreja de Van Gogh â€” Vuelve\n",
      " - Aerosmith â€” I Don't Want to Miss a Thing - From the Touchstone film, \"Armageddon\"\n",
      " - Ha*Ash â€” EstÃ©s Donde EstÃ©s\n",
      "\n",
      "Cluster 10 examples:\n",
      " - Lady Gaga â€” Marry The Night\n",
      " - Fall Out Boy â€” Centuries\n",
      " - The Covers Duo â€” Soldier Dream\n",
      " - Imagine Dragons â€” Believer\n",
      " - La Oreja de Van Gogh â€” Mariposa\n",
      " - David Guetta â€” Titanium (feat. Sia)\n",
      " - The Covers Duo â€” Atraparlos Ya! (Pokemon)\n",
      " - Daniel Powter â€” Bad Day\n",
      " - PelleK â€” Pegasus Fantasy (Saint Seiya)\n",
      " - Miley Cyrus â€” Flowers\n",
      "\n",
      "Cluster 11 examples:\n",
      " - Luny Tunes â€” Mayor Que Yo 3\n",
      " - Shawn Mendes â€” SeÃ±orita\n",
      " - Calle 13 â€” No Hay Nadie Como TÃº (feat. CafÃ© Tacuba)\n",
      " - Jerry Rivera â€” Ese\n",
      " - Maluma â€” Felices los 4\n",
      " - Mon Laferte â€” AmÃ¡rrame\n",
      " - Adolescent's Orquesta â€” Persona Ideal\n",
      " - Shakira â€” Ciega, Sordomuda\n",
      " - Los AutÃ©nticos Decadentes â€” CorazÃ³n\n",
      " - Rihanna â€” Work\n",
      "\n",
      "Cluster 12 examples:\n",
      " - Ricky Martin â€” La Mordidita (feat. Yotuel)\n",
      " - Kevin Lyttle â€” Turn Me On\n",
      " - Gipsy Kings â€” Djobi, Djoba\n",
      " - ABBA â€” Voulez-Vous\n",
      " - Jerry Rivera â€” Vuela Muy Alto - Salsa Version\n",
      " - PSY â€” Gangnam Style (ê°•ë‚¨ìŠ¤íƒ€ì¼)\n",
      " - DJ Memo â€” Fotogenica\n",
      " - The Pointer Sisters â€” I'm So Excited\n",
      " - KAROL G â€” Pineapple\n",
      " - Sia â€” Cheap Thrills\n",
      "\n",
      "Cluster 13 examples:\n",
      " - CeeLo Green â€” Fuck You\n",
      " - Kudai â€” DÃ©jame Gritar\n",
      " - PXNDX â€” Tus Palabras Punzocortantes\n",
      " - My Chemical Romance â€” Mama\n",
      " - The Covers Duo â€” 99 (Mob Psycho 100)\n",
      " - Thirty Seconds To Mars â€” The Kill\n",
      " - System Of A Down â€” Violent Pornography\n",
      " - My Chemical Romance â€” Dead!\n",
      " - System Of A Down â€” Radio/Video\n",
      " - System Of A Down â€” B.Y.O.B.\n",
      "\n",
      "Cluster 14 examples:\n",
      " - Dua Lipa â€” Break My Heart\n",
      " - Lady Gaga â€” Paparazzi\n",
      " - Shakira â€” Can't Remember to Forget You\n",
      " - All Time Low â€” Dear Maria, Count Me In\n",
      " - PXNDX â€” Amiguito\n",
      " - Glee Cast â€” My Life Would Suck Without You\n",
      " - Lil Nas X â€” THATS WHAT I WANT\n",
      " - Shakira â€” What We Said (feat. MAGIC!) - Comme moi English Version\n",
      " - Taylor Swift â€” You Belong With Me\n",
      " - Gym Class Heroes â€” Stereo Hearts (feat. Adam Levine)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from collections import Counter, defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Build embeddings per track\n",
    "# ---------------------------\n",
    "def compute_track_embeddings(\n",
    "        df: pd.DataFrame,\n",
    "        keywords_col: str = \"normalized_keywords\",\n",
    "        model_name: str = \"all-MiniLM-L6-v2\",\n",
    "        normalize: bool = True,\n",
    "        max_keywords: int | None = None\n",
    ") -> tuple[np.ndarray, List[int], SentenceTransformer]:\n",
    "    \"\"\"\n",
    "    Computes a dense embedding per track by averaging the embeddings of its normalized keywords.\n",
    "    Returns:\n",
    "      - X: np.ndarray of shape (n_tracks, dim)\n",
    "      - valid_idx: indices of rows that have at least 1 keyword (used to subset df later)\n",
    "      - model: the SentenceTransformer model used\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    vectors = []\n",
    "    valid_idx = []\n",
    "\n",
    "    for i, kws in enumerate(df[keywords_col].tolist()):\n",
    "        if not isinstance(kws, list) or len(kws) == 0:\n",
    "            continue\n",
    "        toks = [str(k).strip() for k in kws if isinstance(k, str) and str(k).strip()]\n",
    "        if not toks:\n",
    "            continue\n",
    "        if max_keywords is not None:\n",
    "            toks = toks[:max_keywords]\n",
    "\n",
    "        kw_emb = model.encode(toks, normalize_embeddings=normalize)\n",
    "        if isinstance(kw_emb, list):\n",
    "            kw_emb = np.asarray(kw_emb)\n",
    "        track_vec = kw_emb.mean(axis=0)  # mean pooling over keywords\n",
    "        vectors.append(track_vec)\n",
    "        valid_idx.append(i)\n",
    "\n",
    "    if not vectors:\n",
    "        raise ValueError(\"No tracks produced embeddings. Check 'normalized_keywords' column.\")\n",
    "    X = np.vstack(vectors)\n",
    "    return X, valid_idx, model\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2) KMeans clustering\n",
    "# ---------------------------\n",
    "def cluster_tracks_embeddings(\n",
    "        X: np.ndarray,\n",
    "        k: int = 8,\n",
    "        random_state: int = 42\n",
    ") -> tuple[np.ndarray, KMeans, float]:\n",
    "    \"\"\"\n",
    "    Clusters the embedding matrix X with KMeans.\n",
    "    Returns:\n",
    "      - labels: cluster assignment per row in X\n",
    "      - kmeans: fitted model\n",
    "      - sil: silhouette score (for quick quality check)\n",
    "    \"\"\"\n",
    "    km = KMeans(n_clusters=k, random_state=random_state, n_init=\"auto\")\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels) if len(set(labels)) > 1 else np.nan\n",
    "    return labels, km, sil\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Attach clusters back to df\n",
    "# ---------------------------\n",
    "def attach_clusters_to_df(\n",
    "        df: pd.DataFrame,\n",
    "        labels: np.ndarray,\n",
    "        valid_idx: List[int],\n",
    "        cluster_col: str = \"cluster_emb\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds the cluster labels to a copy of df at the indices used for embeddings.\n",
    "    Rows without embeddings remain with NaN clusters.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    out[cluster_col] = np.nan\n",
    "    out.loc[out.index[valid_idx], cluster_col] = labels\n",
    "    if out[cluster_col].isna().any():\n",
    "        out[cluster_col] = out[cluster_col].astype(\"Int64\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Cluster summaries (human-readable)\n",
    "# ---------------------------\n",
    "def top_keywords_per_cluster(\n",
    "        df_with_clusters: pd.DataFrame,\n",
    "        cluster_col: str = \"cluster_emb\",\n",
    "        keywords_col: str = \"normalized_keywords\",\n",
    "        top_n: int = 12\n",
    ") -> dict[int, List[tuple[str, int]]]:\n",
    "    \"\"\"\n",
    "    For interpretability: returns top-N most frequent normalized keywords per cluster.\n",
    "    \"\"\"\n",
    "    summary = {}\n",
    "    for c in sorted(df_with_clusters[cluster_col].dropna().unique()):\n",
    "        sub = df_with_clusters[df_with_clusters[cluster_col] == c]\n",
    "        cnt = Counter()\n",
    "        for kws in sub[keywords_col]:\n",
    "            if isinstance(kws, list):\n",
    "                cnt.update([k for k in kws if isinstance(k, str) and k.strip()])\n",
    "        summary[int(c)] = cnt.most_common(top_n)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def sample_titles_per_cluster(\n",
    "        df_with_clusters: pd.DataFrame,\n",
    "        cluster_col: str = \"cluster_emb\",\n",
    "        n_samples: int = 5\n",
    ") -> dict[int, List[str]]:\n",
    "    \"\"\"\n",
    "    Returns up to n_samples \"Artist â€” Title\" examples per cluster.\n",
    "    \"\"\"\n",
    "    examples = {}\n",
    "    for c in sorted(df_with_clusters[cluster_col].dropna().unique()):\n",
    "        sub = df_with_clusters[df_with_clusters[cluster_col] == c].head(n_samples)\n",
    "        examples[int(c)] = [\n",
    "            f\"{row['artist_name']} â€” {row['title']}\"\n",
    "            for _, row in sub.iterrows()\n",
    "        ]\n",
    "    return examples\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 5) End-to-end usage\n",
    "# ---------------------------\n",
    "# df is your DataFrame with columns:\n",
    "# - track_spotify_id\n",
    "# - artist_name\n",
    "# - title\n",
    "# - normalized_keywords (list[str])\n",
    "\n",
    "# Build embeddings\n",
    "X, valid_idx, st_model = compute_track_embeddings(\n",
    "    df, keywords_col=\"normalized_keywords\", model_name=\"all-MiniLM-L6-v2\", normalize=True\n",
    ")\n",
    "\n",
    "# Cluster (choose k based on your dataset size; start with 6â€“12)\n",
    "labels, kmeans, sil = cluster_tracks_embeddings(X, k=15, random_state=42)\n",
    "print(\"Silhouette:\", sil)\n",
    "\n",
    "# Attach cluster labels back to df\n",
    "df_emb = attach_clusters_to_df(df, labels, valid_idx, cluster_col=\"cluster_emb\")\n",
    "\n",
    "# Inspect\n",
    "df_emb[[\"track_spotify_id\", \"artist_name\", \"title\", \"cluster_emb\"]].head(10)\n",
    "\n",
    "# Top keywords per cluster (for interpretation)\n",
    "cluster_keywords = top_keywords_per_cluster(df_emb, cluster_col=\"cluster_emb\", keywords_col=\"normalized_keywords\",\n",
    "                                            top_n=20)\n",
    "for cid, tops in cluster_keywords.items():\n",
    "    print(f\"\\nCluster {cid} top keywords:\")\n",
    "    print(\", \".join([f\"{w}({c})\" for w, c in tops]))\n",
    "\n",
    "# Optional: sample titles per cluster\n",
    "examples = sample_titles_per_cluster(df_emb, cluster_col=\"cluster_emb\", n_samples=10)\n",
    "for cid, ex in examples.items():\n",
    "    print(f\"\\nCluster {cid} examples:\")\n",
    "    for s in ex:\n",
    "        print(\" -\", s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d2b99da215242f",
   "metadata": {},
   "source": [
    "Here weâ€™ll do something interesting: we will send our clusters with their top keywords to the AI so that it can return a name and an engaging description, which we will use to build our playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4d028bf8f2b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import json, re\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "def build_cluster_payload(df_with_clusters: pd.DataFrame,\n",
    "                          cluster_col: str = \"cluster_emb\",\n",
    "                          keywords_col: str = \"normalized_keywords\",\n",
    "                          title_cols=(\"artist_name\", \"title\"),\n",
    "                          top_k_keywords: int = 15,\n",
    "                          max_examples: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Prepares a payload per cluster:\n",
    "      {cluster_id: {\"keywords\": [...], \"examples\": [\"Artist â€” Title\", ...]}}\n",
    "    \"\"\"\n",
    "    payload = {}\n",
    "    for cid in sorted(df_with_clusters[cluster_col].dropna().unique()):\n",
    "        sub = df_with_clusters[df_with_clusters[cluster_col] == cid]\n",
    "        # top keywords by simple frequency inside the cluster\n",
    "        kw_series = sub[keywords_col].explode()\n",
    "        kw_series = kw_series[kw_series.notna()].astype(str).str.strip().str.lower()\n",
    "        top = (kw_series.value_counts().head(top_k_keywords).index.tolist()\n",
    "               if not kw_series.empty else [])\n",
    "        payload[int(cid)] = {\"keywords\": top}\n",
    "    return payload\n",
    "\n",
    "\n",
    "\n",
    "def parse_json_maybe(s: str) -> dict:\n",
    "    # Normalize input\n",
    "    if s is None:\n",
    "        return {}\n",
    "    s = s.strip().lstrip(\"\\ufeff\")\n",
    "\n",
    "    # 1) Strip code fences if the whole thing is fence-wrapped\n",
    "    s_nofence = re.sub(r\"^\\s*```(?:json|js|javascript)?\\s*|\\s*```\\s*$\", \"\", s, flags=re.I|re.S)\n",
    "    try:\n",
    "        return json.loads(s_nofence)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Extract JSON from a fenced block if present\n",
    "    m = re.search(r\"```(?:json|js|javascript)?\\s*(\\{.*?\\})\\s*```\", s, flags=re.I|re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(1))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3) Raw-decode from first '{' that yields valid JSON\n",
    "    dec = json.JSONDecoder()\n",
    "    for i, ch in enumerate(s):\n",
    "        if ch == \"{\":\n",
    "            try:\n",
    "                obj, end = dec.raw_decode(s[i:])\n",
    "                return obj\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    # 4) Very last resort: grab a curly block and try fixing trailing commas\n",
    "    m2 = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "    if m2:\n",
    "        frag = m2.group(0)\n",
    "        # remove trailing commas before } or ]\n",
    "        frag = re.sub(r\",\\s*([}\\]])\", r\"\\1\", frag)\n",
    "        try:\n",
    "            return json.loads(frag)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return {}\n",
    "\n",
    "def name_clusters_with_llm(cluster_payload: dict,\n",
    "                           model: str = \"gpt-4.1-mini\",\n",
    "                           temperature: float = 0.2) -> dict:\n",
    "    \"\"\"\n",
    "    Sends cluster summaries to the LLM and returns:\n",
    "      {cluster_id: {\"name\": str, \"description\": str}}\n",
    "    Robust JSON parsing with fallback.\n",
    "    \"\"\"\n",
    "    # Build prompt\n",
    "    items = []\n",
    "    for cid, data in cluster_payload.items():\n",
    "        kws = \", \".join(data[\"keywords\"])\n",
    "        items.append(\n",
    "            f\"Cluster {cid}:\\n\"\n",
    "            f\"- Top keywords: {kws or '(none)'}\\n\"\n",
    "        )\n",
    "    clusters_block = \"\\n\\n\".join(items)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an expert music curator. Name each cluster of songs concisely.\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"- Provide a short, human-friendly name (â‰¤ 4 words) and a 1â€“2 sentence description.\\n\"\n",
    "        \"- Avoid artist names and proper nouns; focus on themes/moods.\\n\"\n",
    "        \"- Prefer genre/vibe/emotion terms (e.g., 'Dancefloor Energy', 'Melancholic Love').\\n\"\n",
    "        \"- Keep names in Title Case, English only.\\n\"\n",
    "        \"Return ONLY a JSON object mapping cluster id to an object with fields 'name' and 'description'.\\n\\n\"\n",
    "        \"Example output format:\\n\"\n",
    "        \"{\\n\"\n",
    "        \"  \\\"0\\\": {\\\"name\\\": \\\"Late-Night Romance\\\", \\\"description\\\": \\\"Sensual, intimate themes with slow dance vibes.\\\"},\\n\"\n",
    "        \"  \\\"1\\\": {\\\"name\\\": \\\"Party Anthems\\\", \\\"description\\\": \\\"High-energy club tracks centered on dancing and celebration.\\\"}\\n\"\n",
    "        \"}\\n\\n\"\n",
    "        \"Clusters to label:\\n\"\n",
    "        f\"{clusters_block}\\n\\n\"\n",
    "        \"Now return ONLY the JSON.\"\n",
    "    )\n",
    "\n",
    "    resp = client.responses.create(model=model, input=prompt, temperature=temperature)\n",
    "    text = resp.output_text.strip()\n",
    "\n",
    "    data = parse_json_maybe(text)\n",
    "    # Normalize keys to ints if possible\n",
    "    out = {}\n",
    "    for k, v in data.items():\n",
    "        try:\n",
    "            cid = int(k)\n",
    "        except Exception:\n",
    "            cid = k\n",
    "        name = (v or {}).get(\"name\", \"\").strip()\n",
    "        desc = (v or {}).get(\"description\", \"\").strip()\n",
    "        if name:\n",
    "            out[cid] = {\"name\": name, \"description\": desc}\n",
    "    return out\n",
    "\n",
    "\n",
    "def attach_cluster_names(df_with_clusters: pd.DataFrame,\n",
    "                         labels_map: dict,\n",
    "                         cluster_col: str = \"cluster_emb\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds 'cluster_name' and 'cluster_desc' columns using labels_map from LLM.\n",
    "    \"\"\"\n",
    "    out = df_with_clusters.copy()\n",
    "    out[\"cluster_name\"] = out[cluster_col].map(\n",
    "        lambda c: labels_map.get(int(c), {}).get(\"name\") if pd.notna(c) else None)\n",
    "    out[\"cluster_desc\"] = out[cluster_col].map(\n",
    "        lambda c: labels_map.get(int(c), {}).get(\"description\") if pd.notna(c) else None)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b34361d3f2c81e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_emb</th>\n",
       "      <th>cluster_name</th>\n",
       "      <th>cluster_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Empowered Rebellion</td>\n",
       "      <td>Bold, energetic tracks centered on defiance, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Heartbreak Drama</td>\n",
       "      <td>Emotional pop songs exploring vulnerability, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Joyful Summer Pop</td>\n",
       "      <td>Upbeat, vibrant dance tracks filled with youth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Confident Dance Anthems</td>\n",
       "      <td>Bold, modern pop with empowering attitudes and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Moody Rock Reflection</td>\n",
       "      <td>Intense, introspective rock with dark atmosphe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Sensual Nightlife Energy</td>\n",
       "      <td>Dynamic, passionate dance tracks with bold bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>Nostalgic Acoustic Romance</td>\n",
       "      <td>Gentle, heartfelt pop with reflective melodies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.0</td>\n",
       "      <td>Vibrant Party Vibes</td>\n",
       "      <td>Energetic, confident dance music perfect for l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.0</td>\n",
       "      <td>Sentimental Breakup Ballads</td>\n",
       "      <td>Vulnerable, introspective pop exploring regret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Tender Romantic Ballads</td>\n",
       "      <td>Expressive, heartfelt songs focused on love, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Uplifting Empowerment Anthems</td>\n",
       "      <td>Motivational pop tracks filled with hope, resi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>Sensual Dance Romance</td>\n",
       "      <td>Vibrant, rhythmic songs blending passion and n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>Festive Dance Celebration</td>\n",
       "      <td>Upbeat, lively party music with dynamic rhythm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>13.0</td>\n",
       "      <td>Angsty Rock Rebellion</td>\n",
       "      <td>Intense, chaotic rock expressing frustration, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.0</td>\n",
       "      <td>Passionate Pop Breakups</td>\n",
       "      <td>Catchy, emotional pop tracks about love, confl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cluster_emb                   cluster_name  \\\n",
       "6           0.0            Empowered Rebellion   \n",
       "31          1.0               Heartbreak Drama   \n",
       "12          2.0              Joyful Summer Pop   \n",
       "1           3.0        Confident Dance Anthems   \n",
       "14          4.0          Moody Rock Reflection   \n",
       "51          5.0       Sensual Nightlife Energy   \n",
       "3           6.0     Nostalgic Acoustic Romance   \n",
       "22          7.0            Vibrant Party Vibes   \n",
       "5           8.0    Sentimental Breakup Ballads   \n",
       "9           9.0        Tender Romantic Ballads   \n",
       "10         10.0  Uplifting Empowerment Anthems   \n",
       "0          11.0          Sensual Dance Romance   \n",
       "2          12.0      Festive Dance Celebration   \n",
       "32         13.0          Angsty Rock Rebellion   \n",
       "4          14.0        Passionate Pop Breakups   \n",
       "\n",
       "                                         cluster_desc  \n",
       "6   Bold, energetic tracks centered on defiance, c...  \n",
       "31  Emotional pop songs exploring vulnerability, c...  \n",
       "12  Upbeat, vibrant dance tracks filled with youth...  \n",
       "1   Bold, modern pop with empowering attitudes and...  \n",
       "14  Intense, introspective rock with dark atmosphe...  \n",
       "51  Dynamic, passionate dance tracks with bold bea...  \n",
       "3   Gentle, heartfelt pop with reflective melodies...  \n",
       "22  Energetic, confident dance music perfect for l...  \n",
       "5   Vulnerable, introspective pop exploring regret...  \n",
       "9   Expressive, heartfelt songs focused on love, p...  \n",
       "10  Motivational pop tracks filled with hope, resi...  \n",
       "0   Vibrant, rhythmic songs blending passion and n...  \n",
       "2   Upbeat, lively party music with dynamic rhythm...  \n",
       "32  Intense, chaotic rock expressing frustration, ...  \n",
       "4   Catchy, emotional pop tracks about love, confl...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Build payload from your clustered DataFrame (assumes 'cluster_emb' and 'normalized_keywords')\n",
    "payload = build_cluster_payload(df_emb, cluster_col=\"cluster_emb\", keywords_col=\"normalized_keywords\",\n",
    "                                top_k_keywords=20, max_examples=5)\n",
    "\n",
    "# 2) Ask the LLM for names/descriptions\n",
    "labels_map = name_clusters_with_llm(payload, model=\"gpt-4.1-mini\", temperature=0.2)\n",
    "\n",
    "# 3) Attach names back to your DataFrame\n",
    "df_named = attach_cluster_names(df_emb, labels_map, cluster_col=\"cluster_emb\")\n",
    "df_named[[\"cluster_emb\", \"cluster_name\", \"cluster_desc\"]].drop_duplicates().sort_values(\"cluster_emb\").head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbdff4d94452992",
   "metadata": {},
   "source": [
    "We update the CSV to streamline the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "149514f6a6d639c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_named.to_csv(\"tracks_with_normalized_keywords.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e5e1cf9324b2f",
   "metadata": {},
   "source": [
    "In this step, we connected our DataFrame (`df_named`) with the database to properly manage clusters and link them to tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9d6bb7cfe4fb81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted clusters: 15 | Updated tracks: 911\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import libraries.sqlite as db\n",
    "\n",
    "conn = db.create_connection()\n",
    "db.sync_clusters_and_update_tracks(conn, df_named)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0dbdcf459b7a90",
   "metadata": {},
   "source": [
    "# Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eccc55fcf8095f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Useful Functions\n",
    "\n",
    "Weâ€™ll start with functions that will help us get what we want. In this case, weâ€™re going to create a function that retrieves the overall sentiment of all the songs along with the specific data, taking the top 5 most representative songs for each emotion.\n",
    "Weâ€™ll also be able to see it by album, and even by individual song if we want. Additionally, weâ€™ll obtain the suggested playlists along with their main emotion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d38dffcd3a2f07",
   "metadata": {},
   "source": [
    "####\n",
    "1. Get the overall emotion of the entire Spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a3d3564cc5bff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T04:19:50.466779Z",
     "start_time": "2025-09-02T04:19:50.456476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'emotion': 'admiration', 'normalize': 'FP', 'emoji': 'ðŸ˜'},\n",
       " {'emotion': 'amusement', 'normalize': 'FP', 'emoji': 'ðŸ˜‚'},\n",
       " {'emotion': 'anger', 'normalize': 'FN', 'emoji': 'ðŸ˜ '},\n",
       " {'emotion': 'annoyance', 'normalize': 'MN', 'emoji': 'ðŸ˜’'},\n",
       " {'emotion': 'approval', 'normalize': 'FP', 'emoji': 'ðŸ‘'},\n",
       " {'emotion': 'caring', 'normalize': 'FP', 'emoji': 'â¤ï¸'},\n",
       " {'emotion': 'confusion', 'normalize': 'N', 'emoji': 'ðŸ˜•'},\n",
       " {'emotion': 'curiosity', 'normalize': 'MP', 'emoji': 'ðŸ¤”'},\n",
       " {'emotion': 'desire', 'normalize': 'FP', 'emoji': 'ðŸ˜'},\n",
       " {'emotion': 'disappointment', 'normalize': 'MN', 'emoji': 'ðŸ˜ž'},\n",
       " {'emotion': 'disapproval', 'normalize': 'MN', 'emoji': 'ðŸ‘Ž'},\n",
       " {'emotion': 'disgust', 'normalize': 'FN', 'emoji': 'ðŸ¤¢'},\n",
       " {'emotion': 'embarrassment', 'normalize': 'MN', 'emoji': 'ðŸ˜³'},\n",
       " {'emotion': 'excitement', 'normalize': 'FP', 'emoji': 'ðŸŽ‰'},\n",
       " {'emotion': 'fear', 'normalize': 'FN', 'emoji': 'ðŸ˜¨'},\n",
       " {'emotion': 'gratitude', 'normalize': 'FP', 'emoji': 'ðŸ™'},\n",
       " {'emotion': 'grief', 'normalize': 'FN', 'emoji': 'ðŸ˜¢'},\n",
       " {'emotion': 'joy', 'normalize': 'FP', 'emoji': 'ðŸ˜Š'},\n",
       " {'emotion': 'love', 'normalize': 'FP', 'emoji': 'â¤ï¸'},\n",
       " {'emotion': 'nervousness', 'normalize': 'MN', 'emoji': 'ðŸ˜¬'},\n",
       " {'emotion': 'neutral', 'normalize': 'N', 'emoji': 'ðŸ˜'},\n",
       " {'emotion': 'optimism', 'normalize': 'FP', 'emoji': 'ðŸŒŸ'},\n",
       " {'emotion': 'pride', 'normalize': 'FP', 'emoji': 'ðŸ˜Œ'},\n",
       " {'emotion': 'realization', 'normalize': 'MP', 'emoji': 'ðŸ’¡'},\n",
       " {'emotion': 'relief', 'normalize': 'FP', 'emoji': 'ðŸ˜Œ'},\n",
       " {'emotion': 'remorse', 'normalize': 'MN', 'emoji': 'ðŸ˜”'},\n",
       " {'emotion': 'sadness', 'normalize': 'FN', 'emoji': 'ðŸ˜¢'},\n",
       " {'emotion': 'surprise', 'normalize': 'MP', 'emoji': 'ðŸ˜²'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import libraries.sqlite as db\n",
    "db.fetch_emotions_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b6ac164f43af9",
   "metadata": {},
   "source": [
    "####\n",
    "2. Fetch clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7114d3e7d7fbaf94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T04:18:47.540536Z",
     "start_time": "2025-09-02T04:18:47.531232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'neutral',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.get_emotions_from_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea27b475e853f7",
   "metadata": {},
   "source": [
    "####\n",
    "3. Fetch info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e262acbd-33f2-43f2-aa8b-c988e16eeea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.fetch_cluster_avg_emotions(1)\n",
    "db.fetch_all_artists()\n",
    "db.fetch_all_albums()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649ceb97-e64f-4dd3-b54e-e848a02c872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import libraries.sqlite as db\n",
    "conn=db.create_connection()\n",
    "db.fetch_tracks_with_buckets_paginated(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40beefbbea6438d3",
   "metadata": {},
   "source": [
    "## FastAPI Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e5cf3cf606fcf3",
   "metadata": {},
   "source": [
    "We are going to set up a FastAPI service to provide the endpoints that will power a frontend, so you can visualize the lists created by the app and more information about your emotions. For this, Iâ€™ve built a small interface with Vue."
   ]
  },
  {
   "cell_type": "code",
   "id": "1bebc40be855d121",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T05:20:47.931011Z",
     "start_time": "2025-09-09T05:20:47.273907Z"
    }
   },
   "source": [
    "# server.py\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from typing import Optional\n",
    "\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, APIRouter, HTTPException, Query\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from starlette.routing import Mount\n",
    "\n",
    "import libraries.sqlite as db\n",
    "from pydantic import BaseModel\n",
    "from libraries.spotify import get_spotify_auth, get_spotify_client, upsert_playlist_by_name\n",
    "\n",
    "# Spotify config (env-driven; must match your app's settings)\n",
    "SPOTIPY_CLIENT_ID = os.getenv(\"SPOTIPY_CLIENT_ID\", \"ec6c43ef03034d7393a4906cd1df5f06\")\n",
    "SPOTIPY_REDIRECT_URI = os.getenv(\"SPOTIPY_REDIRECT_URI\", \"https://spotify-auth.viant.dev/callback\")\n",
    "SPOTIPY_CACHE_PATH = os.getenv(\"SPOTIPY_CACHE_PATH\", \".cache\")\n",
    "# For playlist ops we need these scopes; add others as required\n",
    "SPOTIPY_SCOPE = os.getenv(\"SPOTIPY_SCOPE\", \"user-library-read playlist-modify-public playlist-modify-private\")\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# FastAPI app\n",
    "# =========================\n",
    "app = FastAPI()\n",
    "\n",
    "# CORS (relax as needed)\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# API router under /api\n",
    "# =========================\n",
    "api = APIRouter(prefix=\"/api\")\n",
    "\n",
    "@api.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "@api.get(\"/playlist-emotions\")\n",
    "def getNormalizeEmotions():\n",
    "    avgEmotions = db.group_avg_emotions_by_normalize(db.fetch_avg_per_emotion())\n",
    "    return {\"emotions\": avgEmotions}\n",
    "\n",
    "@api.get(\"/playlist-emotions/all\")\n",
    "def getAllEmotions():\n",
    "    avgEmotions = db.fetch_avg_per_emotion()\n",
    "    return {\"emotions\": avgEmotions, \"dictionary\": db.fetch_emotions_dictionary()}\n",
    "\n",
    "@api.get(\"/dictionary/emotions\")\n",
    "def get_emotions_dic():\n",
    "    return db.fetch_emotions_dictionary()\n",
    "\n",
    "@app.get(\"/callback\")\n",
    "def spotify_callback(code: str | None = None, error: str | None = None):\n",
    "    if error:\n",
    "        raise HTTPException(status_code=400, detail=f\"Spotify error: {error}\")\n",
    "    if not code:\n",
    "        raise HTTPException(status_code=400, detail=\"Missing authorization code\")\n",
    "\n",
    "    auth = get_spotify_auth(\n",
    "        client_id=SPOTIPY_CLIENT_ID,\n",
    "        redirect_uri=SPOTIPY_REDIRECT_URI,\n",
    "        scope=SPOTIPY_SCOPE,\n",
    "        cache_path=SPOTIPY_CACHE_PATH,\n",
    "        open_browser=True,\n",
    "    )\n",
    "    # Intercambia el code y guarda el token en .cache\n",
    "    auth.get_access_token(code, check_cache=True)\n",
    "    return {\"status\": \"ok\", \"message\": \"Auth OK. You can close this tab.\"}\n",
    "\n",
    "@api.get(\"/emotion-tracks\")\n",
    "def list_tracks(\n",
    "    page: int = Query(1, ge=1),\n",
    "    page_size: int = Query(10, ge=1, le=200),\n",
    "    track: str | None = Query(None, description=\"Filter by track name (case-insensitive)\"),\n",
    "    artist: str | None = Query(None, description=\"Filter by artist name (case-insensitive)\"),\n",
    "    album: str | None = Query(None, description=\"Filter by album name (case-insensitive)\"),\n",
    "    sort_by: str = Query(\"track\", description=\"Sort key\"),\n",
    "    sort_dir: str = Query(\"asc\", description=\"asc or desc\"),\n",
    "):\n",
    "    sort_by = sort_by if sort_by in db.ALLOWED_SORT_KEYS else \"track\"\n",
    "    sort_dir = \"desc\" if sort_dir.lower() == \"desc\" else \"asc\"\n",
    "\n",
    "    try:\n",
    "        conn = db.create_connection()\n",
    "        items, total = db.fetch_tracks_with_buckets_paginated(\n",
    "            conn=conn,\n",
    "            page=page,\n",
    "            page_size=page_size,\n",
    "            track=track,\n",
    "            artist=artist,\n",
    "            album=album,\n",
    "            sort_by=sort_by,\n",
    "            sort_dir=sort_dir,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        parts = [str(a) for a in getattr(e, \"args\", []) if a]\n",
    "        detail = \" \".join(parts) if parts else str(e)\n",
    "        raise HTTPException(status_code=500, detail=f\"Database error: {detail}\".strip())\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return {\n",
    "        \"page\": page,\n",
    "        \"page_size\": page_size,\n",
    "        \"total\": total,\n",
    "        \"sort_by\": sort_by,\n",
    "        \"sort_dir\": sort_dir,\n",
    "        \"items\": items,\n",
    "    }\n",
    "\n",
    "@api.get(\"/playlist-cluster\")\n",
    "def get_playlist_cluster():\n",
    "    return db.fetch_all_tracks_with_clusters()\n",
    "\n",
    "@api.get(\"/playlist-cluster/{cluster_id}\")\n",
    "def get_playlist_cluster(\n",
    "        cluster_id: int,\n",
    "):\n",
    "    return db.fetch_tracks_by_cluster(cluster_id)\n",
    "\n",
    "@api.get(\"/clusters\")\n",
    "def get_playlist_cluster_all():\n",
    "    return db.fetch_clusters()\n",
    "\n",
    "@app.get(\"/spotify/callback\")\n",
    "def spotify_callback(code: str | None = None, error: str | None = None):\n",
    "    \"\"\"\n",
    "    PKCE callback: exchange 'code' for tokens and persist them in the cache.\n",
    "    \"\"\"\n",
    "    if error:\n",
    "        raise HTTPException(status_code=400, detail=f\"Spotify error: {error}\")\n",
    "    if not code:\n",
    "        raise HTTPException(status_code=400, detail=\"Missing authorization code\")\n",
    "\n",
    "    auth = get_spotify_auth(\n",
    "        client_id=SPOTIPY_CLIENT_ID,\n",
    "        redirect_uri=SPOTIPY_REDIRECT_URI,\n",
    "        scope=SPOTIPY_SCOPE,\n",
    "        cache_path=SPOTIPY_CACHE_PATH,\n",
    "        open_browser=True,\n",
    "    )\n",
    "    # Store tokens in cache\n",
    "    auth.get_access_token(code, check_cache=True)\n",
    "    return {\"status\": \"ok\", \"message\": \"Auth OK. You can close this tab.\"}\n",
    "\n",
    "class ClusterPlaylistBody(BaseModel):\n",
    "    description: str = \"Generated from cluster\"\n",
    "    public: bool = False\n",
    "    replace: bool = True  # replace items instead of append\n",
    "\n",
    "@api.post(\"/clusters/{cluster_id}/playlist\")\n",
    "def create_or_update_cluster_playlist(cluster_id: int, body: ClusterPlaylistBody):\n",
    "    \"\"\"\n",
    "    Build a Spotify client from cached PKCE tokens, fetch cluster tracks from SQLite,\n",
    "    and upsert a playlist named after the cluster.\n",
    "    \"\"\"\n",
    "    # 1) Create Spotipy client with playlist scopes using your existing helper\n",
    "    sp = get_spotify_client(\n",
    "        client_id=SPOTIPY_CLIENT_ID,\n",
    "        redirect_uri=SPOTIPY_REDIRECT_URI,\n",
    "        scope=SPOTIPY_SCOPE,\n",
    "        cache_path=SPOTIPY_CACHE_PATH,\n",
    "        open_browser=False,\n",
    "    )\n",
    "\n",
    "    # 2) Pull tracks for cluster\n",
    "    rows = db.fetch_tracks_by_cluster(cluster_id)\n",
    "    if not rows:\n",
    "        raise HTTPException(status_code=404, detail=f\"No tracks found for cluster_id={cluster_id}\")\n",
    "\n",
    "    cluster_name = rows[0].get(\"cluster_name\") or f\"Cluster {cluster_id}\"\n",
    "    track_ids = [r[\"track_spotify_id\"] for r in rows if r.get(\"track_spotify_id\")]\n",
    "\n",
    "    print(f\"Tracks found: {len(track_ids)}\")\n",
    "    print(track_ids)\n",
    "\n",
    "    if not track_ids:\n",
    "        raise HTTPException(status_code=404, detail=f\"No track_spotify_id values for cluster_id={cluster_id}\")\n",
    "\n",
    "    # 3) Create/update playlist by name\n",
    "    pid, url = upsert_playlist_by_name(\n",
    "        sp=sp,\n",
    "        name=cluster_name,\n",
    "        description=body.description,\n",
    "        track_ids_or_urls=track_ids,\n",
    "        public=body.public,\n",
    "        replace=body.replace,\n",
    "    )\n",
    "\n",
    "    # Comprueba propiedad del playlist (debe ser tu usuario actual)\n",
    "    owner = sp.playlist(pid, fields=\"owner.id\").get(\"owner\", {}).get(\"id\")\n",
    "    current = sp.current_user().get(\"id\")\n",
    "    if owner != current:\n",
    "        raise HTTPException(status_code=403, detail=f\"Playlist owner is {owner}, not {current}. You cannot modify it.\")\n",
    "\n",
    "    # Lee total de items tras la operaciÃ³n\n",
    "    total_after = sp.playlist_items(pid, fields=\"total\").get(\"total\")\n",
    "    print(f\"Total tracks: {total_after}\")\n",
    "\n",
    "    snapshots = []  # captura lo que retorne upsert internamente si lo propagas\n",
    "    pl_info = sp.playlist_items(pid, fields=\"total\")\n",
    "    return {\n",
    "        \"cluster_id\": cluster_id,\n",
    "        \"cluster_name\": cluster_name,\n",
    "        \"requested_tracks\": len(track_ids),\n",
    "        \"playlist_id\": pid,\n",
    "        \"playlist_url\": url,\n",
    "        \"playlist_items_after\": pl_info.get(\"total\"),\n",
    "        \"snapshots\": snapshots,  # Ãºtil para confirmar mutaciones\n",
    "    }\n",
    "\n",
    "\n",
    "app.include_router(api)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# SPA static mounting with history fallback\n",
    "# =========================\n",
    "class SPAStaticFiles(StaticFiles):\n",
    "    \"\"\"\n",
    "    Static files with history-fallback: if path not found, serve index.html.\n",
    "    This enables Vue Router (history mode) deep links.\n",
    "    \"\"\"\n",
    "    async def get_response(self, path, scope):\n",
    "        response = await super().get_response(path, scope)\n",
    "        if response.status_code == 404:\n",
    "            return await super().get_response(\"index.html\", scope)\n",
    "        return response\n",
    "\n",
    "\n",
    "def initialize_frontend(\n",
    "    frontend_dir: Optional[str] = None,\n",
    "    mount_path: str = \"/\",\n",
    "    env_var: str = \"FRONTEND_DIR\",\n",
    "    default_rel: str = \"../frontend/dist\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Mount a built Vue app (SPA) at `mount_path`, with history fallback.\n",
    "      - If `frontend_dir` is None, uses $FRONTEND_DIR or `default_rel`.\n",
    "      - Idempotent: skips if already mounted at `mount_path`.\n",
    "      - Returns absolute path used.\n",
    "    \"\"\"\n",
    "    base = frontend_dir or os.getenv(env_var, default_rel)\n",
    "    abs_dir = os.path.abspath(base)\n",
    "\n",
    "    # Avoid duplicate mounts\n",
    "    for r in app.routes:\n",
    "        if isinstance(r, Mount) and r.path == mount_path:\n",
    "            print(f\"[init] SPA already mounted at {mount_path} â†’ {abs_dir}\")\n",
    "            return abs_dir\n",
    "\n",
    "    index_path = os.path.join(abs_dir, \"index.html\")\n",
    "    if not os.path.exists(index_path):\n",
    "        print(f\"[warn] {abs_dir} does not contain index.html. Did you run `npm run build`?\")\n",
    "\n",
    "    app.mount(mount_path, SPAStaticFiles(directory=abs_dir, html=True), name=\"spa\")\n",
    "    print(f\"[init] SPA mounted at {mount_path} from {abs_dir}  (API under /api)\")\n",
    "    return abs_dir\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Server controls\n",
    "# =========================\n",
    "SERVER_STATE = {\"server\": None, \"thread\": None}\n",
    "\n",
    "def start_server(host=\"127.0.0.1\", port=8080):\n",
    "    \"\"\"\n",
    "    Start Uvicorn in a background thread.\n",
    "    \"\"\"\n",
    "    if SERVER_STATE[\"server\"] is not None:\n",
    "        print(f\"âš ï¸ Server already running at http://{host}:{port}\")\n",
    "        return\n",
    "\n",
    "    config = uvicorn.Config(app, host=host, port=port, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "\n",
    "    thread = threading.Thread(target=server.run, daemon=True)\n",
    "    thread.start()\n",
    "\n",
    "    time.sleep(0.5)\n",
    "    SERVER_STATE.update({\"server\": server, \"thread\": thread})\n",
    "    print(f\"ðŸš€ Server running at http://{host}:{port}  (API under /api)\")\n",
    "\n",
    "def stop_server():\n",
    "    \"\"\"\n",
    "    Stop the background Uvicorn server.\n",
    "    \"\"\"\n",
    "    server = SERVER_STATE.get(\"server\")\n",
    "    thread = SERVER_STATE.get(\"thread\")\n",
    "    if server is None:\n",
    "        print(\"â„¹ï¸ No server is currently running.\")\n",
    "        return\n",
    "    server.should_exit = True\n",
    "    if thread and thread.is_alive():\n",
    "        thread.join(timeout=3)\n",
    "    SERVER_STATE.update({\"server\": None, \"thread\": None})\n",
    "    print(\"ðŸ›‘ Server stopped.\")\n",
    "\n",
    "def main(frontend_dir:str = None):\n",
    "    # Mount frontend (uses FRONTEND_DIR env var or ../frontend/dist)\n",
    "    initialize_frontend(frontend_dir=frontend_dir, mount_path=\"/\")\n",
    "    # Start server\n",
    "    start_server(host=os.getenv(\"HOST\", \"127.0.0.1\"), port=int(os.getenv(\"PORT\", \"8080\")))"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "1a0072ca27d29a53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T05:20:49.903477Z",
     "start_time": "2025-09-09T05:20:49.397518Z"
    }
   },
   "source": "main(\"playlist-emotions/dist\")",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [78413]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8080 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init] SPA mounted at / from /Users/vchiriguaya/ProjectOffline/spotify-emotions/playlist-emotions/dist  (API under /api)\n",
      "ðŸš€ Server running at http://127.0.0.1:8080  (API under /api)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "a3cb3fab635f4f6c",
   "metadata": {},
   "source": [
    "## Stop Server\n",
    "\n",
    "When you finish, run this cell to shut down the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc344ad689ff8130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T04:21:07.299202Z",
     "start_time": "2025-09-02T04:21:07.126040Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_server()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
