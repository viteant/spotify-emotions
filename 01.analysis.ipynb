{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Requirements\n",
    "# !pip install os pymysql dotenv typing json pandas transformers torch pandas sentence-transformers scikit-learn numpy openai"
   ],
   "id": "4509a8e0982bed2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sentimental Analysis for Spotify Playlist",
   "id": "f02ca1acf1acbd8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exploratory Data Analysis",
   "id": "c13e00b7b6669067"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import libraries.db as db\n",
    "import pandas as pd"
   ],
   "id": "1ceb4914f0472db1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We retrieve the data from the database",
   "id": "822ba37bb36eb2f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conn = db.create_connection()\n",
    "tracks = db.fetch_tracks_dataset(conn)\n",
    "conn.close()\n",
    "print(len(tracks))"
   ],
   "id": "55477bcc298240ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We build the dataframe to better analyze the data.",
   "id": "89bdb1aad8f43d7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(tracks, columns=[\"track_spotify_id\", \"artist_name\", \"title\", \"keywords\"])\n",
    "df.head(10)"
   ],
   "id": "6fb6f055b2c3b32d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we are going to create a dictionary based on the main keywords to normalize them. Using embeddings, we can adjust the threshold to make the group larger or smaller—the lower the number, the larger the group. In this case, I set it to 0.4.",
   "id": "fac2f74d64dca02d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# ========= 1) Create a unique keywords list =========\n",
    "all_kws = list(itertools.chain.from_iterable(df['keywords']))\n",
    "unique_kws = sorted({\n",
    "    \" \".join(str(k).lower().split())\n",
    "    for k in all_kws\n",
    "    if isinstance(k, str) and k.strip()\n",
    "})\n",
    "\n",
    "print(f\"Total unique keywords: {len(unique_kws)}\")\n",
    "\n",
    "# ========= 2) Embeddings =========\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "emb = model.encode(unique_kws, normalize_embeddings=True)\n",
    "\n",
    "# ========= 3) Hierarchical cluster =========\n",
    "clu = AgglomerativeClustering(\n",
    "    n_clusters=None,\n",
    "    metric='cosine',\n",
    "    linkage='average',\n",
    "    distance_threshold=0.4  # ajusta el threshold: más pequeño = clusters más finos\n",
    ")\n",
    "labels = clu.fit_predict(emb)\n",
    "\n",
    "# ========= 4) Build the clusters =========\n",
    "clusters = {}\n",
    "for kw, lab in zip(unique_kws, labels):\n",
    "    clusters.setdefault(lab, []).append(kw)\n",
    "\n",
    "\n",
    "# ========= 5) Choose canonical representative per cluster =========\n",
    "def pick_rep(words):\n",
    "    # heurística: palabra con menos tokens, si empata, la más corta\n",
    "    return sorted(words, key=lambda w: (len(w.split()), len(w)))[0]\n",
    "\n",
    "\n",
    "suggested_norm = {}\n",
    "for words in clusters.values():\n",
    "    rep = pick_rep(words)\n",
    "    for w in words:\n",
    "        suggested_norm[w] = rep\n",
    "\n",
    "# ========= 6) Save suggested dictionary =========\n",
    "with open(\"keyword_norm_suggested.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(suggested_norm, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Example of normalized (First 20):\")\n",
    "for k, v in list(suggested_norm.items())[:20]:\n",
    "    print(f\"{k} -> {v}\")\n"
   ],
   "id": "6969abc933c80007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Count unique normalized keywords (the canonical representatives). If the number of suggested words is considered optimal compared to the number of unique words, we can proceed; otherwise, we adjust the threshold.",
   "id": "bda478309ec2fd91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unique_norm = set(suggested_norm.values())\n",
    "print(f\"{len(unique_kws)} unique words - suggested words {len(unique_norm)}\")"
   ],
   "id": "a50d812caeddd830",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Keeps the original 'keywords' column and adds a new column\n",
    "    'normalized_keywords' with normalized keywords as a list (unique, no duplicates)."
   ],
   "id": "70e2f133bfdef488"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def apply_normalization_array(df: pd.DataFrame, norm_dict: dict) -> pd.DataFrame:\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"normalized_keywords\"] = df_copy[\"keywords\"].apply(\n",
    "        lambda kws: list({\n",
    "            norm_dict.get(str(k).lower().strip(), str(k).lower().strip())\n",
    "            for k in kws if isinstance(k, str) and k.strip()\n",
    "        })\n",
    "    )\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "df_norm = apply_normalization_array(df, suggested_norm)\n",
    "df_norm[[\"title\", \"artist_name\", \"keywords\", \"normalized_keywords\"]].head(5)"
   ],
   "id": "96f3f424d0e697c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see the top keywords, and if we want, we can go back to the dictionary creation step to re-normalize with another threshold (this is the key).",
   "id": "bae06a4663fe5cdd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def top_normalized_keywords(df: pd.DataFrame, top_n: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns the top N most frequent normalized keywords across all tracks.\n",
    "    \"\"\"\n",
    "    all_kws = list(itertools.chain.from_iterable(df[\"normalized_keywords\"]))\n",
    "    counter = Counter(all_kws)\n",
    "    top = counter.most_common(top_n)\n",
    "    return pd.DataFrame(top, columns=[\"keyword\", \"count\"])\n",
    "\n",
    "\n",
    "top_keywords_df = top_normalized_keywords(df_norm, top_n=20)\n",
    "top_keywords_df\n"
   ],
   "id": "87646202ca7e1f64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To simplify the process, we will save the DataFrame with normalized keywords into a CSV file.",
   "id": "2c36eb8fe4850fa0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_norm.to_csv(\"tracks_with_normalized_keywords.csv\", index=False, encoding=\"utf-8\")",
   "id": "c1c48764be4b8ee5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We also save our normalized keywords in the database to ensure persistence.",
   "id": "46a4ceeb2d93deec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def update_normalized_keywords(conn, df_norm):\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            sql = \"\"\"\n",
    "                UPDATE Tracks\n",
    "                SET normalized_keywords = %s\n",
    "                WHERE spotify_id = %s\n",
    "            \"\"\"\n",
    "            data = []\n",
    "            for _, row in df_norm.iterrows():\n",
    "                norm_kw = json.dumps(row[\"normalized_keywords\"])\n",
    "                track_id = row[\"track_spotify_id\"]\n",
    "                data.append((norm_kw, track_id))\n",
    "\n",
    "            cursor.executemany(sql, data)\n",
    "        conn.commit()\n",
    "        print(f\"{len(data)} filas actualizadas correctamente.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(\"Error al actualizar:\", e)\n",
    "\n",
    "\n",
    "conn = db.create_connection()\n",
    "update_normalized_keywords(conn, df_norm)\n",
    "conn.close()"
   ],
   "id": "6ede7ad30806bc24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentimental Analysis",
   "id": "f087764a035b14ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Utilities functions",
   "id": "420cecf9206795e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:56:04.999545Z",
     "start_time": "2025-08-29T14:56:04.981321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "import json\n",
    "import ast\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def load_df_if_needed(df: pd.DataFrame | None = None,\n",
    "                      csv_path: str = \"tracks_with_normalized_keywords.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame ready for processing.\n",
    "    If df is None OR df does not contain 'normalized_keywords', tries to load from CSV.\n",
    "    \"\"\"\n",
    "    if df is None or \"normalized_keywords\" not in df.columns:\n",
    "        if not os.path.exists(csv_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"DataFrame is missing and '{csv_path}' was not found. \"\n",
    "                \"Provide a DataFrame with columns: track_spotify_id, artist_name, title, keywords (list).\"\n",
    "            )\n",
    "        df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def ensure_keywords_list(df: pd.DataFrame, col: str = \"keywords\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robustly converts the column `col` into a Python list[str] per row.\n",
    "    Handles:\n",
    "      - JSON lists: [\"love\",\"party\"]\n",
    "      - Python repr lists: ['love', 'party']\n",
    "      - Double-escaped JSON (e.g., '\"[\\\"love\\\",\\\"party\\\"]\"')\n",
    "      - Comma-separated fallbacks: love, party\n",
    "      - NaN/None → []\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        raise KeyError(f\"Column '{col}' not found in DataFrame.\")\n",
    "\n",
    "    def _strip_outer_quotes(s: str) -> str:\n",
    "        # Remove a single pair of wrapping quotes if present: '\"[...]' or \"'[...]'\"\n",
    "        if len(s) >= 2 and ((s[0] == s[-1] == '\"') or (s[0] == s[-1] == \"'\")):\n",
    "            return s[1:-1]\n",
    "        return s\n",
    "\n",
    "    def _parse(x):\n",
    "        # Already a list\n",
    "        if isinstance(x, list):\n",
    "            return [str(t).strip() for t in x if isinstance(t, (str, int, float)) and str(t).strip()]\n",
    "\n",
    "        # Missing\n",
    "        if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "            return []\n",
    "\n",
    "        # String-like\n",
    "        if isinstance(x, str):\n",
    "            s = x.strip()\n",
    "            if not s:\n",
    "                return []\n",
    "\n",
    "            # Try to un-wrap if double-escaped: e.g., '\"[\\\"love\\\",\\\"party\\\"]\"'\n",
    "            s_unwrapped = _strip_outer_quotes(s)\n",
    "\n",
    "            # 1) Try JSON\n",
    "            for candidate in (s, s_unwrapped):\n",
    "                if candidate.startswith(\"[\") and candidate.endswith(\"]\"):\n",
    "                    try:\n",
    "                        v = json.loads(candidate)\n",
    "                        if isinstance(v, list):\n",
    "                            return [str(t).strip() for t in v if str(t).strip()]\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            # 2) Try Python literal (handles single quotes lists)\n",
    "            for candidate in (s, s_unwrapped):\n",
    "                if candidate.startswith(\"[\") and candidate.endswith(\"]\"):\n",
    "                    try:\n",
    "                        v = ast.literal_eval(candidate)\n",
    "                        if isinstance(v, list):\n",
    "                            return [str(t).strip() for t in v if str(t).strip()]\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            # 3) Fallback: comma-separated\n",
    "            parts = [p.strip().strip(\"'\").strip('\"') for p in s.split(\",\")]\n",
    "            return [p for p in parts if p]\n",
    "\n",
    "        # Anything else → try stringify\n",
    "        try:\n",
    "            s = str(x).strip()\n",
    "            if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "                v = ast.literal_eval(s)\n",
    "                if isinstance(v, list):\n",
    "                    return [str(t).strip() for t in v if str(t).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "        return []\n",
    "\n",
    "    out = df.copy()\n",
    "    out[col] = out[col].apply(_parse)\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Emotion analysis (model-based, no lexicon)\n",
    "# -------------------------\n",
    "# Load a pre-trained emotion classifier (GoEmotions fine-tuned DistilRoBERTa)\n",
    "# 1) Multi-label emotion pipeline (GoEmotions)\n",
    "_emotion_pipe = None\n",
    "\n",
    "\n",
    "def get_emotion_pipeline_multilabel():\n",
    "    \"\"\"\n",
    "    Multi-label emotion classifier based on GoEmotions.\n",
    "    Uses sigmoid (not softmax) under the hood.\n",
    "    \"\"\"\n",
    "    global _emotion_pipe\n",
    "    if _emotion_pipe is None:\n",
    "        _emotion_pipe = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"joeddav/distilbert-base-uncased-go-emotions-student\",\n",
    "            return_all_scores=True,\n",
    "            top_k=None,  # return all labels with scores\n",
    "            truncation=True\n",
    "        )\n",
    "    return _emotion_pipe\n",
    "\n",
    "\n",
    "# 2) Utility: classify a list of keywords IN BATCH, then aggregate per label\n",
    "def analyze_emotion_from_keywords_multilabel(\n",
    "        keywords: list[str],\n",
    "        min_score_threshold: float = 0.15,  # keep labels with mean score >= threshold\n",
    "        top_k: int = 5,  # return top-k emotions after threshold\n",
    "        batch_size: int = 32,\n",
    "        max_keywords: int | None = 100  # cap to avoid very long batches\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Multi-label emotion analysis:\n",
    "      - Classifies each keyword separately (batched).\n",
    "      - Averages scores per emotion across keywords.\n",
    "      - Filters by threshold and returns top_k labels.\n",
    "\n",
    "    Returns a list of dicts: [{\"label\": \"...\", \"score\": 0.xx}, ...]\n",
    "    \"\"\"\n",
    "    if not keywords:\n",
    "        return []\n",
    "\n",
    "    # Clean and cap keywords\n",
    "    kws = [str(k).strip() for k in keywords if isinstance(k, str) and str(k).strip()]\n",
    "    if not kws:\n",
    "        return []\n",
    "    if max_keywords is not None:\n",
    "        kws = kws[:max_keywords]\n",
    "\n",
    "    pipe = get_emotion_pipeline_multilabel()\n",
    "\n",
    "    # Batch inference\n",
    "    all_scores = pipe(kws, batch_size=batch_size)  # list of list[{\"label\",\"score\"}]\n",
    "\n",
    "    # Aggregate scores per label (mean over keywords)\n",
    "    # Initialize label space from the first result\n",
    "    if not all_scores or not all_scores[0]:\n",
    "        return []\n",
    "\n",
    "    label_scores = defaultdict(list)\n",
    "    labels = [d[\"label\"] for d in all_scores[0]]\n",
    "    for per_kw in all_scores:\n",
    "        for d in per_kw:\n",
    "            label_scores[d[\"label\"]].append(float(d[\"score\"]))\n",
    "\n",
    "    mean_scores = {lab: (sum(vals) / len(vals)) for lab, vals in label_scores.items()}\n",
    "\n",
    "    # Threshold and sort\n",
    "    filtered = [{\"label\": lab, \"score\": sc} for lab, sc in mean_scores.items() if sc >= min_score_threshold]\n",
    "    filtered.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    # Take top_k (if threshold filters too much and nothing remains, fall back to top_k without threshold)\n",
    "    if not filtered:\n",
    "        fallback = [{\"label\": lab, \"score\": sc} for lab, sc in mean_scores.items()]\n",
    "        fallback.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return fallback[:top_k]\n",
    "    return filtered[:top_k]\n",
    "\n",
    "\n",
    "# 3) Attach emotions to your DataFrame using the ORIGINAL 'keywords' column\n",
    "def attach_emotions_multilabel(df: pd.DataFrame, top_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds 'emotions' column with the aggregated multi-label results\n",
    "    computed from the 'keywords' column for each row.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    df_out[\"emotions\"] = df_out[\"keywords\"].apply(\n",
    "        lambda kws: analyze_emotion_from_keywords_multilabel(\n",
    "            kws,\n",
    "            min_score_threshold=0.15,\n",
    "            top_k=top_k\n",
    "        )\n",
    "    )\n",
    "    return df_out"
   ],
   "id": "6b4bd61f322fb18",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are going to perform sentiment analysis. To do this, we’ll use a pre-trained model: joeddav/distilbert-base-uncased-go-emotions-student. This model will help us extract the emotions from our keywords. We can define how many emotions we want to retrieve by setting the top_k parameter in our attach_emotions_multilabel function. Depending on this value, we can decide how many emotions to visualize and how deep we want the analysis to be.",
   "id": "ec384ffd9db2104c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:56:18.548829Z",
     "start_time": "2025-08-29T14:56:07.393557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If you already have a DataFrame named `df`, pass it directly to attach_emotions(df, top_k=3).\n",
    "# Otherwise, load from CSV (expects at least: track_spotify_id, artist_name, title, keywords):\n",
    "try:\n",
    "    df = load_df_if_needed(df=None, csv_path=\"tracks_with_normalized_keywords.csv\")\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    raise\n",
    "\n",
    "# Ensure keywords are lists, then attach emotions\n",
    "df = ensure_keywords_list(df, col=\"keywords\")\n",
    "df = attach_emotions_multilabel(df, top_k=28)  # Top K depends on how many emotions the dataset can describe.\n",
    "\n",
    "# Inspect result (emotions column contains top-3 emotions with scores per track)\n",
    "df[[\"track_spotify_id\", \"artist_name\", \"title\", \"emotions\"]].head(10)"
   ],
   "id": "d128a88da989dc14",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         track_spotify_id              artist_name  \\\n",
       "0  00AoRZ8103mVeOVfpTfGuR               Luny Tunes   \n",
       "1  00BuKLSAFkaEkaVAgIMbeA                Lady Gaga   \n",
       "2  00i0O74dXdaKKdCrqHnfXm             Ricky Martin   \n",
       "3  00iQcGMeC6agUvBjHdkAAM     La Oreja de Van Gogh   \n",
       "4  017PF4Q3l4DBUiWoXk4OWT                 Dua Lipa   \n",
       "5  01uqI4H13Gsd8Lyl1EYd8H  Macklemore & Ryan Lewis   \n",
       "6  01YedSX5OYtSCWdO1DRhvY     La Oreja de Van Gogh   \n",
       "7  02bKaAG61tMw9c63fzKXal               Alex Ubago   \n",
       "8  02d1E4NRuh7OEQO4vCb9PD                Lady Gaga   \n",
       "9  02JIdsrod3BYucThfUFDUX                  Camille   \n",
       "\n",
       "                                    title  \\\n",
       "0                          Mayor Que Yo 3   \n",
       "1                               Telephone   \n",
       "2             La Mordidita (feat. Yotuel)   \n",
       "3                 Un Cuento Sobre el Agua   \n",
       "4                          Break My Heart   \n",
       "5          Same Love (feat. Mary Lambert)   \n",
       "6                             20 de Enero   \n",
       "7  Sin miedo a nada (feat. Amaia Montero)   \n",
       "8                         Marry The Night   \n",
       "9                               Le Festin   \n",
       "\n",
       "                                            emotions  \n",
       "0  [{'label': 'desire', 'score': 0.14666313482448...  \n",
       "1  [{'label': 'excitement', 'score': 0.0790549959...  \n",
       "2  [{'label': 'desire', 'score': 0.08667167080566...  \n",
       "3  [{'label': 'caring', 'score': 0.07104973383247...  \n",
       "4  [{'label': 'sadness', 'score': 0.0907404598547...  \n",
       "5  [{'label': 'realization', 'score': 0.104344944...  \n",
       "6  [{'label': 'realization', 'score': 0.062199747...  \n",
       "7  [{'label': 'desire', 'score': 0.10555011378601...  \n",
       "8  [{'label': 'realization', 'score': 0.068049886...  \n",
       "9  [{'label': 'excitement', 'score': 0.0745692684...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_spotify_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>title</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00AoRZ8103mVeOVfpTfGuR</td>\n",
       "      <td>Luny Tunes</td>\n",
       "      <td>Mayor Que Yo 3</td>\n",
       "      <td>[{'label': 'desire', 'score': 0.14666313482448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00BuKLSAFkaEkaVAgIMbeA</td>\n",
       "      <td>Lady Gaga</td>\n",
       "      <td>Telephone</td>\n",
       "      <td>[{'label': 'excitement', 'score': 0.0790549959...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00i0O74dXdaKKdCrqHnfXm</td>\n",
       "      <td>Ricky Martin</td>\n",
       "      <td>La Mordidita (feat. Yotuel)</td>\n",
       "      <td>[{'label': 'desire', 'score': 0.08667167080566...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00iQcGMeC6agUvBjHdkAAM</td>\n",
       "      <td>La Oreja de Van Gogh</td>\n",
       "      <td>Un Cuento Sobre el Agua</td>\n",
       "      <td>[{'label': 'caring', 'score': 0.07104973383247...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>017PF4Q3l4DBUiWoXk4OWT</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>Break My Heart</td>\n",
       "      <td>[{'label': 'sadness', 'score': 0.0907404598547...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>01uqI4H13Gsd8Lyl1EYd8H</td>\n",
       "      <td>Macklemore &amp; Ryan Lewis</td>\n",
       "      <td>Same Love (feat. Mary Lambert)</td>\n",
       "      <td>[{'label': 'realization', 'score': 0.104344944...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>01YedSX5OYtSCWdO1DRhvY</td>\n",
       "      <td>La Oreja de Van Gogh</td>\n",
       "      <td>20 de Enero</td>\n",
       "      <td>[{'label': 'realization', 'score': 0.062199747...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>02bKaAG61tMw9c63fzKXal</td>\n",
       "      <td>Alex Ubago</td>\n",
       "      <td>Sin miedo a nada (feat. Amaia Montero)</td>\n",
       "      <td>[{'label': 'desire', 'score': 0.10555011378601...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>02d1E4NRuh7OEQO4vCb9PD</td>\n",
       "      <td>Lady Gaga</td>\n",
       "      <td>Marry The Night</td>\n",
       "      <td>[{'label': 'realization', 'score': 0.068049886...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>02JIdsrod3BYucThfUFDUX</td>\n",
       "      <td>Camille</td>\n",
       "      <td>Le Festin</td>\n",
       "      <td>[{'label': 'excitement', 'score': 0.0745692684...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We verify what we obtained.",
   "id": "791fbe4274e1f5ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:56:24.048280Z",
     "start_time": "2025-08-29T14:56:24.042284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the first non-empty emotions entry\n",
    "first_non_empty = df.loc[df[\"emotions\"].map(lambda x: isinstance(x, list) and len(x) > 0), \"emotions\"].iloc[0]\n",
    "second = df.loc[df[\"emotions\"].map(lambda x: isinstance(x, list) and len(x) > 0), \"emotions\"].iloc[1]\n",
    "\n",
    "# Pretty print as JSON for readability\n",
    "print(json.dumps(first_non_empty, indent=2))\n",
    "print(json.dumps(second, indent=2))\n"
   ],
   "id": "47c5678daa6a3846",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"label\": \"desire\",\n",
      "    \"score\": 0.1466631348244846\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"caring\",\n",
      "    \"score\": 0.08240479482337833\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"realization\",\n",
      "    \"score\": 0.07717188512906432\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"excitement\",\n",
      "    \"score\": 0.06025317693129182\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"admiration\",\n",
      "    \"score\": 0.05247069443576038\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"pride\",\n",
      "    \"score\": 0.05070672715082765\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"approval\",\n",
      "    \"score\": 0.04951061676256359\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"love\",\n",
      "    \"score\": 0.04948227442800999\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"curiosity\",\n",
      "    \"score\": 0.04367535659112036\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"optimism\",\n",
      "    \"score\": 0.03690654546953738\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"joy\",\n",
      "    \"score\": 0.03547618095763028\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"relief\",\n",
      "    \"score\": 0.03178699493873864\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"amusement\",\n",
      "    \"score\": 0.026313653052784502\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"surprise\",\n",
      "    \"score\": 0.026229654159396886\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"neutral\",\n",
      "    \"score\": 0.02188704744912684\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"remorse\",\n",
      "    \"score\": 0.0214458767673932\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"confusion\",\n",
      "    \"score\": 0.02102863962063566\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"gratitude\",\n",
      "    \"score\": 0.020929685165174305\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"nervousness\",\n",
      "    \"score\": 0.01916179701220244\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"annoyance\",\n",
      "    \"score\": 0.018208866752684117\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"embarrassment\",\n",
      "    \"score\": 0.01767279250198044\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"sadness\",\n",
      "    \"score\": 0.014251383923692629\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"disapproval\",\n",
      "    \"score\": 0.014025136973941699\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"fear\",\n",
      "    \"score\": 0.013670685049146414\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"grief\",\n",
      "    \"score\": 0.013568005216075107\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"disappointment\",\n",
      "    \"score\": 0.012079014541814103\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"disgust\",\n",
      "    \"score\": 0.012035022140480578\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"anger\",\n",
      "    \"score\": 0.010984355112304911\n",
      "  }\n",
      "]\n",
      "[\n",
      "  {\n",
      "    \"label\": \"excitement\",\n",
      "    \"score\": 0.07905499592889101\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"caring\",\n",
      "    \"score\": 0.06265590065158903\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"amusement\",\n",
      "    \"score\": 0.049206477485131475\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"realization\",\n",
      "    \"score\": 0.04917162247002125\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"desire\",\n",
      "    \"score\": 0.04743241309188306\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"joy\",\n",
      "    \"score\": 0.046023852832149714\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"disappointment\",\n",
      "    \"score\": 0.043464998167473824\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"annoyance\",\n",
      "    \"score\": 0.043068909982685\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"pride\",\n",
      "    \"score\": 0.042846090137027205\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"curiosity\",\n",
      "    \"score\": 0.04153381164651364\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"approval\",\n",
      "    \"score\": 0.040989372227340934\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"disapproval\",\n",
      "    \"score\": 0.03579476631712168\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"confusion\",\n",
      "    \"score\": 0.035550276702269915\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"embarrassment\",\n",
      "    \"score\": 0.03512042299844324\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"relief\",\n",
      "    \"score\": 0.03386619959492236\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"nervousness\",\n",
      "    \"score\": 0.028681439044885338\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"remorse\",\n",
      "    \"score\": 0.027342716977000237\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"grief\",\n",
      "    \"score\": 0.027108398429118098\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"optimism\",\n",
      "    \"score\": 0.026130570820532738\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"sadness\",\n",
      "    \"score\": 0.02604051292873919\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"admiration\",\n",
      "    \"score\": 0.02548316939501092\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"surprise\",\n",
      "    \"score\": 0.024335826840251686\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"love\",\n",
      "    \"score\": 0.02373911301838234\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"anger\",\n",
      "    \"score\": 0.023157978767994792\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"disgust\",\n",
      "    \"score\": 0.022885591199155897\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"neutral\",\n",
      "    \"score\": 0.022160540358163415\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"gratitude\",\n",
      "    \"score\": 0.021410722797736526\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"fear\",\n",
      "    \"score\": 0.015743314963765444\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will denormalize, create an appropriate table, and insert the extracted emotions for later analysis.",
   "id": "56fec9a24d7eadf9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. Discover unique emotions from df",
   "id": "14974d3ff603ca30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:56:29.019529Z",
     "start_time": "2025-08-29T14:56:29.009788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def all_emotions_from_df(df):\n",
    "    \"\"\"\n",
    "    Returns a sorted list of unique emotion labels present in df['emotions'].\n",
    "    \"\"\"\n",
    "    labels = set()\n",
    "    for lst in df[\"emotions\"]:\n",
    "        if isinstance(lst, list):\n",
    "            for d in lst:\n",
    "                lab = str(d.get(\"label\", \"\")).strip().lower()\n",
    "                if lab:\n",
    "                    labels.add(lab)\n",
    "    return sorted(labels)\n",
    "\n",
    "\n",
    "def to_snake(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Safe snake_case for MySQL column names.\n",
    "    \"\"\"\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    return re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "\n",
    "\n",
    "emotions = all_emotions_from_df(df)\n",
    "emotion_cols = [to_snake(e) for e in emotions]\n",
    "print(len(emotion_cols), \"emotions discovered\")\n"
   ],
   "id": "fe1c98a2e5882808",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 emotions discovered\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " 2. Build DDL for a wide, denormalized table",
   "id": "de815c27553bac6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T15:09:02.874068Z",
     "start_time": "2025-08-29T15:09:02.871353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_create_table_sql(table: str, emotion_cols: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Builds a CREATE TABLE statement with one DECIMAL(6,3) column per emotion (0–100%).\n",
    "    Primary key: track_spotify_id\n",
    "    \"\"\"\n",
    "    cols_sql = \",\\n  \".join([f\"`{c}` DECIMAL(6,4) NOT NULL DEFAULT 0\" for c in emotion_cols])\n",
    "    ddl = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS `{table}` (\n",
    "  `track_spotify_id` VARCHAR(64) NOT NULL,\n",
    "  {cols_sql},\n",
    "  PRIMARY KEY (`track_spotify_id`)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "\"\"\".strip()\n",
    "    return ddl\n",
    "\n",
    "\n",
    "# Example: run once in MySQL\n",
    "table_name = \"track_emotions_wide\"\n",
    "ddl = build_create_table_sql(table_name, emotion_cols)"
   ],
   "id": "9b34c46d10e65c83",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. Pivot df to wide (percentages per emotion)",
   "id": "ab3586c74a471f13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T15:10:13.559458Z",
     "start_time": "2025-08-29T15:10:13.487038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def df_to_emotion_wide(df: pd.DataFrame, emotions: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a wide DataFrame with one column per emotion (percentage 0–100),\n",
    "    filling 0 where the emotion is absent.\n",
    "    \"\"\"\n",
    "    cols = [\"track_spotify_id\"] + [to_snake(e) for e in emotions]\n",
    "    out_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        base = {c: 0.0 for c in cols}\n",
    "        base[\"track_spotify_id\"] = str(row[\"track_spotify_id\"]).strip()\n",
    "        lst = row[\"emotions\"] if isinstance(row[\"emotions\"], list) else []\n",
    "        for d in lst:\n",
    "            lab = str(d.get(\"label\", \"\")).strip().lower()\n",
    "            score = round(float(d.get(\"score\", 0.0) * 100), 4)\n",
    "            col = to_snake(lab)\n",
    "            if col in base:\n",
    "                val = score  # store as percentage\n",
    "                # if multiple entries for the same label, keep max\n",
    "                base[col] = max(base[col], val)\n",
    "        out_rows.append(base)\n",
    "    wide = pd.DataFrame(out_rows, columns=cols)\n",
    "    return wide\n",
    "\n",
    "\n",
    "wide_df = df_to_emotion_wide(df, emotions)\n",
    "print(f\"Value: {wide_df.iloc[0, 1:].sum()}\")  # The value needs to be ~= 100\n",
    "wide_df.head()\n"
   ],
   "id": "1adcf0aa383f27e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 100.00020000000002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         track_spotify_id  admiration  amusement   anger  annoyance  approval  \\\n",
       "0  00AoRZ8103mVeOVfpTfGuR      5.2471     2.6314  1.0984     1.8209    4.9511   \n",
       "1  00BuKLSAFkaEkaVAgIMbeA      2.5483     4.9206  2.3158     4.3069    4.0989   \n",
       "2  00i0O74dXdaKKdCrqHnfXm      2.4668     4.2057  2.0524     3.5313    4.0391   \n",
       "3  00iQcGMeC6agUvBjHdkAAM      3.1316     4.4714  1.6054     2.7343    3.8828   \n",
       "4  017PF4Q3l4DBUiWoXk4OWT      1.6237     2.1248  1.3385     1.8788    2.4960   \n",
       "\n",
       "   caring  confusion  curiosity   desire  ...    love  nervousness  neutral  \\\n",
       "0  8.2405     2.1029     4.3675  14.6663  ...  4.9482       1.9162   2.1887   \n",
       "1  6.2656     3.5550     4.1534   4.7432  ...  2.3739       2.8681   2.2161   \n",
       "2  5.9327     2.2842     4.1299   8.6672  ...  3.3810       2.9953   2.0670   \n",
       "3  7.1050     2.9283     5.0686   5.7196  ...  4.8880       3.0396   2.4263   \n",
       "4  6.4593     3.7262     3.2620   3.6006  ...  4.1245       2.7741   1.6141   \n",
       "\n",
       "   optimism   pride  realization  relief  remorse  sadness  surprise  \n",
       "0    3.6907  5.0707       7.7172  3.1787   2.1446   1.4251    2.6230  \n",
       "1    2.6131  4.2846       4.9172  3.3866   2.7343   2.6041    2.4336  \n",
       "2    3.8467  4.0853       5.7221  4.3536   2.4415   1.8666    2.9859  \n",
       "3    6.8149  3.7328       5.4130  5.2132   3.2846   2.7707    2.8820  \n",
       "4    3.6729  2.0447       6.1625  2.7624   6.5137   9.0740    2.2786  \n",
       "\n",
       "[5 rows x 29 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_spotify_id</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>neutral</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00AoRZ8103mVeOVfpTfGuR</td>\n",
       "      <td>5.2471</td>\n",
       "      <td>2.6314</td>\n",
       "      <td>1.0984</td>\n",
       "      <td>1.8209</td>\n",
       "      <td>4.9511</td>\n",
       "      <td>8.2405</td>\n",
       "      <td>2.1029</td>\n",
       "      <td>4.3675</td>\n",
       "      <td>14.6663</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9482</td>\n",
       "      <td>1.9162</td>\n",
       "      <td>2.1887</td>\n",
       "      <td>3.6907</td>\n",
       "      <td>5.0707</td>\n",
       "      <td>7.7172</td>\n",
       "      <td>3.1787</td>\n",
       "      <td>2.1446</td>\n",
       "      <td>1.4251</td>\n",
       "      <td>2.6230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00BuKLSAFkaEkaVAgIMbeA</td>\n",
       "      <td>2.5483</td>\n",
       "      <td>4.9206</td>\n",
       "      <td>2.3158</td>\n",
       "      <td>4.3069</td>\n",
       "      <td>4.0989</td>\n",
       "      <td>6.2656</td>\n",
       "      <td>3.5550</td>\n",
       "      <td>4.1534</td>\n",
       "      <td>4.7432</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3739</td>\n",
       "      <td>2.8681</td>\n",
       "      <td>2.2161</td>\n",
       "      <td>2.6131</td>\n",
       "      <td>4.2846</td>\n",
       "      <td>4.9172</td>\n",
       "      <td>3.3866</td>\n",
       "      <td>2.7343</td>\n",
       "      <td>2.6041</td>\n",
       "      <td>2.4336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00i0O74dXdaKKdCrqHnfXm</td>\n",
       "      <td>2.4668</td>\n",
       "      <td>4.2057</td>\n",
       "      <td>2.0524</td>\n",
       "      <td>3.5313</td>\n",
       "      <td>4.0391</td>\n",
       "      <td>5.9327</td>\n",
       "      <td>2.2842</td>\n",
       "      <td>4.1299</td>\n",
       "      <td>8.6672</td>\n",
       "      <td>...</td>\n",
       "      <td>3.3810</td>\n",
       "      <td>2.9953</td>\n",
       "      <td>2.0670</td>\n",
       "      <td>3.8467</td>\n",
       "      <td>4.0853</td>\n",
       "      <td>5.7221</td>\n",
       "      <td>4.3536</td>\n",
       "      <td>2.4415</td>\n",
       "      <td>1.8666</td>\n",
       "      <td>2.9859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00iQcGMeC6agUvBjHdkAAM</td>\n",
       "      <td>3.1316</td>\n",
       "      <td>4.4714</td>\n",
       "      <td>1.6054</td>\n",
       "      <td>2.7343</td>\n",
       "      <td>3.8828</td>\n",
       "      <td>7.1050</td>\n",
       "      <td>2.9283</td>\n",
       "      <td>5.0686</td>\n",
       "      <td>5.7196</td>\n",
       "      <td>...</td>\n",
       "      <td>4.8880</td>\n",
       "      <td>3.0396</td>\n",
       "      <td>2.4263</td>\n",
       "      <td>6.8149</td>\n",
       "      <td>3.7328</td>\n",
       "      <td>5.4130</td>\n",
       "      <td>5.2132</td>\n",
       "      <td>3.2846</td>\n",
       "      <td>2.7707</td>\n",
       "      <td>2.8820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>017PF4Q3l4DBUiWoXk4OWT</td>\n",
       "      <td>1.6237</td>\n",
       "      <td>2.1248</td>\n",
       "      <td>1.3385</td>\n",
       "      <td>1.8788</td>\n",
       "      <td>2.4960</td>\n",
       "      <td>6.4593</td>\n",
       "      <td>3.7262</td>\n",
       "      <td>3.2620</td>\n",
       "      <td>3.6006</td>\n",
       "      <td>...</td>\n",
       "      <td>4.1245</td>\n",
       "      <td>2.7741</td>\n",
       "      <td>1.6141</td>\n",
       "      <td>3.6729</td>\n",
       "      <td>2.0447</td>\n",
       "      <td>6.1625</td>\n",
       "      <td>2.7624</td>\n",
       "      <td>6.5137</td>\n",
       "      <td>9.0740</td>\n",
       "      <td>2.2786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. Bulk upsert into MySQL (denormalized table)",
   "id": "412a7977cf34c58c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T15:10:16.804589Z",
     "start_time": "2025-08-29T15:10:16.711432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import libraries.db as db\n",
    "\n",
    "\n",
    "def upsert_emotion_wide(conn, table: str, wide_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Bulk UPSERT wide_df into MySQL table:\n",
    "    - INSERT ... ON DUPLICATE KEY UPDATE for all emotion columns.\n",
    "    \"\"\"\n",
    "    if wide_df.empty:\n",
    "        return\n",
    "    cols = list(wide_df.columns)  # first col must be track_spotify_id\n",
    "    placeholders = \", \".join([\"%s\"] * len(cols))\n",
    "    col_list = \", \".join([f\"`{c}`\" for c in cols])\n",
    "    update_clause = \", \".join([f\"`{c}`=VALUES(`{c}`)\" for c in cols[1:]])  # skip PK\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        INSERT INTO `{table}` ({col_list})\n",
    "        VALUES ({placeholders})\n",
    "        ON DUPLICATE KEY UPDATE {update_clause}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        cur.executemany(sql, [tuple(row) for row in wide_df.itertuples(index=False, name=None)])\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def truncate_track_emotions_wide(conn):\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"TRUNCATE TABLE track_emotions_wide;\")\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "\n",
    "\n",
    "conn = db.create_connection()\n",
    "truncate_track_emotions_wide(conn)\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(ddl)\n",
    "conn.commit()\n",
    "upsert_emotion_wide(conn, table_name, wide_df)\n",
    "conn.close()"
   ],
   "id": "460edef314387e9b",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We store the extracted emotions in the track table of the database for added reliability.",
   "id": "211dc6ce6b3f6b23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T15:10:19.811696Z",
     "start_time": "2025-08-29T15:10:19.645549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_track_emotions(conn, df):\n",
    "    \"\"\"\n",
    "    Update the 'emotions' field in tracks table.\n",
    "    - df must have columns: track_spotify_id, emotions\n",
    "    - emotions is a list[dict] -> will be stored as JSON string\n",
    "    - match df.track_spotify_id to tracks.spotify_id\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            sql = \"\"\"\n",
    "                UPDATE tracks\n",
    "                SET emotions = %s\n",
    "                WHERE spotify_id = %s\n",
    "            \"\"\"\n",
    "            data = []\n",
    "            for _, row in df.iterrows():\n",
    "                spid = str(row[\"track_spotify_id\"]).strip()\n",
    "                emos = row[\"emotions\"]\n",
    "\n",
    "                # Ensure JSON string\n",
    "                emo_json = json.dumps(emos, ensure_ascii=False) if emos is not None else \"[]\"\n",
    "\n",
    "                data.append((emo_json, spid))\n",
    "\n",
    "            cur.executemany(sql, data)\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"Updated emotions for {len(data)} tracks.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(\"Error while updating emotions:\", e)\n",
    "        raise\n",
    "\n",
    "\n",
    "conn = db.create_connection()\n",
    "update_track_emotions(conn, df)\n",
    "conn.close()"
   ],
   "id": "6ccc51b3368adc10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated emotions for 911 tracks.\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Emotion Dictionary Creation\n",
    "\n",
    "In this step, we will **extract the emotion columns from `wide_df`**, use the language model (LLM) to classify them as **Positive, Neutral, or Negative**, and also assign a representative **emoji**.\n",
    "\n",
    "With this information, we will build an **emotion dictionary** and store it in the database in the `emotions_dictionary` table (creating or truncating it beforehand)."
   ],
   "id": "22024bc6681c482"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:59:19.750293Z",
     "start_time": "2025-08-29T14:59:19.744701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carga las variables del archivo .env en el entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Ahora puedes verificar que la clave está disponible\n",
    "print(\"✅ OPENAI_API_KEY found:\", bool(os.getenv(\"OPENAI_API_KEY\")))"
   ],
   "id": "e2d73d44a5238040",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OPENAI_API_KEY found: True\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:59:22.795701Z",
     "start_time": "2025-08-29T14:59:22.753884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Emotions Dictionary: wide_df -> LLM -> DB\n",
    "# =========================\n",
    "# Requisitos:\n",
    "#   - pip install openai python-dotenv pymysql\n",
    "#   - .env con OPENAI_API_KEY=<tu_key>\n",
    "#   - objeto 'db' con db.create_connection() -> PyMySQL connection\n",
    "#   - DataFrame 'wide_df' ya en memoria\n",
    "#\n",
    "# Qué hace:\n",
    "#   1) Extrae emociones desde wide_df.columns (excluye 'track_spotify_id')\n",
    "#   2) Pide al LLM: emotion -> {emotion, normalize (Positive|Neutral|Negative), emoji}\n",
    "#   3) Crea/TRUNCATE emotions_dictionary\n",
    "#   4) Inserta mapping en MySQL\n",
    "#   5) Devuelve el mapping\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import pymysql\n",
    "from pymysql.cursors import DictCursor\n",
    "\n",
    "# 1) Cargar .env (si existe)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 2) Cliente OpenAI (lee OPENAI_API_KEY del entorno)\n",
    "from openai import OpenAI\n",
    "\n",
    "_client = OpenAI()  # si no existe la var, lanzará error al invocar\n",
    "\n",
    "\n",
    "# ---------- Utilidades ----------\n",
    "\n",
    "def _parse_json_lenient(text: str) -> Any:\n",
    "    \"\"\"\n",
    "    Intenta parsear JSON aunque el modelo devuelva texto extra o ```json fences.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        raise ValueError(\"Empty LLM response.\")\n",
    "    cleaned = text.strip()\n",
    "\n",
    "    # quitar fences ```json ... ```\n",
    "    cleaned = re.sub(r\"^```(?:json|JSON)?\\s*\", \"\", cleaned)\n",
    "    cleaned = re.sub(r\"\\s*```$\", \"\", cleaned)\n",
    "\n",
    "    # intento directo\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # buscar objeto {...}\n",
    "    s, e = cleaned.find(\"{\"), cleaned.rfind(\"}\")\n",
    "    if s != -1 and e != -1 and s < e:\n",
    "        try:\n",
    "            return json.loads(cleaned[s:e + 1])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # buscar array [...]\n",
    "    s, e = cleaned.find(\"[\"), cleaned.rfind(\"]\")\n",
    "    if s != -1 and e != -1 and s < e:\n",
    "        try:\n",
    "            return json.loads(cleaned[s:e + 1])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    raise ValueError(f\"Could not parse JSON from LLM output. Got:\\n{cleaned[:500]}\")\n",
    "\n",
    "\n",
    "def _canon_norm(label: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza etiquetas a Positive|Neutral|Negative (acepta inglés/español/minúsculas).\n",
    "    \"\"\"\n",
    "    if not label:\n",
    "        return \"Neutral\"\n",
    "    l = label.strip().lower()\n",
    "    if l.startswith((\"pos\", \"poz\", \"positivo\")):\n",
    "        return \"Positive\"\n",
    "    if l.startswith((\"neu\", \"neutro\")):\n",
    "        return \"Neutral\"\n",
    "    if l.startswith((\"neg\", \"negativo\")):\n",
    "        return \"Negative\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "\n",
    "# ---------- LLM ----------\n",
    "\n",
    "def llm_classify_emotions_with_openai(\n",
    "        emotions: List[str],\n",
    "        model: str = \"gpt-4o-mini\",\n",
    "        temperature: float = 0.0,\n",
    "        max_retries: int = 3,\n",
    "        dry_run: bool = False,\n",
    "        debug_print: bool = False,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Pide al chat.completions un JSON con:\n",
    "    [ {emotion, normalize (FP:Full Positive|MP: Positive|N:Neutral|MN:Mid Negative|FN: Full Negative), emoji}, ... ]\n",
    "    Usa parser tolerante para extraer el JSON.\n",
    "    \"\"\"\n",
    "    if not emotions:\n",
    "        return []\n",
    "    if dry_run:\n",
    "        return [{\"emotion\": e, \"normalize\": \"N\", \"emoji\": \"❓\"} for e in emotions]\n",
    "\n",
    "    system_msg = '''\n",
    "    You are a precise JSON generator.\n",
    "    For each input emotion, return a JSON array of objects with the following keys:\n",
    "    - \"emotion\": the emotion name,\n",
    "    - \"normalize\": one of [\"FP\" (Full Positive), \"MP\" (Mid Positive), \"N\" (Neutral), \"MN\" (Mid Negative), \"FN\" (Full Negative)],\n",
    "    - \"emoji\": a single representative emoji.\n",
    "\n",
    "    Return ONLY a valid JSON array, with no explanations and no markdown.\n",
    "    '''\n",
    "    user_msg = \"Emotions:\\n\" + json.dumps(emotions, ensure_ascii=False)\n",
    "\n",
    "    last_text: Optional[str] = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = _client.chat.completions.create(\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_msg},\n",
    "                    {\"role\": \"user\", \"content\": user_msg},\n",
    "                ],\n",
    "                max_tokens=1200,\n",
    "            )\n",
    "            text = resp.choices[0].message.content\n",
    "            last_text = text\n",
    "            parsed = _parse_json_lenient(text)\n",
    "\n",
    "            # Acepta lista directa o objeto con \"result\"\n",
    "            if isinstance(parsed, dict) and \"result\" in parsed and isinstance(parsed[\"result\"], list):\n",
    "                items = parsed[\"result\"]\n",
    "            elif isinstance(parsed, list):\n",
    "                items = parsed\n",
    "            else:\n",
    "                raise ValueError(\"Parsed JSON is not a list nor an object with 'result'.\")\n",
    "\n",
    "            out: List[Dict[str, Any]] = []\n",
    "            for row in items:\n",
    "                if not isinstance(row, dict):\n",
    "                    continue\n",
    "                emo = str(row.get(\"emotion\", \"\")).strip()\n",
    "                norm = str(row.get(\"normalize\", \"\")).strip()\n",
    "                emoji = str(row.get(\"emoji\", \"\")).strip()\n",
    "                if emo and emoji:\n",
    "                    out.append({\"emotion\": emo, \"normalize\": norm, \"emoji\": emoji})\n",
    "\n",
    "            if not out:\n",
    "                raise ValueError(\"Empty mapping after validation.\")\n",
    "            return out\n",
    "\n",
    "        except Exception as e:\n",
    "            if debug_print and last_text:\n",
    "                print(\"LLM RAW OUTPUT (truncated):\\n\", last_text[:800])\n",
    "            if attempt == max_retries:\n",
    "                raise\n",
    "            time.sleep(0.7 * attempt)\n",
    "\n",
    "\n",
    "# ---------- DB ----------\n",
    "\n",
    "def create_and_truncate_emotions_dictionary():\n",
    "    \"\"\"\n",
    "    Crea la tabla emotions_dictionary si no existe y la limpia (TRUNCATE).\n",
    "    \"\"\"\n",
    "    conn = db.create_connection()\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS emotions_dictionary (\n",
    "                    emotion   VARCHAR(128) PRIMARY KEY,\n",
    "                    normalize VARCHAR(20) NOT NULL,\n",
    "                    emoji     VARCHAR(16) NOT NULL\n",
    "                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "            \"\"\")\n",
    "            cur.execute(\"TRUNCATE TABLE emotions_dictionary;\")\n",
    "        conn.commit()\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "def insert_emotions_dictionary(rows: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Inserta filas en emotions_dictionary: [{emotion, normalize, emoji}, ...]\n",
    "    \"\"\"\n",
    "    if not rows:\n",
    "        return\n",
    "    conn = db.create_connection()\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.executemany(\n",
    "                \"\"\"\n",
    "                INSERT INTO emotions_dictionary (emotion, normalize, emoji)\n",
    "                VALUES (%s, %s, %s)\n",
    "                \"\"\",\n",
    "                [(r[\"emotion\"], r[\"normalize\"], r[\"emoji\"]) for r in rows]\n",
    "            )\n",
    "        conn.commit()\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "# ---------- Orquestación ----------\n",
    "\n",
    "def build_and_store_emotions_dictionary_with_llm(\n",
    "        wide_df,\n",
    "        llm_model: str = \"gpt-4o-mini\",\n",
    "        temperature: float = 0.0,\n",
    "        dry_run_llm: bool = False,\n",
    "        verbose: bool = False,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    1) Toma columnas de wide_df (excluye 'track_spotify_id') => emociones.\n",
    "    2) Llama LLM para clasificar y asignar emoji.\n",
    "    3) Crea/TRUNCATE emotions_dictionary y guarda.\n",
    "    4) Devuelve la lista final insertada.\n",
    "    \"\"\"\n",
    "    # 1) Extraer nombres de emociones desde los headers del DF\n",
    "    all_cols = list(wide_df.columns)\n",
    "    emotions_in_df = [c for c in all_cols if c.lower() != \"track_spotify_id\"]\n",
    "    if verbose:\n",
    "        print(\"Emotions from wide_df:\", emotions_in_df)\n",
    "\n",
    "    if not emotions_in_df:\n",
    "        raise ValueError(\"No emotion columns found in wide_df (other than 'track_spotify_id').\")\n",
    "\n",
    "    # 2) LLM -> mapping\n",
    "    mapping = llm_classify_emotions_with_openai(\n",
    "        emotions=emotions_in_df,\n",
    "        model=llm_model,\n",
    "        temperature=temperature,\n",
    "        dry_run=dry_run_llm,\n",
    "        debug_print=verbose,\n",
    "    )\n",
    "\n",
    "    # 3) Ajustar a headers exactos (por si el LLM cambia mayúsculas/minúsculas)\n",
    "    by_lower_original = {c.lower(): c for c in emotions_in_df}\n",
    "    cleaned: List[Dict[str, Any]] = []\n",
    "    for item in mapping:\n",
    "        key = item[\"emotion\"].strip().lower()\n",
    "        if key in by_lower_original:\n",
    "            cleaned.append({\n",
    "                \"emotion\": by_lower_original[key],\n",
    "                \"normalize\": item[\"normalize\"],\n",
    "                \"emoji\": item[\"emoji\"]\n",
    "            })\n",
    "\n",
    "    if not cleaned:\n",
    "        raise ValueError(\"LLM produced no valid emotions present in wide_df.\")\n",
    "\n",
    "    # 4) Persistir en DB\n",
    "    create_and_truncate_emotions_dictionary()\n",
    "    insert_emotions_dictionary(cleaned)\n",
    "\n",
    "    return cleaned\n"
   ],
   "id": "c49e6918bc4d9d8a",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:59:34.998927Z",
     "start_time": "2025-08-29T14:59:25.311290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = build_and_store_emotions_dictionary_with_llm(\n",
    "    wide_df,\n",
    "    llm_model=\"gpt-4o-mini\",\n",
    "    temperature=0.2,\n",
    "    dry_run_llm=False,  # True para probar sin consumir API\n",
    "    verbose=True  # imprime las columnas/RAW si hay errores\n",
    ")\n",
    "print(f\"Inserted {len(result)} emotions into emotions_dictionary\")\n",
    "result"
   ],
   "id": "d56048994145fe97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions from wide_df: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise']\n",
      "Inserted 28 emotions into emotions_dictionary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'emotion': 'admiration', 'normalize': 'FP', 'emoji': '😍'},\n",
       " {'emotion': 'amusement', 'normalize': 'FP', 'emoji': '😂'},\n",
       " {'emotion': 'anger', 'normalize': 'FN', 'emoji': '😠'},\n",
       " {'emotion': 'annoyance', 'normalize': 'MN', 'emoji': '😒'},\n",
       " {'emotion': 'approval', 'normalize': 'FP', 'emoji': '👍'},\n",
       " {'emotion': 'caring', 'normalize': 'FP', 'emoji': '❤️'},\n",
       " {'emotion': 'confusion', 'normalize': 'N', 'emoji': '😕'},\n",
       " {'emotion': 'curiosity', 'normalize': 'MP', 'emoji': '🤔'},\n",
       " {'emotion': 'desire', 'normalize': 'FP', 'emoji': '😍'},\n",
       " {'emotion': 'disappointment', 'normalize': 'MN', 'emoji': '😞'},\n",
       " {'emotion': 'disapproval', 'normalize': 'FN', 'emoji': '👎'},\n",
       " {'emotion': 'disgust', 'normalize': 'FN', 'emoji': '🤢'},\n",
       " {'emotion': 'embarrassment', 'normalize': 'MN', 'emoji': '😳'},\n",
       " {'emotion': 'excitement', 'normalize': 'FP', 'emoji': '🎉'},\n",
       " {'emotion': 'fear', 'normalize': 'FN', 'emoji': '😨'},\n",
       " {'emotion': 'gratitude', 'normalize': 'FP', 'emoji': '🙏'},\n",
       " {'emotion': 'grief', 'normalize': 'FN', 'emoji': '😢'},\n",
       " {'emotion': 'joy', 'normalize': 'FP', 'emoji': '😊'},\n",
       " {'emotion': 'love', 'normalize': 'FP', 'emoji': '❤️'},\n",
       " {'emotion': 'nervousness', 'normalize': 'MN', 'emoji': '😬'},\n",
       " {'emotion': 'neutral', 'normalize': 'N', 'emoji': '😐'},\n",
       " {'emotion': 'optimism', 'normalize': 'MP', 'emoji': '🌈'},\n",
       " {'emotion': 'pride', 'normalize': 'FP', 'emoji': '😌'},\n",
       " {'emotion': 'realization', 'normalize': 'MP', 'emoji': '💡'},\n",
       " {'emotion': 'relief', 'normalize': 'MP', 'emoji': '😌'},\n",
       " {'emotion': 'remorse', 'normalize': 'MN', 'emoji': '😔'},\n",
       " {'emotion': 'sadness', 'normalize': 'FN', 'emoji': '😢'},\n",
       " {'emotion': 'surprise', 'normalize': 'MP', 'emoji': '😮'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CLustering",
   "id": "c32e94e97a729b0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will now apply data clustering to generate groups that will serve as playlists, built around key aspects of related songs",
   "id": "17efe6b1209b4ed8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json, ast\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def ensure_list_column(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensures column `col` is a Python list[str].\n",
    "    Handles:\n",
    "      - Python repr lists with single quotes: \"['a', 'b']\"\n",
    "      - JSON lists: [\"a\",\"b\"]\n",
    "      - Comma-separated fallback: a, b\n",
    "    \"\"\"\n",
    "\n",
    "    def _parse(x):\n",
    "        if isinstance(x, list):\n",
    "            return [str(t).strip() for t in x if str(t).strip()]\n",
    "        if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "            return []\n",
    "        if isinstance(x, str):\n",
    "            s = x.strip()\n",
    "            if not s:\n",
    "                return []\n",
    "            # 1) Try Python literal (handles single quotes)\n",
    "            if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "                try:\n",
    "                    v = ast.literal_eval(s)\n",
    "                    if isinstance(v, list):\n",
    "                        return [str(t).strip() for t in v if str(t).strip()]\n",
    "                except Exception:\n",
    "                    pass\n",
    "                # 2) Try JSON (double quotes)\n",
    "                try:\n",
    "                    v = json.loads(s)\n",
    "                    if isinstance(v, list):\n",
    "                        return [str(t).strip() for t in v if str(t).strip()]\n",
    "                except Exception:\n",
    "                    pass\n",
    "                # 3) Fallback: split inner by comma\n",
    "                inner = s[1:-1]\n",
    "                parts = [p.strip().strip(\"'\").strip('\"') for p in inner.split(\",\")]\n",
    "                return [p for p in parts if p]\n",
    "            # 4) Plain comma-separated\n",
    "            return [t.strip() for t in s.split(\",\") if t.strip()]\n",
    "        # last resort\n",
    "        return [str(x).strip()] if str(x).strip() else []\n",
    "\n",
    "    out = df.copy()\n",
    "    out[col] = out[col].apply(_parse)\n",
    "    return out"
   ],
   "id": "684e9ba0afaa40b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We check that the normalized_keywords column is of type list.",
   "id": "3bff17ca6adc25f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert the column from string to list[str]\n",
    "df = ensure_list_column(df, col=\"normalized_keywords\")\n",
    "\n",
    "# Sanity check\n",
    "print(type(df.loc[df.index[0], \"normalized_keywords\"]), df.loc[df.index[0], \"normalized_keywords\"][:5])"
   ],
   "id": "8eeb9d1a9236ae21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This script implements an end-to-end pipeline to group songs into **thematic playlists** using **semantic keyword embeddings** and clustering with **KMeans**.\n",
    "\n",
    "1. Build embeddings per track\n",
    "- Each song has a set of *normalized keywords*.\n",
    "- Using a *Sentence Transformers* model (`all-MiniLM-L6-v2`), vector embeddings are generated for those keywords.\n",
    "- A *mean pooling* operation (averaging) is applied to obtain a single dense vector representing the entire track.\n",
    "\n",
    "2. Clustering with KMeans\n",
    "- The embeddings of all tracks are clustered into *k* groups using KMeans.\n",
    "- The **Silhouette Score** is computed to quickly evaluate clustering quality.\n",
    "\n",
    "3. Attach clusters back to the DataFrame\n",
    "- A new column `cluster_emb` is added to the original DataFrame, indicating the cluster assignment for each track.\n",
    "\n",
    "4. Cluster interpretability\n",
    "- The most frequent keywords per cluster are extracted, producing a top-N list that summarizes the common themes of each group.\n",
    "- Representative “Artist — Title” examples are sampled for each cluster, making it easier to inspect them manually."
   ],
   "id": "9836fe2fb4fec140"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from collections import Counter, defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Build embeddings per track\n",
    "# ---------------------------\n",
    "def compute_track_embeddings(\n",
    "        df: pd.DataFrame,\n",
    "        keywords_col: str = \"normalized_keywords\",\n",
    "        model_name: str = \"all-MiniLM-L6-v2\",\n",
    "        normalize: bool = True,\n",
    "        max_keywords: int | None = None\n",
    ") -> tuple[np.ndarray, List[int], SentenceTransformer]:\n",
    "    \"\"\"\n",
    "    Computes a dense embedding per track by averaging the embeddings of its normalized keywords.\n",
    "    Returns:\n",
    "      - X: np.ndarray of shape (n_tracks, dim)\n",
    "      - valid_idx: indices of rows that have at least 1 keyword (used to subset df later)\n",
    "      - model: the SentenceTransformer model used\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    vectors = []\n",
    "    valid_idx = []\n",
    "\n",
    "    for i, kws in enumerate(df[keywords_col].tolist()):\n",
    "        if not isinstance(kws, list) or len(kws) == 0:\n",
    "            continue\n",
    "        toks = [str(k).strip() for k in kws if isinstance(k, str) and str(k).strip()]\n",
    "        if not toks:\n",
    "            continue\n",
    "        if max_keywords is not None:\n",
    "            toks = toks[:max_keywords]\n",
    "\n",
    "        kw_emb = model.encode(toks, normalize_embeddings=normalize)\n",
    "        if isinstance(kw_emb, list):\n",
    "            kw_emb = np.asarray(kw_emb)\n",
    "        track_vec = kw_emb.mean(axis=0)  # mean pooling over keywords\n",
    "        vectors.append(track_vec)\n",
    "        valid_idx.append(i)\n",
    "\n",
    "    if not vectors:\n",
    "        raise ValueError(\"No tracks produced embeddings. Check 'normalized_keywords' column.\")\n",
    "    X = np.vstack(vectors)\n",
    "    return X, valid_idx, model\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2) KMeans clustering\n",
    "# ---------------------------\n",
    "def cluster_tracks_embeddings(\n",
    "        X: np.ndarray,\n",
    "        k: int = 8,\n",
    "        random_state: int = 42\n",
    ") -> tuple[np.ndarray, KMeans, float]:\n",
    "    \"\"\"\n",
    "    Clusters the embedding matrix X with KMeans.\n",
    "    Returns:\n",
    "      - labels: cluster assignment per row in X\n",
    "      - kmeans: fitted model\n",
    "      - sil: silhouette score (for quick quality check)\n",
    "    \"\"\"\n",
    "    km = KMeans(n_clusters=k, random_state=random_state, n_init=\"auto\")\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels) if len(set(labels)) > 1 else np.nan\n",
    "    return labels, km, sil\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Attach clusters back to df\n",
    "# ---------------------------\n",
    "def attach_clusters_to_df(\n",
    "        df: pd.DataFrame,\n",
    "        labels: np.ndarray,\n",
    "        valid_idx: List[int],\n",
    "        cluster_col: str = \"cluster_emb\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds the cluster labels to a copy of df at the indices used for embeddings.\n",
    "    Rows without embeddings remain with NaN clusters.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    out[cluster_col] = np.nan\n",
    "    out.loc[out.index[valid_idx], cluster_col] = labels\n",
    "    if out[cluster_col].isna().any():\n",
    "        out[cluster_col] = out[cluster_col].astype(\"Int64\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Cluster summaries (human-readable)\n",
    "# ---------------------------\n",
    "def top_keywords_per_cluster(\n",
    "        df_with_clusters: pd.DataFrame,\n",
    "        cluster_col: str = \"cluster_emb\",\n",
    "        keywords_col: str = \"normalized_keywords\",\n",
    "        top_n: int = 12\n",
    ") -> dict[int, List[tuple[str, int]]]:\n",
    "    \"\"\"\n",
    "    For interpretability: returns top-N most frequent normalized keywords per cluster.\n",
    "    \"\"\"\n",
    "    summary = {}\n",
    "    for c in sorted(df_with_clusters[cluster_col].dropna().unique()):\n",
    "        sub = df_with_clusters[df_with_clusters[cluster_col] == c]\n",
    "        cnt = Counter()\n",
    "        for kws in sub[keywords_col]:\n",
    "            if isinstance(kws, list):\n",
    "                cnt.update([k for k in kws if isinstance(k, str) and k.strip()])\n",
    "        summary[int(c)] = cnt.most_common(top_n)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def sample_titles_per_cluster(\n",
    "        df_with_clusters: pd.DataFrame,\n",
    "        cluster_col: str = \"cluster_emb\",\n",
    "        n_samples: int = 5\n",
    ") -> dict[int, List[str]]:\n",
    "    \"\"\"\n",
    "    Returns up to n_samples \"Artist — Title\" examples per cluster.\n",
    "    \"\"\"\n",
    "    examples = {}\n",
    "    for c in sorted(df_with_clusters[cluster_col].dropna().unique()):\n",
    "        sub = df_with_clusters[df_with_clusters[cluster_col] == c].head(n_samples)\n",
    "        examples[int(c)] = [\n",
    "            f\"{row['artist_name']} — {row['title']}\"\n",
    "            for _, row in sub.iterrows()\n",
    "        ]\n",
    "    return examples\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 5) End-to-end usage\n",
    "# ---------------------------\n",
    "# df is your DataFrame with columns:\n",
    "# - track_spotify_id\n",
    "# - artist_name\n",
    "# - title\n",
    "# - normalized_keywords (list[str])\n",
    "\n",
    "# Build embeddings\n",
    "X, valid_idx, st_model = compute_track_embeddings(\n",
    "    df, keywords_col=\"normalized_keywords\", model_name=\"all-MiniLM-L6-v2\", normalize=True\n",
    ")\n",
    "\n",
    "# Cluster (choose k based on your dataset size; start with 6–12)\n",
    "labels, kmeans, sil = cluster_tracks_embeddings(X, k=15, random_state=42)\n",
    "print(\"Silhouette:\", sil)\n",
    "\n",
    "# Attach cluster labels back to df\n",
    "df_emb = attach_clusters_to_df(df, labels, valid_idx, cluster_col=\"cluster_emb\")\n",
    "\n",
    "# Inspect\n",
    "df_emb[[\"track_spotify_id\", \"artist_name\", \"title\", \"cluster_emb\"]].head(10)\n",
    "\n",
    "# Top keywords per cluster (for interpretation)\n",
    "cluster_keywords = top_keywords_per_cluster(df_emb, cluster_col=\"cluster_emb\", keywords_col=\"normalized_keywords\",\n",
    "                                            top_n=20)\n",
    "for cid, tops in cluster_keywords.items():\n",
    "    print(f\"\\nCluster {cid} top keywords:\")\n",
    "    print(\", \".join([f\"{w}({c})\" for w, c in tops]))\n",
    "\n",
    "# Optional: sample titles per cluster\n",
    "examples = sample_titles_per_cluster(df_emb, cluster_col=\"cluster_emb\", n_samples=10)\n",
    "for cid, ex in examples.items():\n",
    "    print(f\"\\nCluster {cid} examples:\")\n",
    "    for s in ex:\n",
    "        print(\" -\", s)\n"
   ],
   "id": "1244512e690bbf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we’ll do something interesting: we will send our clusters with their top keywords to the AI so that it can return a name and an engaging description, which we will use to build our playlists.",
   "id": "b1d2b99da215242f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os, json, re\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "def build_cluster_payload(df_with_clusters: pd.DataFrame,\n",
    "                          cluster_col: str = \"cluster_emb\",\n",
    "                          keywords_col: str = \"normalized_keywords\",\n",
    "                          title_cols=(\"artist_name\", \"title\"),\n",
    "                          top_k_keywords: int = 15,\n",
    "                          max_examples: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Prepares a payload per cluster:\n",
    "      {cluster_id: {\"keywords\": [...], \"examples\": [\"Artist — Title\", ...]}}\n",
    "    \"\"\"\n",
    "    payload = {}\n",
    "    for cid in sorted(df_with_clusters[cluster_col].dropna().unique()):\n",
    "        sub = df_with_clusters[df_with_clusters[cluster_col] == cid]\n",
    "        # top keywords by simple frequency inside the cluster\n",
    "        kw_series = sub[keywords_col].explode()\n",
    "        kw_series = kw_series[kw_series.notna()].astype(str).str.strip().str.lower()\n",
    "        top = (kw_series.value_counts().head(top_k_keywords).index.tolist()\n",
    "               if not kw_series.empty else [])\n",
    "        payload[int(cid)] = {\"keywords\": top}\n",
    "    return payload\n",
    "\n",
    "\n",
    "def name_clusters_with_llm(cluster_payload: dict,\n",
    "                           model: str = \"gpt-4.1-mini\",\n",
    "                           temperature: float = 0.2) -> dict:\n",
    "    \"\"\"\n",
    "    Sends cluster summaries to the LLM and returns:\n",
    "      {cluster_id: {\"name\": str, \"description\": str}}\n",
    "    Robust JSON parsing with fallback.\n",
    "    \"\"\"\n",
    "    # Build prompt\n",
    "    items = []\n",
    "    for cid, data in cluster_payload.items():\n",
    "        kws = \", \".join(data[\"keywords\"])\n",
    "        items.append(\n",
    "            f\"Cluster {cid}:\\n\"\n",
    "            f\"- Top keywords: {kws or '(none)'}\\n\"\n",
    "        )\n",
    "    clusters_block = \"\\n\\n\".join(items)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an expert music curator. Name each cluster of songs concisely.\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"- Provide a short, human-friendly name (≤ 4 words) and a 1–2 sentence description.\\n\"\n",
    "        \"- Avoid artist names and proper nouns; focus on themes/moods.\\n\"\n",
    "        \"- Prefer genre/vibe/emotion terms (e.g., 'Dancefloor Energy', 'Melancholic Love').\\n\"\n",
    "        \"- Keep names in Title Case, English only.\\n\"\n",
    "        \"Return ONLY a JSON object mapping cluster id to an object with fields 'name' and 'description'.\\n\\n\"\n",
    "        \"Example output format:\\n\"\n",
    "        \"{\\n\"\n",
    "        \"  \\\"0\\\": {\\\"name\\\": \\\"Late-Night Romance\\\", \\\"description\\\": \\\"Sensual, intimate themes with slow dance vibes.\\\"},\\n\"\n",
    "        \"  \\\"1\\\": {\\\"name\\\": \\\"Party Anthems\\\", \\\"description\\\": \\\"High-energy club tracks centered on dancing and celebration.\\\"}\\n\"\n",
    "        \"}\\n\\n\"\n",
    "        \"Clusters to label:\\n\"\n",
    "        f\"{clusters_block}\\n\\n\"\n",
    "        \"Now return ONLY the JSON.\"\n",
    "    )\n",
    "\n",
    "    resp = client.responses.create(model=model, input=prompt, temperature=temperature)\n",
    "    text = resp.output_text.strip()\n",
    "\n",
    "    # Parse JSON robustly\n",
    "    def parse_json_maybe(s: str) -> dict:\n",
    "        try:\n",
    "            return json.loads(s)\n",
    "        except Exception:\n",
    "            m = re.search(r\"\\{(?:.|\\n)*\\}\\s*$\", s)  # grab last JSON-looking block\n",
    "            if m:\n",
    "                try:\n",
    "                    return json.loads(m.group(0))\n",
    "                except Exception:\n",
    "                    return {}\n",
    "            return {}\n",
    "\n",
    "    data = parse_json_maybe(text)\n",
    "    # Normalize keys to ints if possible\n",
    "    out = {}\n",
    "    for k, v in data.items():\n",
    "        try:\n",
    "            cid = int(k)\n",
    "        except Exception:\n",
    "            cid = k\n",
    "        name = (v or {}).get(\"name\", \"\").strip()\n",
    "        desc = (v or {}).get(\"description\", \"\").strip()\n",
    "        if name:\n",
    "            out[cid] = {\"name\": name, \"description\": desc}\n",
    "    return out\n",
    "\n",
    "\n",
    "def attach_cluster_names(df_with_clusters: pd.DataFrame,\n",
    "                         labels_map: dict,\n",
    "                         cluster_col: str = \"cluster_emb\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds 'cluster_name' and 'cluster_desc' columns using labels_map from LLM.\n",
    "    \"\"\"\n",
    "    out = df_with_clusters.copy()\n",
    "    out[\"cluster_name\"] = out[cluster_col].map(\n",
    "        lambda c: labels_map.get(int(c), {}).get(\"name\") if pd.notna(c) else None)\n",
    "    out[\"cluster_desc\"] = out[cluster_col].map(\n",
    "        lambda c: labels_map.get(int(c), {}).get(\"description\") if pd.notna(c) else None)\n",
    "    return out\n"
   ],
   "id": "e4d028bf8f2b32d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Build payload from your clustered DataFrame (assumes 'cluster_emb' and 'normalized_keywords')\n",
    "payload = build_cluster_payload(df_emb, cluster_col=\"cluster_emb\", keywords_col=\"normalized_keywords\",\n",
    "                                top_k_keywords=20, max_examples=5)\n",
    "\n",
    "# 2) Ask the LLM for names/descriptions\n",
    "labels_map = name_clusters_with_llm(payload, model=\"gpt-4.1-mini\", temperature=0.2)\n",
    "\n",
    "# 3) Attach names back to your DataFrame\n",
    "df_named = attach_cluster_names(df_emb, labels_map, cluster_col=\"cluster_emb\")\n",
    "df_named[[\"cluster_emb\", \"cluster_name\", \"cluster_desc\"]].drop_duplicates().sort_values(\"cluster_emb\").head(20)\n",
    "\n"
   ],
   "id": "b34361d3f2c81e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We update the CSV to streamline the process.",
   "id": "5bbdff4d94452992"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_emb.to_csv(\"tracks_with_normalized_keywords.csv\", index=False, encoding=\"utf-8\")",
   "id": "149514f6a6d639c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this step, we connected our DataFrame (`df_named`) with the database to properly manage clusters and link them to tracks.",
   "id": "de0e5e1cf9324b2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def sync_clusters_and_update_tracks(conn, df_named: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Safe sync without TRUNCATE (FK-friendly).\n",
    "\n",
    "    Steps:\n",
    "    1) UPDATE tracks SET cluster_id = NULL        # break FK references safely\n",
    "    2) DELETE FROM clusters;                      # clear parent table\n",
    "    3) ALTER TABLE clusters AUTO_INCREMENT = 1;   # optional reset\n",
    "    4) INSERT unique (name, description) from df_named\n",
    "    5) SELECT id, name FROM clusters -> name→id map\n",
    "    6) UPDATE tracks.cluster_id by matching spotify_id (df) to tracks.spotify_id (db)\n",
    "    \"\"\"\n",
    "\n",
    "    required_cols = {\"cluster_name\", \"cluster_desc\", \"track_spotify_id\"}\n",
    "    missing = required_cols - set(df_named.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"df_named is missing required columns: {missing}\")\n",
    "\n",
    "    clusters_df = (\n",
    "        df_named[[\"cluster_name\", \"cluster_desc\"]]\n",
    "        .dropna(subset=[\"cluster_name\"])\n",
    "        .drop_duplicates(subset=[\"cluster_name\"])\n",
    "        .rename(columns={\"cluster_name\": \"name\", \"cluster_desc\": \"description\"})\n",
    "    )\n",
    "    track_map_df = df_named.dropna(subset=[\"cluster_name\"])[[\"track_spotify_id\", \"cluster_name\"]]\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # 1) Break FK references in child table\n",
    "            cur.execute(\"UPDATE tracks SET cluster_id = NULL;\")\n",
    "\n",
    "            # 2) Clear parent table without TRUNCATE (FK-safe)\n",
    "            cur.execute(\"DELETE FROM clusters;\")\n",
    "\n",
    "            # 3) Optionally reset AUTO_INCREMENT\n",
    "            cur.execute(\"ALTER TABLE clusters AUTO_INCREMENT = 1;\")\n",
    "\n",
    "            # 4) Insert new clusters\n",
    "            cur.executemany(\n",
    "                \"INSERT INTO clusters (name, description) VALUES (%s, %s)\",\n",
    "                [(str(r[\"name\"]).strip(),\n",
    "                  str(r[\"description\"]).strip() if pd.notna(r[\"description\"]) else \"\")\n",
    "                 for _, r in clusters_df.iterrows()]\n",
    "            )\n",
    "\n",
    "        # 5) Build name -> id map\n",
    "        with conn.cursor(pymysql.cursors.DictCursor) as cur:\n",
    "            cur.execute(\"SELECT id, name FROM clusters;\")\n",
    "            rows = cur.fetchall()\n",
    "            name_to_id = {row[\"name\"]: row[\"id\"] for row in rows}\n",
    "\n",
    "        # 6) Update tracks with cluster_id\n",
    "        with conn.cursor() as cur:\n",
    "            update_sql = \"\"\"\n",
    "                UPDATE tracks\n",
    "                SET cluster_id = %s\n",
    "                WHERE spotify_id = %s\n",
    "            \"\"\"\n",
    "            data = []\n",
    "            for _, r in track_map_df.iterrows():\n",
    "                name = str(r[\"cluster_name\"]).strip()\n",
    "                track_id = str(r[\"track_spotify_id\"]).strip()\n",
    "                cid = name_to_id.get(name)\n",
    "                if cid is not None and track_id:\n",
    "                    data.append((cid, track_id))\n",
    "            if data:\n",
    "                cur.executemany(update_sql, data)\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"Inserted clusters: {len(clusters_df)} | Updated tracks: {len(data)}\")\n",
    "\n",
    "    except Exception:\n",
    "        conn.rollback()\n",
    "        raise\n",
    "\n",
    "\n",
    "conn = db.create_connection()\n",
    "sync_clusters_and_update_tracks(conn, df_named)\n",
    "conn.close()"
   ],
   "id": "c9d6bb7cfe4fb81d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Show Results",
   "id": "1a0dbdcf459b7a90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Useful Functions\n",
    "\n",
    "We’ll start with functions that will help us get what we want. In this case, we’re going to create a function that retrieves the overall sentiment of all the songs along with the specific data, taking the top 5 most representative songs for each emotion.\n",
    "We’ll also be able to see it by album, and even by individual song if we want. Additionally, we’ll obtain the suggested playlists along with their main emotion."
   ],
   "id": "9eccc55fcf8095f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "####\n",
    "1. Get the overall emotion of the entire Spotify"
   ],
   "id": "d8d38dffcd3a2f07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T15:36:54.376566Z",
     "start_time": "2025-08-29T15:36:54.365284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict, List, Any\n",
    "from pymysql.cursors import DictCursor\n",
    "\n",
    "\n",
    "def get_emotions_from_dictionary() -> list[str]:\n",
    "    \"\"\"\n",
    "    Step 1: Fetch all emotions from emotions_dictionary.\n",
    "    \"\"\"\n",
    "    conn = db.create_connection()\n",
    "    try:\n",
    "        with conn.cursor(DictCursor) as cur:\n",
    "            cur.execute(\"SELECT emotion FROM emotions_dictionary\")\n",
    "            emotions = [r[\"emotion\"] for r in cur.fetchall()]\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if not emotions:\n",
    "        raise ValueError(\"No emotions found in emotions_dictionary\")\n",
    "\n",
    "    return emotions\n",
    "\n",
    "\n",
    "def build_avg_emotions_sql(emotions: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Step 2: Build a dynamic SQL query with AVG(col) for each emotion.\n",
    "    Note: we use aliases equal to the original emotion names.\n",
    "    \"\"\"\n",
    "    avg_exprs = [f\"AVG(`{emo}`) AS `{emo}`\" for emo in emotions]\n",
    "    sql = f\"SELECT {', '.join(avg_exprs)} FROM track_emotions_wide\"\n",
    "    return sql\n",
    "\n",
    "\n",
    "def execute_avg_emotions_query(sql: str) -> dict:\n",
    "    \"\"\"\n",
    "    Step 3: Execute the generated SQL query and return the results\n",
    "    with keys = original emotion names.\n",
    "    \"\"\"\n",
    "    conn = db.create_connection()\n",
    "    try:\n",
    "        with conn.cursor(DictCursor) as cur:\n",
    "            cur.execute(sql)\n",
    "            row = cur.fetchone()\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return {k: float(v) if v is not None else None for k, v in row.items()}\n",
    "\n",
    "\n",
    "def fetch_avg_per_emotion() -> dict:\n",
    "    \"\"\"\n",
    "    Orchestrates the 3 steps:\n",
    "    1. Fetch emotions from the dictionary,\n",
    "    2. Build the dynamic SQL query,\n",
    "    3. Execute the query and return the dict with averages.\n",
    "    \"\"\"\n",
    "    emotions = get_emotions_from_dictionary()\n",
    "    sql = build_avg_emotions_sql(emotions)\n",
    "    result = execute_avg_emotions_query(sql)\n",
    "    return result\n",
    "\n",
    "\n",
    "def fetch_emotion_normalize_mapping() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Step 1: Query emotions_dictionary and return a mapping:\n",
    "      { emotion_lower: normalize_code }\n",
    "\n",
    "    Works case-insensitive by lowering all emotion names.\n",
    "    \"\"\"\n",
    "    conn = db.create_connection()\n",
    "    try:\n",
    "        with conn.cursor(DictCursor) as cur:\n",
    "            cur.execute(\"SELECT emotion, normalize FROM emotions_dictionary\")\n",
    "            rows = cur.fetchall()\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if not rows:\n",
    "        raise ValueError(\"emotions_dictionary is empty or missing.\")\n",
    "\n",
    "    # Build a dict where keys are lowercase emotion names and values are normalize codes\n",
    "    return {r[\"emotion\"].strip().lower(): str(r[\"normalize\"]).strip() for r in rows}\n",
    "\n",
    "\n",
    "def group_avg_emotions_by_normalize(avg_per_emotion: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Step 2: Receive a dict with per-emotion averages, e.g.:\n",
    "      {'admiration': 3.2, 'anger': 1.8, ...}\n",
    "\n",
    "    Group those averages by their normalize category from emotions_dictionary.\n",
    "\n",
    "    Returns a list of dicts with:\n",
    "      - normalize: category (FP/MP/N/MN/FN or Positive/Neutral/Negative)\n",
    "      - sum_avg: sum of averages of all emotions in this group\n",
    "      - mean_avg: mean of those averages (sum_avg / emotion_count)\n",
    "      - emotion_count: how many emotions were grouped\n",
    "    \"\"\"\n",
    "    if not isinstance(avg_per_emotion, dict) or not avg_per_emotion:\n",
    "        raise ValueError(\"avg_per_emotion must be a non-empty dict like {'admiration': 3.2, ...}\")\n",
    "\n",
    "    # Get mapping from emotions to normalize codes\n",
    "    mapping = fetch_emotion_normalize_mapping()  # {emotion_lower: normalize}\n",
    "    groups: Dict[str, List[float]] = {}\n",
    "\n",
    "    # Match emotions to normalize groups\n",
    "    for emo_name, avg_val in avg_per_emotion.items():\n",
    "        if avg_val is None:\n",
    "            continue\n",
    "        try:\n",
    "            v = float(avg_val)\n",
    "        except (TypeError, ValueError):\n",
    "            continue\n",
    "        norm = mapping.get(str(emo_name).strip().lower())\n",
    "        if not norm:\n",
    "            # Emotion not found in dictionary → skip\n",
    "            continue\n",
    "        groups.setdefault(norm, []).append(v)\n",
    "\n",
    "    # Build final result list\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    # Preferred order (if you are using 5-level system); fallback: alphabetical\n",
    "    preferred_order = [\"FP\", \"MP\", \"N\", \"MN\", \"FN\", \"Positive\", \"Neutral\", \"Negative\"]\n",
    "    ordered_norms = [n for n in preferred_order if n in groups] + \\\n",
    "                    [n for n in sorted(groups.keys()) if n not in preferred_order]\n",
    "\n",
    "    for norm in ordered_norms:\n",
    "        vals = groups[norm]\n",
    "        s = sum(vals)\n",
    "        c = len(vals)\n",
    "        results.append({\n",
    "            \"normalize\": norm,\n",
    "            \"sum_avg\": s,\n",
    "            \"emotion_count\": c,\n",
    "        })\n",
    "\n",
    "    return results"
   ],
   "id": "d21cadc1a40c6809",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T04:21:02.232415Z",
     "start_time": "2025-09-02T04:21:02.226004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Dict, Any\n",
    "from pymysql.cursors import DictCursor\n",
    "\n",
    "def fetch_emotions_dictionary() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetch all rows from emotions_dictionary.\n",
    "    Returns a list of dicts with keys: emotion, normalize, emoji.\n",
    "    \"\"\"\n",
    "    sql = \"SELECT emotion, emoji FROM emotions_dictionary ORDER BY emotion\"\n",
    "\n",
    "    conn = db.create_connection()\n",
    "    try:\n",
    "        with conn.cursor(DictCursor) as cur:\n",
    "            cur.execute(sql)\n",
    "            rows = cur.fetchall()\n",
    "    finally:\n",
    "        try: conn.close()\n",
    "        except: pass\n",
    "\n",
    "    return rows\n"
   ],
   "id": "cf796037efb441a4",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T04:19:50.466779Z",
     "start_time": "2025-09-02T04:19:50.456476Z"
    }
   },
   "cell_type": "code",
   "source": "fetch_emotions_dictionary()",
   "id": "d1a3d3564cc5bff6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'emotion': 'admiration', 'normalize': 'FP', 'emoji': '😍'},\n",
       " {'emotion': 'amusement', 'normalize': 'FP', 'emoji': '😂'},\n",
       " {'emotion': 'anger', 'normalize': 'FN', 'emoji': '😠'},\n",
       " {'emotion': 'annoyance', 'normalize': 'MN', 'emoji': '😒'},\n",
       " {'emotion': 'approval', 'normalize': 'FP', 'emoji': '👍'},\n",
       " {'emotion': 'caring', 'normalize': 'FP', 'emoji': '❤️'},\n",
       " {'emotion': 'confusion', 'normalize': 'N', 'emoji': '😕'},\n",
       " {'emotion': 'curiosity', 'normalize': 'MP', 'emoji': '🤔'},\n",
       " {'emotion': 'desire', 'normalize': 'FP', 'emoji': '😍'},\n",
       " {'emotion': 'disappointment', 'normalize': 'MN', 'emoji': '😞'},\n",
       " {'emotion': 'disapproval', 'normalize': 'FN', 'emoji': '👎'},\n",
       " {'emotion': 'disgust', 'normalize': 'FN', 'emoji': '🤢'},\n",
       " {'emotion': 'embarrassment', 'normalize': 'MN', 'emoji': '😳'},\n",
       " {'emotion': 'excitement', 'normalize': 'FP', 'emoji': '🎉'},\n",
       " {'emotion': 'fear', 'normalize': 'FN', 'emoji': '😨'},\n",
       " {'emotion': 'gratitude', 'normalize': 'FP', 'emoji': '🙏'},\n",
       " {'emotion': 'grief', 'normalize': 'FN', 'emoji': '😢'},\n",
       " {'emotion': 'joy', 'normalize': 'FP', 'emoji': '😊'},\n",
       " {'emotion': 'love', 'normalize': 'FP', 'emoji': '❤️'},\n",
       " {'emotion': 'nervousness', 'normalize': 'MN', 'emoji': '😬'},\n",
       " {'emotion': 'neutral', 'normalize': 'N', 'emoji': '😐'},\n",
       " {'emotion': 'optimism', 'normalize': 'MP', 'emoji': '🌈'},\n",
       " {'emotion': 'pride', 'normalize': 'FP', 'emoji': '😌'},\n",
       " {'emotion': 'realization', 'normalize': 'MP', 'emoji': '💡'},\n",
       " {'emotion': 'relief', 'normalize': 'MP', 'emoji': '😌'},\n",
       " {'emotion': 'remorse', 'normalize': 'MN', 'emoji': '😔'},\n",
       " {'emotion': 'sadness', 'normalize': 'FN', 'emoji': '😢'},\n",
       " {'emotion': 'surprise', 'normalize': 'MP', 'emoji': '😮'}]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "####\n",
    "2. Fetch clusters"
   ],
   "id": "483b6ac164f43af9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T18:45:15.479591Z",
     "start_time": "2025-08-29T18:45:15.468638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from pymysql.cursors import DictCursor\n",
    "\n",
    "\n",
    "# ---------- Helpers (reuse-safe) ----------\n",
    "\n",
    "def _get_existing_tew_columns() -> List[str]:\n",
    "    \"\"\"\n",
    "    Return the list of existing columns in track_emotions_wide\n",
    "    (excluding track_spotify_id), preserving table order.\n",
    "    \"\"\"\n",
    "    conn = db.create_connection()\n",
    "    sql = \"\"\"\n",
    "        SELECT COLUMN_NAME\n",
    "        FROM INFORMATION_SCHEMA.COLUMNS\n",
    "        WHERE TABLE_SCHEMA = DATABASE()\n",
    "          AND TABLE_NAME = 'track_emotions_wide'\n",
    "        ORDER BY ORDINAL_POSITION\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor(DictCursor) as cur:\n",
    "            cur.execute(sql)\n",
    "            cols = [r[\"COLUMN_NAME\"] for r in cur.fetchall()]\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return [c for c in cols if c and c.lower() != \"track_spotify_id\"]\n",
    "\n",
    "\n",
    "def get_emotions_from_dictionary() -> List[str]:\n",
    "    \"\"\"\n",
    "    Step 1 for dynamic AVG building: fetch emotion column names from emotions_dictionary.\n",
    "    \"\"\"\n",
    "    conn = db.create_connection()\n",
    "    try:\n",
    "        with conn.cursor(DictCursor) as cur:\n",
    "            cur.execute(\"SELECT emotion FROM emotions_dictionary\")\n",
    "            emotions = [r[\"emotion\"] for r in cur.fetchall()]\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if not emotions:\n",
    "        raise ValueError(\"No emotions found in emotions_dictionary\")\n",
    "    return emotions\n",
    "\n",
    "\n",
    "# ---------- 1) All tracks with their clusters, artist name, and album title ----------\n",
    "\n",
    "def fetch_all_tracks_with_clusters() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Return all tracks (expected ~990) with:\n",
    "      - track_spotify_id\n",
    "      - cluster_id, cluster_name\n",
    "      - artist_spotify_id, artist_name\n",
    "      - album_spotify_id, album_name\n",
    "    \"\"\"\n",
    "    sql = \"\"\"\n",
    "        SELECT\n",
    "          t.spotify_id          AS track_spotify_id,\n",
    "          t.cluster_id          AS cluster_id,\n",
    "          t.name                AS track_name,\n",
    "          c.name                AS cluster_name,\n",
    "          ar.spotify_id         AS artist_spotify_id,\n",
    "          ar.name               AS artist_name,\n",
    "          a.spotify_id          AS album_spotify_id,\n",
    "          a.name                AS album_name\n",
    "        FROM tracks t\n",
    "        LEFT JOIN clusters c ON c.id = t.cluster_id\n",
    "        LEFT JOIN artists  ar ON ar.spotify_id = t.artist_spotify_id\n",
    "        LEFT JOIN albums   a  ON a.spotify_id  = t.album_spotify_id\n",
    "        ORDER BY ar.name, a.name, t.spotify_id\n",
    "    \"\"\"\n",
    "    conn = db.create_connection()\n",
    "    try:\n",
    "        with conn.cursor(DictCursor) as cur:\n",
    "            cur.execute(sql)\n",
    "            rows = cur.fetchall()\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ---------- 2) All tracks for a given cluster (with artist and album names) ----------\n",
    "\n",
    "def fetch_tracks_by_cluster(cluster_id: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Return tracks for a specific cluster with the same fields as above.\n",
    "    \"\"\"\n",
    "    sql = \"\"\"\n",
    "        SELECT\n",
    "          t.spotify_id          AS track_spotify_id,\n",
    "          t.cluster_id          AS cluster_id,\n",
    "          t.name                AS track_name,\n",
    "          c.name                AS cluster_name,\n",
    "          ar.spotify_id         AS artist_spotify_id,\n",
    "          ar.name               AS artist_name,\n",
    "          a.spotify_id          AS album_spotify_id,\n",
    "          a.name                AS album_name\n",
    "        FROM tracks t\n",
    "        JOIN clusters c  ON c.id = t.cluster_id\n",
    "        LEFT JOIN artists ar ON ar.spotify_id = t.artist_spotify_id\n",
    "        LEFT JOIN albums  a  ON a.spotify_id  = t.album_spotify_id\n",
    "        WHERE t.cluster_id = %s\n",
    "        ORDER BY ar.name, a.name, t.spotify_id\n",
    "    \"\"\"\n",
    "    conn = db.create_connection()\n",
    "    try:\n",
    "        with conn.cursor(DictCursor) as cur:\n",
    "            cur.execute(sql, (cluster_id,))\n",
    "            rows = cur.fetchall()\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ---------- 4) Per-cluster averages for every emotion column ----------\n",
    "\n",
    "def build_cluster_avg_emotions_sql(emotions: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Build a dynamic SQL SELECT that computes AVG(col) AS `col` for all given emotions,\n",
    "    restricted to a cluster via WHERE t.cluster_id = %s.\n",
    "    The SELECT joins track_emotions_wide with tracks by spotify_id.\n",
    "    \"\"\"\n",
    "    # Intersect requested emotions with actual columns in track_emotions_wide\n",
    "    existing = set(_get_existing_tew_columns())\n",
    "    valid_emotions = [e for e in emotions if e in existing]\n",
    "    if not valid_emotions:\n",
    "        raise ValueError(\"None of the emotions exist in track_emotions_wide\")\n",
    "\n",
    "    avg_exprs = [f\"AVG(tew.`{emo}`) AS `{emo}`\" for emo in valid_emotions]\n",
    "    sql = f\"\"\"\n",
    "        SELECT {', '.join(avg_exprs)}\n",
    "        FROM track_emotions_wide AS tew\n",
    "        JOIN tracks AS t ON t.spotify_id = tew.track_spotify_id\n",
    "        WHERE t.cluster_id = %s\n",
    "    \"\"\"\n",
    "    return sql\n",
    "\n",
    "\n",
    "def fetch_cluster_avg_emotions(cluster_id: int) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute emotion-wise averages for a given cluster.\n",
    "    - Uses emotions from emotions_dictionary\n",
    "    - Validates columns against track_emotions_wide\n",
    "    - Returns a dict: { 'admiration': 3.2, 'anger': 1.8, ... }\n",
    "    \"\"\"\n",
    "    emotions = get_emotions_from_dictionary()\n",
    "    sql = build_cluster_avg_emotions_sql(emotions)\n",
    "\n",
    "    conn = db.create_connection()\n",
    "    try:\n",
    "        with conn.cursor(DictCursor) as cur:\n",
    "            cur.execute(sql, (cluster_id,))\n",
    "            row = cur.fetchone()  # one row with AVG per emotion\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Convert Decimal/None to float/None\n",
    "    return {k: (float(v) if v is not None else None) for k, v in row.items()}\n",
    "\n",
    "\n",
    "# ---------- Optional convenience: also return track_count for the cluster ----------\n",
    "\n",
    "def fetch_cluster_avg_emotions_with_count(cluster_id: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Same as fetch_cluster_avg_emotions, but also returns 'track_count' for the cluster.\n",
    "    \"\"\"\n",
    "    # Averages\n",
    "    emo_avgs = fetch_cluster_avg_emotions(cluster_id)\n",
    "\n",
    "    # Track count\n",
    "    conn = db.create_connection()\n",
    "    try:\n",
    "        with conn.cursor(DictCursor) as cur:\n",
    "            cur.execute(\"SELECT COUNT(*) AS n FROM tracks WHERE cluster_id = %s\", (cluster_id,))\n",
    "            n = cur.fetchone()[\"n\"]\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    out: Dict[str, Any] = {\"cluster_id\": cluster_id, \"track_count\": int(n)}\n",
    "    out.update(emo_avgs)\n",
    "    return out\n"
   ],
   "id": "f028cf58b8a3084d",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T04:18:47.540536Z",
     "start_time": "2025-09-02T04:18:47.531232Z"
    }
   },
   "cell_type": "code",
   "source": "get_emotions_from_dictionary()",
   "id": "7114d3e7d7fbaf94",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'neutral',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "####\n",
    "3. Fetch info"
   ],
   "id": "6ea27b475e853f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T18:45:55.269936Z",
     "start_time": "2025-08-29T18:45:55.263515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from pymysql.cursors import DictCursor\n",
    "\n",
    "\n",
    "# ------------------- Artists -------------------\n",
    "\n",
    "def fetch_all_artists() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Return all artists with their spotify_id and name.\n",
    "    \"\"\"\n",
    "    sql = \"SELECT spotify_id, name FROM artists ORDER BY name\"\n",
    "    conn = db.create_connection()\n",
    "    try:\n",
    "        with conn.cursor(DictCursor) as cur:\n",
    "            cur.execute(sql)\n",
    "            rows = cur.fetchall()\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "    return rows\n",
    "\n",
    "\n",
    "def fetch_artist_by_id(artist_spotify_id: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Return a single artist by its spotify_id.\n",
    "    \"\"\"\n",
    "    sql = \"SELECT spotify_id, name FROM artists WHERE spotify_id = %s\"\n",
    "    conn = db.create_connection()\n",
    "    try:\n",
    "        with conn.cursor(DictCursor) as cur:\n",
    "            cur.execute(sql, (artist_spotify_id,))\n",
    "            row = cur.fetchone()\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "    return row\n",
    "\n",
    "\n",
    "# ------------------- Albums -------------------\n",
    "\n",
    "def fetch_all_albums() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Return all albums with their spotify_id and name.\n",
    "    \"\"\"\n",
    "    sql = \"SELECT spotify_id, name FROM albums ORDER BY name\"\n",
    "    conn = db.create_connection()\n",
    "    try:\n",
    "        with conn.cursor(DictCursor) as cur:\n",
    "            cur.execute(sql)\n",
    "            rows = cur.fetchall()\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "    return rows\n",
    "\n",
    "\n",
    "def fetch_album_by_id(album_spotify_id: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Return a single album by its spotify_id.\n",
    "    \"\"\"\n",
    "    sql = \"SELECT spotify_id, name FROM albums WHERE spotify_id = %s\"\n",
    "    conn = db.create_connection()\n",
    "    try:\n",
    "        with conn.cursor(DictCursor) as cur:\n",
    "            cur.execute(sql, (album_spotify_id,))\n",
    "            row = cur.fetchone()\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except:\n",
    "            pass\n",
    "    return row\n"
   ],
   "id": "f1fd5467a2874573",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FastAPI Service",
   "id": "40beefbbea6438d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are going to set up a FastAPI service to provide the endpoints that will power a frontend, so you can visualize the lists created by the app and more information about your emotions. For this, I’ve built a small interface with Vue.",
   "id": "a8e5cf3cf606fcf3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T04:21:10.938598Z",
     "start_time": "2025-09-02T04:21:10.931306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import threading, time\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],      # permite todos los orígenes\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],      # permite todos los métodos (GET, POST, etc.)\n",
    "    allow_headers=[\"*\"],      # permite todos los headers\n",
    ")\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"Hello World\"}\n",
    "\n",
    "@app.get(\"/playlist-emotions\")\n",
    "def getNormalizeEmotions():\n",
    "    avgEmotions = group_avg_emotions_by_normalize(fetch_avg_per_emotion())\n",
    "    return {\"emotions\": avgEmotions}\n",
    "\n",
    "\n",
    "@app.get(\"/playlist-emotions/all\")\n",
    "def getAllEmotions():\n",
    "    avgEmotions = fetch_avg_per_emotion()\n",
    "    return {\"emotions\": avgEmotions, \"dictionary\": fetch_emotions_dictionary()}\n",
    "\n",
    "\n",
    "# Global state to keep track of the server\n",
    "SERVER_STATE = {\"server\": None, \"thread\": None}\n",
    "\n",
    "\n",
    "def start_server(host=\"127.0.0.1\", port=8000):\n",
    "    \"\"\"\n",
    "    Start the Uvicorn server in a separate thread.\n",
    "    \"\"\"\n",
    "    if SERVER_STATE[\"server\"] is not None:\n",
    "        print(f\"⚠️ A server is already running at http://{host}:{port}\")\n",
    "        return\n",
    "\n",
    "    config = uvicorn.Config(app, host=host, port=port, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "\n",
    "    thread = threading.Thread(target=server.run, daemon=True)\n",
    "    thread.start()\n",
    "\n",
    "    time.sleep(1)  # wait a bit to let the server start\n",
    "    SERVER_STATE.update({\"server\": server, \"thread\": thread})\n",
    "    print(f\"🚀 Server running at http://{host}:{port}\")\n",
    "\n",
    "\n",
    "def stop_server():\n",
    "    \"\"\"\n",
    "    Stop the server if it is running.\n",
    "    \"\"\"\n",
    "    server = SERVER_STATE.get(\"server\")\n",
    "    thread = SERVER_STATE.get(\"thread\")\n",
    "    if server is None:\n",
    "        print(\"ℹ️ No server is currently running.\")\n",
    "        return\n",
    "    server.should_exit = True\n",
    "    if thread and thread.is_alive():\n",
    "        thread.join(timeout=3)\n",
    "    SERVER_STATE.update({\"server\": None, \"thread\": None})\n",
    "    print(\"🛑 Server stopped.\")\n"
   ],
   "id": "1bebc40be855d121",
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T04:21:13.039072Z",
     "start_time": "2025-09-02T04:21:12.031822Z"
    }
   },
   "cell_type": "code",
   "source": "start_server()",
   "id": "1a0072ca27d29a53",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [33886]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Server running at http://127.0.0.1:8000\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Stop Server\n",
    "\n",
    "When you finish, run this cell to shut down the server."
   ],
   "id": "a3cb3fab635f4f6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T04:21:07.299202Z",
     "start_time": "2025-09-02T04:21:07.126040Z"
    }
   },
   "cell_type": "code",
   "source": "stop_server()",
   "id": "dc344ad689ff8130",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛑 Server stopped.\n"
     ]
    }
   ],
   "execution_count": 128
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
